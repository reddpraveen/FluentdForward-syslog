#!/bin/bash

echo "=== 1. Check YOUR user permissions ==="
oc whoami
oc auth can-i get prometheus --namespace openshift-monitoring
oc auth can-i get prometheus --namespace openshift-user-workload-monitoring

echo -e "\n=== 2. Check what roles you have ==="
oc describe clusterrolebinding | grep "$(oc whoami)"
oc describe rolebinding -n openshift-monitoring | grep "$(oc whoami)"

echo -e "\n=== 3. Test Thanos route directly ==="
THANOS_URL=$(oc get route thanos-querier -n openshift-monitoring -o jsonpath='{.spec.host}' 2>/dev/null)
if [ -n "$THANOS_URL" ]; then
  echo "Thanos Route: https://$THANOS_URL"
  curl -sk -L -w "\nHTTP Code: %{http_code}\n" "https://$THANOS_URL" -o /dev/null
else
  echo "No Thanos route exposed externally"
fi

echo -e "\n=== 4. Check if monitoring routes exist ==="
oc get routes -n openshift-monitoring
oc get routes -n openshift-user-workload-monitoring

echo -e "\n=== 5. Check OAuth configuration ==="
oc get pod -n openshift-monitoring -l app.kubernetes.io/name=thanos-query -o yaml | grep -A5 "oauth-proxy"








#!/bin/bash
# Diagnostic script for OSSM3 monitoring issues

echo "=== 1. Check if monitoring pods have Istio sidecars (CRITICAL) ==="
echo "Prometheus pods:"
oc get pods -n openshift-monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].name}{"\n"}{end}'

echo -e "\nThanos Querier pods:"
oc get pods -n openshift-monitoring -l app.kubernetes.io/name=thanos-query -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].name}{"\n"}{end}'

echo -e "\nUser Workload Prometheus pods:"
oc get pods -n openshift-user-workload-monitoring -l app.kubernetes.io/name=prometheus -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.containers[*].name}{"\n"}{end}'

echo -e "\n=== 2. Check namespace labels (sidecar injection triggers) ==="
oc get ns openshift-monitoring -o jsonpath='{.metadata.labels}' | jq
oc get ns openshift-user-workload-monitoring -o jsonpath='{.metadata.labels}' | jq

echo -e "\n=== 3. Check your Istio discoverySelectors ==="
oc get istio -A -o jsonpath='{range .items[*]}{"Istio: "}{.metadata.name}{" in namespace: "}{.metadata.namespace}{"\n"}{"  discoverySelectors: "}{.spec.values.meshConfig.discoverySelectors}{"\n\n"}{end}'

echo -e "\n=== 4. Test Thanos Querier access directly ==="
THANOS_ROUTE=$(oc get route thanos-querier -n openshift-monitoring -o jsonpath='{.spec.host}' 2>/dev/null)
if [ -n "$THANOS_ROUTE" ]; then
  echo "Testing: https://$THANOS_ROUTE"
  curl -sk -w "\nHTTP Code: %{http_code}\n" "https://$THANOS_ROUTE/api/v1/query?query=up" -o /dev/null
else
  echo "No Thanos route found - testing internal service"
  TOKEN=$(oc create token prometheus-k8s -n openshift-monitoring --duration=10m)
  PROM_POD=$(oc get pod -n openshift-user-workload-monitoring -l app.kubernetes.io/name=prometheus -o name | head -1)
  oc exec -n openshift-user-workload-monitoring $PROM_POD -c prometheus -- \
    curl -sk -w "\nHTTP Code: %{http_code}\n" \
    -H "Authorization: Bearer $TOKEN" \
    "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query?query=up"
fi

echo -e "\n=== 5. Check for mesh-wide PeerAuthentication policies ==="
oc get peerauthentication -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,MODE:.spec.mtls.mode,SELECTOR:.spec.selector

echo -e "\n=== 6. Check for AuthorizationPolicy blocking monitoring ==="
oc get authorizationpolicy -A -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,ACTION:.spec.action





apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: mesh-metrics-permissive
  namespace: istio-system  # Root namespace for mesh policies
  annotations:
    argocd.argoproj.io/sync-wave: "5"
spec:
  # No selector = applies to entire mesh
  mtls:
    mode: PERMISSIVE
  portLevelMtls:
    15020:
      mode: DISABLE
    15090:
      mode: DISABLE

apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: mesh-allow-prometheus
  namespace: istio-system
  annotations:
    argocd.argoproj.io/sync-wave: "10"
spec:
  action: ALLOW
  rules:
    - from:
        - source:
            namespaces:
              - openshift-monitoring
              - openshift-user-workload-monitoring
      to:
        - operation:
            ports: ["15020", "15090", "15014"]

#!/bin/bash

echo "=== Show mesh-wide policies (apply to ALL revisions) ==="
oc get peerauthentication mesh-metrics-permissive -n istio-system
oc get authorizationpolicy mesh-allow-prometheus -n istio-system

echo -e "\n=== Test metrics access for EACH revision ==="
for REV in default stable canary; do
  echo -e "\n--- Testing revision: ${REV} ---"
  
  # Check if this revision exists
  if ! oc get istio ${REV} -n istio-system &>/dev/null; then
    echo "Revision ${REV} not found, skipping"
    continue
  fi
  
  # Get istiod pod for this revision
  ISTIOD_IP=$(oc get pod -n istio-system -l app=istiod,istio.io/rev=${REV} \
    -o jsonpath='{.items[0].status.podIP}' 2>/dev/null)
  
  if [ -z "$ISTIOD_IP" ]; then
    echo "No istiod pod found for ${REV}"
    continue
  fi
  
  # Test from Prometheus
  PROM_POD=$(oc get pod -n openshift-user-workload-monitoring \
    -l app.kubernetes.io/name=prometheus -o name | head -1)
  
  oc exec -n openshift-user-workload-monitoring ${PROM_POD} -c prometheus -- \
    curl -s -o /dev/null -w "Revision ${REV}: HTTP %{http_code}\n" \
    http://${ISTIOD_IP}:15014/metrics
done






---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tempostack-traces-write
rules:
  - apiGroups:
      - tempo.grafana.com
    resources:
      - all
    resourceNames:
      - traces
    verbs:
      - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tempostack-traces-write-otel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tempostack-traces-write
subjects:
  - kind: ServiceAccount
    name: otel-collector
    namespace: istio-system


---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: tempostack-traces-read
rules:
  - apiGroups:
      - tempo.grafana.com
    resources:
      - all
    resourceNames:
      - traces
    verbs:
      - get
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tempostack-traces-read-cluster-admins
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tempostack-traces-read
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:cluster-admins
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: cluster-admins


apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel
  namespace: istio-system
spec:
  observability:
    metrics:
      enableMetrics: true
  config:
    exporters:
      otlp/tempo:
        endpoint: tempo-tempo-blobstack-gateway.tracing-system.svc.cluster.local:8090
        tls:
          insecure: true
        headers:
          x-scope-orgid: all
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http: {}
    service:
      pipelines:
        traces:
          receivers: [otlp]
          exporters: [otlp/tempo]

oc label namespace openshift-tempo-operator openshift.io/cluster-monitoring=true --overwrite
oc label namespace openshift-cluster-observability-operator openshift.io/cluster-monitoring=true --overwrite
oc label namespace tracing-system openshift.io/cluster-monitoring=true --overwrite

# TempoStack status
oc get tempostack tempo-blobstack -n tracing-system

# Gateway pod must be running
oc get pods -n tracing-system -l app.kubernetes.io/component=gateway

# ServiceMonitors auto-created by observability.metrics
oc get servicemonitor -n tracing-system

# PrometheusRules auto-created
oc get prometheusrule -n tracing-system

# Jaeger route
oc get route -n tracing-system

# UIPlugin status
oc get uiplugin distributed-tracing -o yaml

# Generate some traces then check console
# Observe -> Traces -> select tempo-blobstack -> tenant: all

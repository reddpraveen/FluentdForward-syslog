# macOS
brew install yq
# or Linux
sudo wget -O /usr/local/bin/yq \
  https://github.com/mikefarah/yq/releases/latest/download/yq_$(uname -s | tr '[:upper:]' '[:lower:]')_amd64
sudo chmod +x /usr/local/bin/yq



# 1) Save the CLF to a file
oc get clusterlogforwarder instance -n openshift-logging -o yaml > clf.yaml

# 2) Normalize inputs so every item has the same keys as the first item
yq -oy '
  def empty_like(x):
    (x | type) as $t |
    if   $t == "!!map" then {} 
    elif $t == "!!seq" then [] 
    else null end;

  def ensure_keys($tmpl):
    reduce ($tmpl | keys)[] as $k (.;
      if has($k) then .
      else . *+ {($k): empty_like($tmpl[$k])}
      end
    );

  . as $root
  | $root.spec.inputs as $ins
  | $ins[0] as $first
  | $root
  | .spec.inputs = [ $ins[]
      # ensure top-level keys present like in the first item
      | ensure_keys($first)
      # and recursively for nested maps we know about (e.g. application)
      | (.application |= ( . // {} | ensure_keys($first.application // {}) ))
    ]
' clf.yaml > clf-normalized.yaml

# 3) Apply it back (review diff first!)
oc diff -f clf-normalized.yaml
oc apply -f clf-normalized.yaml




apiVersion: backstage.io/v1alpha1
kind: Group
metadata:
  name: developers
  namespace: default
  description: Development team group
spec:
  type: team
  profile:
    displayName: Developers
    email: developers@company.com
    picture: https://example.com/group-avatar.jpeg
  parent: engineering  # optional: parent group
  children: []         # optional: child groups
  members:
    - maskeen-singh1   # add your username here
    # - other-developer
    # - another-developer


# Complete Operator-Based RHDH Ansible Plugin Implementation
# Based on Red Hat Developer Learning Path - Converted from Helm to Operator

## Prerequisites
- RHDH Operator installed
- OpenShift cluster access
- Red Hat registry credentials
- GitHub authentication configured

---

# STEP 1: Download and Setup Plugin Registry
# ==========================================

# 1.1: Create namespace for plugin registry
apiVersion: v1
kind: Namespace
metadata:
  name: rhdh-plugins

---

# 1.2: Create ImageStream for plugin registry
apiVersion: image.openshift.io/v1
kind: ImageStream
metadata:
  name: rhdh-plugin-registry
  namespace: rhdh-plugins
spec:
  lookupPolicy:
    local: true

---

# 1.3: Create BuildConfig for plugin registry
apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  name: rhdh-plugin-registry-build
  namespace: rhdh-plugins
spec:
  output:
    to:
      kind: ImageStreamTag
      name: rhdh-plugin-registry:latest
  source:
    type: Dockerfile
    dockerfile: |
      FROM registry.access.redhat.com/ubi8/httpd-24:latest
      USER root
      
      # Create plugins directory
      RUN mkdir -p /var/www/html/plugins
      
      # Copy plugin files (these will be added via ConfigMap)
      COPY plugins/ /var/www/html/plugins/
      
      # Set permissions
      RUN chown -R 1001:0 /var/www/html && \
          chmod -R g+rw /var/www/html
      
      USER 1001
      EXPOSE 8080
  strategy:
    type: Docker
  triggers:
    - type: ConfigChange

---

# 1.4: ConfigMap containing plugin files (simulating downloaded plugins)
apiVersion: v1
kind: ConfigMap
metadata:
  name: ansible-plugin-files
  namespace: rhdh-plugins
data:
  # These represent the extracted plugin files from the bundle
  # In practice, you would download from access.redhat.com and extract
  backstage-plugin-ansible-backend-1.0.0.tgz.integrity: |
    sha512-REPLACE_WITH_ACTUAL_INTEGRITY_HASH_FROM_DOWNLOADED_BUNDLE
  module-backstage-rhaap-1.0.0.tgz.integrity: |
    sha512-REPLACE_WITH_ACTUAL_INTEGRITY_HASH_FROM_DOWNLOADED_BUNDLE
  ansible-plugin-backstage-rhaap-backend-1.0.0.tgz.integrity: |
    sha512-REPLACE_WITH_ACTUAL_INTEGRITY_HASH_FROM_DOWNLOADED_BUNDLE

---

# 1.5: Service for plugin registry
apiVersion: v1
kind: Service
metadata:
  name: rhdh-plugin-registry
  namespace: rhdh-plugins
spec:
  selector:
    app: rhdh-plugin-registry
  ports:
    - name: http
      port: 8080
      targetPort: 8080
  type: ClusterIP

---

# 1.6: Route for plugin registry
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: rhdh-plugin-registry
  namespace: rhdh-plugins
spec:
  to:
    kind: Service
    name: rhdh-plugin-registry
  port:
    targetPort: http
  tls:
    termination: edge

---

# STEP 2: Red Hat Registry Pull Secret
# ===================================

# 2.1: Secret for Red Hat registry access
apiVersion: v1
kind: Secret
metadata:
  name: redhat-registry-secret
  namespace: rhdh-system
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: |
    # Base64 encoded Docker config for registry.redhat.io
    # Replace with your actual Red Hat registry credentials
    # Format: {"auths":{"registry.redhat.io":{"username":"your-username","password":"your-token","auth":"base64-encoded-username:password"}}}
    # Use: echo -n '{"auths":{"registry.redhat.io":{"username":"USERNAME","password":"TOKEN","auth":"BASE64_USERNAME:TOKEN"}}}' | base64 -w 0
    REPLACE_WITH_BASE64_ENCODED_DOCKER_CONFIG

---

# STEP 3: Main Application ConfigMap (app-config-rhdh equivalent)
# ==============================================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config-rhdh-ansible
  namespace: rhdh-system
data:
  app-config.yaml: |
    app:
      title: Red Hat Developer Hub with Ansible
      baseUrl: ${BACKEND_URL}

    backend:
      baseUrl: ${BACKEND_URL}
      listen:
        port: 7007
      csp:
        connect-src: ["'self'", 'http:', 'https:']
      cors:
        origin: ${BACKEND_URL}

    # Authentication configuration
    auth:
      environment: development
      providers:
        github:
          development:
            clientId: ${GITHUB_CLIENT_ID}
            clientSecret: ${GITHUB_CLIENT_SECRET}

    # Integrations configuration
    integrations:
      github:
        - host: github.com
          token: ${GITHUB_TOKEN}

    # Ansible Configuration (equivalent to Helm ansible: section)
    ansible:
      # Dev Spaces integration
      devSpaces:
        baseUrl: ${DEV_SPACES_URL}
      
      # Ansible Automation Platform integration
      rhaap:
        baseUrl: ${AAP_BASE_URL}
        username: ${AAP_USERNAME}
        password: ${AAP_PASSWORD}
        
      # Private Automation Hub integration
      automationHub:
        baseUrl: ${PAH_BASE_URL}
        username: ${PAH_USERNAME}
        password: ${PAH_PASSWORD}

    # Catalog configuration for templates
    catalog:
      import:
        entityFilename: catalog-info.yaml
        pullRequestBranchName: backstage-integration
      rules:
        - allow: [Component, System, API, Resource, Location, Template]
      locations:
        # Ansible templates location
        - type: url
          target: https://github.com/your-org/ansible-templates/blob/main/catalog-info.yaml
          rules:
            - allow: [Template]

    # Plugin configuration
    dynamicPlugins:
      frontend:
        ansible.plugin-ansible:
          appIcons:
            - name: ansible
              importName: AnsibleIcon
          dynamicRoutes:
            - path: /ansible
              importName: AnsiblePage
              menuItem:
                icon: ansible
                text: Ansible

---

# STEP 4: Dynamic Plugins Configuration
# ====================================

apiVersion: v1
kind: ConfigMap
metadata:
  name: rhdh-dynamic-plugins-ansible
  namespace: rhdh-system
data:
  dynamic-plugins.yaml: |
    includes:
      - dynamic-plugins.default.yaml
    
    plugins:
      # Ansible backend plugin
      - package: 'http://rhdh-plugin-registry-rhdh-plugins.apps.your-cluster.com/plugins/backstage-plugin-ansible-backend-1.0.0.tgz'
        disabled: false
        integrity: 'sha512-REPLACE_WITH_ACTUAL_INTEGRITY_HASH'
        pluginConfig:
          ansible:
            baseUrl: ${AAP_BASE_URL}
            username: ${AAP_USERNAME}
            password: ${AAP_PASSWORD}
      
      # RHAAP module
      - package: 'http://rhdh-plugin-registry-rhdh-plugins.apps.your-cluster.com/plugins/module-backstage-rhaap-1.0.0.tgz'
        disabled: false
        integrity: 'sha512-REPLACE_WITH_ACTUAL_INTEGRITY_HASH'
      
      # Ansible RHAAP backend
      - package: 'http://rhdh-plugin-registry-rhdh-plugins.apps.your-cluster.com/plugins/ansible-plugin-backstage-rhaap-backend-1.0.0.tgz'
        disabled: false
        integrity: 'sha512-REPLACE_WITH_ACTUAL_INTEGRITY_HASH'
      
      # GitHub integration plugins
      - package: '@backstage/plugin-catalog-backend-module-github@0.2.0'
        disabled: false
      
      - package: '@backstage/plugin-scaffolder-backend-module-github@0.2.0'
        disabled: false

---

# STEP 5: Environment Variables Secret
# ===================================

apiVersion: v1
kind: Secret
metadata:
  name: rhdh-ansible-env-vars
  namespace: rhdh-system
type: Opaque
data:
  # Base64 encode all these values
  GITHUB_CLIENT_ID: REPLACE_WITH_BASE64_ENCODED_GITHUB_CLIENT_ID
  GITHUB_CLIENT_SECRET: REPLACE_WITH_BASE64_ENCODED_GITHUB_CLIENT_SECRET
  GITHUB_TOKEN: REPLACE_WITH_BASE64_ENCODED_GITHUB_TOKEN
  AAP_BASE_URL: REPLACE_WITH_BASE64_ENCODED_AAP_URL
  AAP_USERNAME: REPLACE_WITH_BASE64_ENCODED_AAP_USERNAME
  AAP_PASSWORD: REPLACE_WITH_BASE64_ENCODED_AAP_PASSWORD
  PAH_BASE_URL: REPLACE_WITH_BASE64_ENCODED_PAH_URL
  PAH_USERNAME: REPLACE_WITH_BASE64_ENCODED_PAH_USERNAME
  PAH_PASSWORD: REPLACE_WITH_BASE64_ENCODED_PAH_PASSWORD
  DEV_SPACES_URL: REPLACE_WITH_BASE64_ENCODED_DEV_SPACES_URL
  BACKEND_URL: REPLACE_WITH_BASE64_ENCODED_BACKEND_URL

---

# STEP 6: Main Backstage Instance Configuration
# =============================================

apiVersion: rhdh.redhat.com/v1alpha1
kind: Backstage
metadata:
  name: rhdh-ansible-instance
  namespace: rhdh-system
spec:
  application:
    # App configuration
    appConfig:
      configMaps:
        - name: app-config-rhdh-ansible
    
    # Environment variables
    extraEnvs:
      envs:
        - name: BACKEND_URL
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: BACKEND_URL
        - name: GITHUB_CLIENT_ID
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: GITHUB_CLIENT_ID
        - name: GITHUB_CLIENT_SECRET
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: GITHUB_CLIENT_SECRET
        - name: GITHUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: GITHUB_TOKEN
        - name: AAP_BASE_URL
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: AAP_BASE_URL
        - name: AAP_USERNAME
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: AAP_USERNAME
        - name: AAP_PASSWORD
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: AAP_PASSWORD
        - name: PAH_BASE_URL
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: PAH_BASE_URL
        - name: PAH_USERNAME
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: PAH_USERNAME
        - name: PAH_PASSWORD
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: PAH_PASSWORD
        - name: DEV_SPACES_URL
          valueFrom:
            secretKeyRef:
              name: rhdh-ansible-env-vars
              key: DEV_SPACES_URL
    
    # Image pull secrets for Red Hat registry
    imagePullSecrets:
      - redhat-registry-secret
    
    # Ansible Dev Tools container (equivalent to Helm extraContainers)
    extraContainers:
      - name: ansible-devtools-server
        image: registry.redhat.io/ansible-automation-platform-25/ansible-dev-tools-rhel8:latest
        imagePullPolicy: IfNotPresent
        command:
          - "adt"
          - "server"
        ports:
          - name: http
            containerPort: 8000
            protocol: TCP
        env:
          - name: ADT_HOST
            value: "0.0.0.0"
          - name: ADT_PORT
            value: "8000"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
        volumeMounts:
          - name: ansible-workspace
            mountPath: /workspace
    
    # Extra volumes
    extraVolumes:
      - name: ansible-workspace
        emptyDir: {}
    
    # Dynamic plugins configuration
    dynamicPluginsConfigMapName: rhdh-dynamic-plugins-ansible
    
  # Database configuration
  database:
    enableLocalDb: true

---

# STEP 7: RBAC Configuration
# =========================

apiVersion: v1
kind: ServiceAccount
metadata:
  name: rhdh-ansible-sa
  namespace: rhdh-system

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: rhdh-ansible-reader
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["build.openshift.io"]
  resources: ["builds", "buildconfigs"]
  verbs: ["get", "list", "watch", "create"]
- apiGroups: ["image.openshift.io"]
  resources: ["imagestreams"]
  verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: rhdh-ansible-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rhdh-ansible-reader
subjects:
- kind: ServiceAccount
  name: rhdh-ansible-sa
  namespace: rhdh-system

---

# STEP 8: Ansible Templates Repository Structure
# =============================================

# This is the structure you need in your GitHub repository for templates

# catalog-info.yaml (root of your ansible-templates repository)
apiVersion: v1
kind: ConfigMap
metadata:
  name: ansible-templates-catalog
  namespace: rhdh-system
data:
  catalog-info.yaml: |
    apiVersion: backstage.io/v1alpha1
    kind: Location
    metadata:
      name: ansible-templates
      description: Ansible playbook and collection templates
    spec:
      targets:
        - ./templates/ansible-playbook/template.yaml
        - ./templates/ansible-collection/template.yaml

---

# IMPLEMENTATION STEPS SCRIPT
# ===========================

apiVersion: v1
kind: ConfigMap
metadata:
  name: implementation-steps
  namespace: rhdh-system
data:
  deploy.sh: |
    #!/bin/bash
    
    echo "=== RHDH Ansible Plugin Implementation Steps ==="
    
    # Step 1: Create namespaces
    echo "Step 1: Creating namespaces..."
    kubectl create namespace rhdh-plugins --dry-run=client -o yaml | kubectl apply -f -
    kubectl create namespace rhdh-system --dry-run=client -o yaml | kubectl apply -f -
    
    # Step 2: Download Ansible plugins (manual step)
    echo "Step 2: MANUAL - Download ansible-backstage-rhaap-bundle from access.redhat.com"
    echo "Extract and note the integrity hashes from *.integrity files"
    
    # Step 3: Update integrity hashes
    echo "Step 3: Update integrity hashes in dynamic-plugins.yaml ConfigMap"
    
    # Step 4: Create Red Hat registry secret
    echo "Step 4: Create Red Hat registry secret..."
    echo "kubectl create secret docker-registry redhat-registry-secret \\"
    echo "  --docker-server=registry.redhat.io \\"
    echo "  --docker-username=YOUR_USERNAME \\"
    echo "  --docker-password=YOUR_TOKEN \\"
    echo "  -n rhdh-system"
    
    # Step 5: Apply all configurations
    echo "Step 5: Applying configurations..."
    kubectl apply -f ansible-plugin-files.yaml
    kubectl apply -f plugin-registry-build.yaml
    kubectl apply -f app-config-rhdh-ansible.yaml
    kubectl apply -f rhdh-dynamic-plugins-ansible.yaml
    kubectl apply -f rhdh-ansible-env-vars.yaml
    kubectl apply -f rhdh-ansible-instance.yaml
    kubectl apply -f rbac-config.yaml
    
    # Step 6: Trigger plugin registry build
    echo "Step 6: Triggering plugin registry build..."
    oc start-build rhdh-plugin-registry-build -n rhdh-plugins
    
    # Step 7: Wait for deployment
    echo "Step 7: Waiting for deployment..."
    kubectl wait --for=condition=Available deployment/rhdh-ansible-instance-backstage-backend -n rhdh-system --timeout=600s
    
    # Step 8: Get route URL
    echo "Step 8: Getting RHDH URL..."
    echo "RHDH URL: $(oc get route rhdh-ansible-instance -n rhdh-system -o jsonpath='{.spec.host}')"
    
    echo "=== Installation Complete ==="
    echo "Next steps:"
    echo "1. Access RHDH using the URL above"
    echo "2. Sign out and sign in with GitHub"
    echo "3. Navigate to Create -> Browse templates"
    echo "4. Verify Ansible templates are available"

  verify.sh: |
    #!/bin/bash
    
    echo "=== Verification Steps ==="
    
    # Check plugin registry
    echo "Plugin registry status:"
    kubectl get pods -n rhdh-plugins
    kubectl get route -n rhdh-plugins
    
    # Check RHDH pods
    echo "RHDH pods status:"
    kubectl get pods -n rhdh-system
    
    # Check logs
    echo "RHDH backend logs:"
    kubectl logs -n rhdh-system deployment/rhdh-ansible-instance-backstage-backend --tail=50
    
    # Check ansible dev tools container
    echo "Ansible dev tools status:"
    kubectl exec -n rhdh-system deployment/rhdh-ansible-instance-backstage-backend -c ansible-devtools-server -- ps aux
    
    echo "=== Verification Complete ==="


























-----------------------------------

#!/bin/bash
# Check CTK for VMs in specific folder
CLUSTER_NAME="ocp4prd-dfg"
VM_FOLDER="/ddd/vm/$CLUSTER_NAME"

echo "=== Checking CTK for VMs in folder: $VM_FOLDER ==="
echo ""

govc find "$VM_FOLDER" -type m | while read vm; do
    echo "=== $vm ==="
    govc vm.info -e "$vm" | grep -E "(ctkEnabled|scsi.*ctkEnabled)"
    echo ""
done

#!/bin/bash
# Check CTK for specific cluster
CLUSTER_NAME="ocp4prd-dfg"

govc find . -type m | while read vm; do
    cluster=$(govc vm.info "$vm" | grep "Cluster:" | cut -d: -f2 | tr -d ' ')
    if [[ "$cluster" == "$CLUSTER_NAME" ]]; then
        echo "=== $vm ==="
        govc vm.info -e "$vm" | grep -E "(ctkEnabled|scsi.*ctkEnabled)"
        echo ""
    fi
done


kind: ConfigMap
apiVersion: v1
metadata:
  name: app-config-rhdh
  namespace: demo-project #project-namespace
data:
  app-config-rhdh.yaml: |
    signInPage: guest
    app:
      title: My Red Hat Developer Hub Instance
      baseUrl: ${baseUrl}  # base url is coming from the 'secrets_rdhd-secret.yaml' config in 'setting-up-developer-hub-through-the-operator'
    backend:
      auth:
        keys:
          - secret: ${BACKEND_SECRET}  # BACKEND_SECRET is coming from the 'secrets_rdhd-secret.yaml' config in 'setting-up-developer-hub-through-the-operator'
      baseUrl: ${baseUrl}   # base url is coming from the 'secrets_rdhd-secret.yaml' config in 'setting-up-developer-hub-through-the-operator'
      cors:
        origin: ${baseUrl}   # base url is coming from the 'secrets_rdhd-secret.yaml' config in 'setting-up-developer-hub-through-the-operator'
    integrations:
      github:
        - host: github.com
          #token: <github personal access token>
          apps:
            - appId: ${RHDH_GITHUB_APP_ID}
              clientId: ${RHDH_GITHUB_APP_CLIENT_ID}
              clientSecret: ${RHDH_GITHUB_APP_CLIENT_SECRET}
              webhookUrl: none
              webhookSecret: none
              privateKey: ${RHDH_GITHUB_APP_PRIVATE_KEY}
    catalog:
      providers:
        github:
          # the provider ID can be any camelCase string
          providerId:
            organization: ${RHDH_GITHUB_ORGANIZATION} # the name of the GitHub organization
            catalogPath: '/catalog-info.yaml' # the path where your catalog info file will be placed within projects than need to be scanned
            filters:
              branch: 'master' # string
              repository: '.*' # Regex
            schedule: # optional; same options as in TaskScheduleDefinition
              # supports cron, ISO duration, "human duration" as used in code
              frequency: { minutes: 1 }
              # supports ISO duration, "human duration" as used in code
              timeout: { minutes: 1 }
              initialDelay: { seconds: 15 }
    auth:
      environment: development
      session:
        secret: ${BACKEND_SECRET}
      providers:
        # allow guest authentication for now
        guest:
          dangerouslyAllowOutsideDevelopment: true








apiVersion: v1
kind: ConfigMap
metadata:
  name: my-backstage-config-backend-base-urls
data:
  "app-config.backend-base-urls.yaml": |
    #app:
      # As of 0.6 (RHDH 1.6), this is not needed on OCP by default, but needed on other platforms
    #  baseUrl: https://my-rhdh.example.com
    #backend:
    #  # As of 0.6 (RHDH 1.6), this is not needed on OCP by default, but needed on other platforms
    #  baseUrl: https://my-rhdh.example.com
    #  cors:
    #    # As of 0.6 (RHDH 1.6), this is not needed on OCP by default, but needed on other platforms
    #    origin: https://my-rhdh.example.com

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-backstage-config-backend-auth
data:
  "app-config.backend-auth.yaml": |
    backend:
      auth:
        externalAccess:
          - type: legacy
            options:
              subject: legacy-default-config
              secret: "${BACKEND_SECRET}"
    auth:
      environment: development
      providers:
        guest:
          # using the guest user to query the '/api/dynamic-plugins-info/loaded-plugins' endpoint.
          dangerouslyAllowOutsideDevelopment: true

---
apiVersion: v1
kind: Secret
metadata:
  name: my-backstage-backend-auth-secret
stringData:
  # generated with the command below (from https://backstage.io/docs/auth/service-to-service-auth/#setup):
  # node -p 'require("crypto").randomBytes(24).toString("base64")'
  BACKEND_SECRET: "R2FxRVNrcmwzYzhhN3l0V1VRcnQ3L1pLT09WaVhDNUEK" # notsecret

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-backstage-config-cm1
data:
  app-config1-cm1.db.yaml: |
    backend:
      database:
        connection:
          password: ${POSTGRESQL_PASSWORD}
          user: ${POSTGRESQL_USER}
  app-config2-cm1.yaml: |
    # Some comment in this file
  app-config3-cm1.odo.yaml: |
    catalog:
      locations:
        - type: url
          target: https://github.com/ododev/odo-backstage-software-template/blob/main/template.yaml
          rules:
            - allow: [Template]
    # # catalog.providers.githubOrg.default.orgUrl

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-backstage-config-cm2
data:
  app-config1-cm2.gh.yaml: |
    auth:
      # see https://backstage.io/docs/auth/ to learn about auth providers
      environment: development
      providers:
        github:
          development:
            clientId: '${GH_CLIENT_ID}'
            clientSecret: '${GH_CLIENT_SECRET}'
  app-config2-cm2.yaml: |
    # a comment

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-dynamic-plugins-config-cm
data:
  dynamic-plugins.yaml: |
    includes:
      - dynamic-plugins.default.yaml
    plugins:
      - package: ./dynamic-plugins/dist/roadiehq-scaffolder-backend-module-utils-dynamic
        disabled: false
      - package: './dynamic-plugins/dist/backstage-plugin-catalog-backend-module-github-dynamic'
        disabled: false
        pluginConfig:
          catalog:
            providers:
              github:
                myorg:
                  organization: '${GITHUB_ORG}'
                  schedule:
                    # supports cron, ISO duration, "human duration" (used below)
                    frequency: { minutes: 30}
                    # supports ISO duration, "human duration (used below)
                    timeout: { minutes: 3}
                    initialDelay: { seconds: 15}
      - package: ./dynamic-plugins/dist/backstage-plugin-techdocs-backend-dynamic
        disabled: false
        pluginConfig:
          # Reference documentation https://backstage.io/docs/features/techdocs/configuration
          # Note: After experimenting with basic setup, use CI/CD to generate docs
          # and an external cloud storage when deploying TechDocs for production use-case.
          # https://backstage.io/docs/features/techdocs/how-to-guides#how-to-migrate-from-techdocs-basic-to-recommended-deployment-approach
          techdocs:
            builder: local
            generator:
              runIn: local
            publisher:
              type: local
      - package: ./dynamic-plugins/dist/backstage-plugin-catalog-backend-module-gitlab-dynamic
        disabled: false
        pluginConfig:
          catalog:
            providers:
              gitlab: {}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-env-cm-1
data:
  CM_ENV1: "cm env 1"
  CM_ENV2: "cm env 2"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-env-cm-11
data:
  CM_ENV11: "cm env 11"
  CM_ENV12: "cm env 12"

---
apiVersion: v1
kind: Secret
metadata:
  name: my-gh-auth-secret
stringData:
  GH_CLIENT_ID: "my GH client ID"
  GH_CLIENT_SECRET: "my GH client secret"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-backstage-extra-files-cm1
data:
  cm_file1.txt: |
    # From ConfigMap
    Lorem Ipsum
    Dolor Sit Amet
  cm_file2.properties: |
    conf.x=y
    conf.y=z

---
apiVersion: v1
kind: Secret
metadata:
  name: my-backstage-extra-files-secret1
stringData:
  secret_file1.txt: |
    # From Secret
    Lorem Ipsum
    Dolor Sit Amet
  secret_file2.properties: |
    sec.a=b
    sec.b=c
  secrets.prod.yaml: |
    appId: 1
    webhookUrl: https://smee.io/foo
    clientId: someGithubAppClientId
    clientSecret: someGithubAppClientSecret
    webhookSecret: someWebhookSecret
    privateKey: |
      SomeRsaPrivateKey

---
apiVersion: rhdh.redhat.com/v1alpha4
kind: Backstage
metadata:
  name: bs-app-config
spec:
  database:
    enableLocalDb: true
  deployment:
    patch:
      spec:
        replicas: 1
  application:
    appConfig:
      configMaps:
        - name: "my-backstage-config-backend-base-urls"
        - name: "my-backstage-config-backend-auth"
        - name: "my-backstage-config-cm1"
        - name: "my-backstage-config-cm2"
          key: "app-config1-cm2.gh.yaml"
    dynamicPluginsConfigMapName: "my-dynamic-plugins-config-cm"
    extraFiles:
      mountPath: /tmp/my-extra-files
      configMaps:
        - name: "my-backstage-extra-files-cm1"
      secrets:
        - name: "my-backstage-extra-files-secret1"
          key: secret_file1.txt
    extraEnvs:
      envs:
        - name: GITHUB_ORG
          value: 'my-gh-org'
        - name: MY_ENV_VAR_2
          value: my-value-2
      configMaps:
        - name: my-env-cm-1
        - name: my-env-cm-11
          key: CM_ENV11
      secrets:
        - name: "my-backstage-backend-auth-secret"
          key: BACKEND_SECRET
        - name: my-gh-auth-secret





First, Extract Credentials from OpenShift Secret
bash# Extract vCenter credentials from OpenShift secret
export GOVC_URL=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.server}' | base64 -d)
export GOVC_USERNAME=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.username}' | base64 -d)
export GOVC_PASSWORD=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.password}' | base64 -d)
export GOVC_INSECURE=true  # Usually needed for OpenShift vSphere setups

# Verify connection
govc about
Alternative: One-liner Setup
bash# Set all govc environment variables in one command
eval $(oc get secret vsphere-creds -n openshift-config -o json | jq -r '.data | to_entries[] | "export GOVC_" + (.key | ascii_upcase | gsub("\\."; "_")) + "=" + (.value | @base64d)')
export GOVC_INSECURE=true
Check VMs Missing CTK Options
bash# List all VMs and check for CTK configuration
echo "=== VMs Missing CTK Configuration ==="
govc find . -type m | while read vm; do
    vm_name=$(basename "$vm")
    echo "Checking: $vm_name"
    
    # Get extra config for the VM
    ctk_main=$(govc vm.info -e "$vm" 2>/dev/null | grep "^ctkEnabled:" | cut -d: -f2 | tr -d ' ')
    ctk_scsi=$(govc vm.info -e "$vm" 2>/dev/null | grep "^scsi0:0.ctkEnabled:" | cut -d: -f2 | tr -d ' ')
    
    # Check if CTK is missing or not set to TRUE
    if [[ "$ctk_main" != "TRUE" ]] || [[ "$ctk_scsi" != "TRUE" ]]; then
        echo "❌ VM: $vm_name"
        echo "   ctkEnabled: ${ctk_main:-MISSING}"
        echo "   scsi0:0.ctkEnabled: ${ctk_scsi:-MISSING}"
        echo "   ---"
    fi
done
More Comprehensive CTK Check Script
bash#!/bin/bash
# Script: check_ctk_status.sh

# Setup govc credentials from OpenShift
setup_govc() {
    echo "Setting up govc credentials from OpenShift secret..."
    export GOVC_URL=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.server}' | base64 -d)
    export GOVC_USERNAME=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.username}' | base64 -d)
    export GOVC_PASSWORD=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.password}' | base64 -d)
    export GOVC_INSECURE=true
    
    # Test connection
    if ! govc about >/dev/null 2>&1; then
        echo "Failed to connect to vCenter"
        exit 1
    fi
    echo "Connected to vCenter: $GOVC_URL"
}

# Check CTK status for all VMs
check_ctk_status() {
    echo -e "\n=== CTK Status Report ==="
    echo -e "VM Name\t\t\t\tctkEnabled\tscsi0:0.ctkEnabled\tStatus"
    echo -e "-------\t\t\t\t----------\t----------------\t------"
    
    govc find . -type m | while read vm; do
        vm_name=$(basename "$vm")
        
        # Get VM extra configuration
        vm_config=$(govc vm.info -e "$vm" 2>/dev/null)
        ctk_main=$(echo "$vm_config" | grep "^ctkEnabled:" | cut -d: -f2 | tr -d ' ')
        ctk_scsi=$(echo "$vm_config" | grep "^scsi0:0.ctkEnabled:" | cut -d: -f2 | tr -d ' ')
        
        # Determine status
        if [[ "$ctk_main" == "TRUE" ]] && [[ "$ctk_scsi" == "TRUE" ]]; then
            status="✅ ENABLED"
        elif [[ -n "$ctk_main" ]] || [[ -n "$ctk_scsi" ]]; then
            status="⚠️  PARTIAL"
        else
            status="❌ MISSING"
        fi
        
        printf "%-30s\t%-10s\t%-16s\t%s\n" \
            "$vm_name" \
            "${ctk_main:-MISSING}" \
            "${ctk_scsi:-MISSING}" \
            "$status"
    done
}

# Main execution
setup_govc
check_ctk_status
Filter for OpenShift Worker Nodes Only
bash# Check CTK status specifically for OpenShift worker nodes
echo "=== OpenShift Worker Nodes CTK Status ==="
govc find . -type m | grep -E "(worker|rhcos)" | while read vm; do
    vm_name=$(basename "$vm")
    
    # Get power state
    power_state=$(govc vm.info "$vm" | grep "Power state:" | cut -d: -f2 | tr -d ' ')
    
    # Get CTK configuration
    ctk_main=$(govc vm.info -e "$vm" 2>/dev/null | grep "^ctkEnabled:" | cut -d: -f2 | tr -d ' ')
    ctk_scsi=$(govc vm.info -e "$vm" 2>/dev/null | grep "^scsi0:0.ctkEnabled:" | cut -d: -f2 | tr -d ' ')
    
    if [[ "$ctk_main" != "TRUE" ]] || [[ "$ctk_scsi" != "TRUE" ]]; then
        echo "Worker Node: $vm_name (Power: $power_state)"
        echo "  ctkEnabled: ${ctk_main:-MISSING}"
        echo "  scsi0:0.ctkEnabled: ${ctk_scsi:-MISSING}"
        echo "  Host: $(govc vm.info "$vm" | grep "Host:" | cut -d: -f2 | tr -d ' ')"
        echo "  ---"
    fi
done
Export Results to File
bash# Create detailed report and save to file
{
    echo "CTK Status Report - $(date)"
    echo "=================================="
    echo ""
    
    govc find . -type m | while read vm; do
        vm_name=$(basename "$vm")
        vm_info=$(govc vm.info "$vm")
        vm_config=$(govc vm.info -e "$vm" 2>/dev/null)
        
        power_state=$(echo "$vm_info" | grep "Power state:" | cut -d: -f2 | tr -d ' ')
        host=$(echo "$vm_info" | grep "Host:" | cut -d: -f2 | tr -d ' ')
        ctk_main=$(echo "$vm_config" | grep "^ctkEnabled:" | cut -d: -f2 | tr -d ' ')
        ctk_scsi=$(echo "$vm_config" | grep "^scsi0:0.ctkEnabled:" | cut -d: -f2 | tr -d ' ')
        
        if [[ "$ctk_main" != "TRUE" ]] || [[ "$ctk_scsi" != "TRUE" ]]; then
            echo "VM: $vm_name"
            echo "  Power State: $power_state"
            echo "  Host: $host"
            echo "  ctkEnabled: ${ctk_main:-MISSING}"
            echo "  scsi0:0.ctkEnabled: ${ctk_scsi:-MISSING}"
            echo ""
        fi
    done
} > ctk_missing_report.txt

echo "Report saved to ctk_missing_report.txt"
Quick Commands Summary
bash# 1. Setup credentials
export GOVC_URL=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.server}' | base64 -d)
export GOVC_USERNAME=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.username}' | base64 -d)
export GOVC_PASSWORD=$(oc get secret vsphere-creds -n openshift-config -o jsonpath='{.data.vcenter\.password}' | base64 -d)
export GOVC_INSECURE=true

# 2. Test connection
govc about

# 3. Quick CTK check
govc find . -type m | xargs -I {} sh -c 'echo "=== {} ==="; govc vm.info -e "{}" | grep -E "(ctkEnabled|scsi.*ctkEnabled)"'
Note: Make sure you have:

oc CLI tool configured and logged into your OpenShift cluster
govc CLI tool installed
jq and base64 utilities available
Proper permissions to access the vsphere-creds secret in the openshift-config namespace

The secret is typically in openshift-config namespace, but if it's in a different namespace in your setup, adjust the namespace accordingly.


cat > 12-performance-snapshot-updated.sh << 'EOF'
#!/bin/bash
# OpenShift 4.16 Master Node Performance Snapshot Script
# Red Hat CoP - Prometheus for etcd, oc CLI timing for API server
# Also includes Prometheus-based API server metrics as alternative

set -euo pipefail

OUTPUT_DIR=${OUTPUT_DIR:-"/tmp/openshift-performance-snapshots"}
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
SNAPSHOT_TYPE=${SNAPSHOT_TYPE:-"baseline"}
LOG_FILE="$OUTPUT_DIR/performance-snapshot-$TIMESTAMP.log"
CURL_TIMEOUT=${CURL_TIMEOUT:-5}

mkdir -p "$OUTPUT_DIR"

log() { echo "[$(date -u +%Y-%m-%dT%H:%M:%SZ)] $1" | tee -a "$LOG_FILE"; }
log_info() { log "INFO: $1"; }
log_error() { log "ERROR: $1"; }

# ---------- Prometheus helpers (Thanos Querier with CA) ----------
get_thanos_host() {
  oc -n openshift-monitoring get route thanos-querier -o jsonpath='{.spec.host}' 2>/dev/null || true
}

# Cache the ingress CA bundle to a temp file
get_ingress_ca() {
  local ca_file="${INGRESS_CA_FILE:-}"
  if [[ -n "$ca_file" && -s "$ca_file" ]]; then
    echo "$ca_file"; return 0
  fi
  ca_file="$(mktemp)"
  # default-ingress-cert is the cluster-wide ingress CA
  if oc get configmap default-ingress-cert -n openshift-config-managed -o jsonpath='{.data.ca-bundle\.crt}' > "$ca_file" 2>/dev/null; then
    echo "$ca_file"; return 0
  fi
  # fallback to openshift-ingress-operator
  if oc get configmap router-ca -n openshift-ingress-operator -o jsonpath='{.data.tls\.crt}' > "$ca_file" 2>/dev/null; then
    echo "$ca_file"; return 0
  fi
  rm -f "$ca_file"
  echo ""; return 1
}

# Run a PromQL query and return the value (or n/a)
prom_query() {
  local q="$1"
  local thanos_host="${THANOS_HOST:-}"
  local token="${TOKEN:-}"
  local ca_file="${INGRESS_CA_FILE:-}"
  
  if [[ -z "$thanos_host" || -z "$token" ]]; then
    echo "n/a"; return 0
  fi
  
  local url="https://${thanos_host}/api/v1/query?query=$(python3 -c "import urllib.parse; print(urllib.parse.quote('''${q}'''))")"
  local curl_opts=(-sS -H "Authorization: Bearer $token" --connect-timeout "$CURL_TIMEOUT" --max-time "$CURL_TIMEOUT")
  
  if [[ -n "$ca_file" && -s "$ca_file" ]]; then
    curl_opts+=(--cacert "$ca_file")
  else
    curl_opts+=(--insecure)
  fi
  
  local result
  result="$(curl "${curl_opts[@]}" "$url" 2>/dev/null | jq -r '.data.result[0].value[1] // "n/a"' 2>/dev/null || echo "n/a")"
  
  if [[ "$result" == "null" || "$result" == "" ]]; then
    echo "n/a"
  else
    echo "$result"
  fi
}

# ---------- ETCD metrics (Prometheus) ----------
collect_etcd_metrics() {
  log_info "Collecting ETCD metrics (Prometheus)"
  
  local wal_fsync_p99 db_size leader_changes_per_hour
  
  # ETCD WAL fsync p99 latency
  wal_fsync_p99="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])))')"
  
  # ETCD DB size
  db_size="$(prom_query 'etcd_mvcc_db_total_size_in_bytes')"
  
  # ETCD leader changes per hour (sum across all etcd instances)
  leader_changes_per_hour="$(prom_query 'sum(rate(etcd_server_leader_changes_seen_total[1h]))')"
  
  # Write to JSON
  cat > "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "etcd": {
    "wal_fsync_p99_seconds": "$wal_fsync_p99",
    "db_size_bytes": "$db_size",
    "leader_changes_per_hour": "$leader_changes_per_hour"
  }
}
EOF
  
  log_info "ETCD metrics collected: WAL fsync p99=$wal_fsync_p99, DB size=$db_size, leader changes/h=$leader_changes_per_hour"
}

# ---------- API server metrics (oc CLI timing) ----------
collect_api_metrics_cli() {
  log_info "Collecting API server metrics (oc CLI timing)"
  
  local get_p99 list_p99 watch_p99 overall_p99
  
  # Measure API response times using oc CLI
  log_info "Measuring API response times..."
  
  # GET operations
  local get_times=()
  for i in {1..10}; do
    local start_time=$(date +%s.%N)
    oc get nodes --no-headers >/dev/null 2>&1
    local end_time=$(date +%s.%N)
    local duration=$(echo "$end_time - $start_time" | bc -l)
    get_times+=("$duration")
  done
  
  # LIST operations
  local list_times=()
  for i in {1..10}; do
    local start_time=$(date +%s.%N)
    oc get pods --all-namespaces --no-headers >/dev/null 2>&1
    local end_time=$(date +%s.%N)
    local duration=$(echo "$end_time - $start_time" | bc -l)
    list_times+=("$duration")
  done
  
  # WATCH operations (simulate with timeout)
  local watch_times=()
  for i in {1..5}; do
    local start_time=$(date +%s.%N)
    timeout 2s oc get pods --watch --no-headers >/dev/null 2>&1 || true
    local end_time=$(date +%s.%N)
    local duration=$(echo "$end_time - $start_time" | bc -l)
    watch_times+=("$duration")
  done
  
  # Calculate p99 for each operation type
  get_p99=$(printf '%s\n' "${get_times[@]}" | sort -n | tail -1)
  list_p99=$(printf '%s\n' "${list_times[@]}" | sort -n | tail -1)
  watch_p99=$(printf '%s\n' "${watch_times[@]}" | sort -n | tail -1)
  
  # Overall p99 (max of all operations)
  overall_p99=$(printf '%s\n' "$get_p99" "$list_p99" "$watch_p99" | sort -n | tail -1)
  
  # Write to JSON
  cat > "$OUTPUT_DIR/api-metrics-cli-$TIMESTAMP.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "api_server": {
    "get_p99_seconds": "$get_p99",
    "list_p99_seconds": "$list_p99",
    "watch_p99_seconds": "$watch_p99",
    "overall_p99_seconds": "$overall_p99"
  }
}
EOF
  
  log_info "API metrics (CLI) collected: GET p99=$get_p99, LIST p99=$list_p99, WATCH p99=$watch_p99, Overall p99=$overall_p99"
}

# ---------- API server metrics (Prometheus) - Alternative method ----------
collect_api_metrics_prometheus() {
  log_info "Collecting API server metrics (Prometheus)"
  
  local overall_p99 get_p99 list_p99 watch_p99
  
  # Overall API server p99 (all verbs combined) - primary metric
  overall_p99="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket[5m])))')"
  
  # Individual verbs (best effort, longer windows for sparse operations)
  get_p99="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="GET"}[5m])))')"
  list_p99="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="LIST"}[5m])))')"
  watch_p99="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="WATCH"}[5m])))')"
  
  # Write to JSON
  cat > "$OUTPUT_DIR/api-metrics-prometheus-$TIMESTAMP.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "api_server": {
    "overall_p99_seconds": "$overall_p99",
    "get_p99_seconds": "$get_p99",
    "list_p99_seconds": "$list_p99",
    "watch_p99_seconds": "$watch_p99"
  }
}
EOF
  
  log_info "API metrics (Prometheus) collected: Overall p99=$overall_p99, GET p99=$get_p99, LIST p99=$list_p99, WATCH p99=$watch_p99"
}

# ---------- Resource counts ----------
collect_resource_counts() {
  log_info "Collecting resource counts"
  
  local total_configmaps total_secrets configmaps_application secrets_application
  
  # Total ConfigMaps and Secrets
  total_configmaps=$(oc get configmaps --all-namespaces --no-headers | wc -l)
  total_secrets=$(oc get secrets --all-namespaces --no-headers | wc -l)
  
  # Application namespace ConfigMaps and Secrets (exclude system namespaces)
  configmaps_application=$(oc get configmaps --all-namespaces --no-headers | grep -v -E '^(openshift-|kube-|default)' | wc -l)
  secrets_application=$(oc get secrets --all-namespaces --no-headers | grep -v -E '^(openshift-|kube-|default)' | wc -l)
  
  # Write to JSON
  cat > "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "resources": {
    "total_configmaps": $total_configmaps,
    "total_secrets": $total_secrets,
    "configmaps_application_namespaces": $configmaps_application,
    "secrets_application_namespaces": $secrets_application
  }
}
EOF
  
  log_info "Resource counts collected: Total CMs=$total_configmaps, Total Secrets=$total_secrets, App CMs=$configmaps_application, App Secrets=$secrets_application"
}

# ---------- Master node metrics ----------
collect_master_metrics() {
  log_info "Collecting master node metrics"
  
  local master_nodes=()
  while IFS= read -r line; do
    master_nodes+=("$line")
  done < <(oc get nodes -l node-role.kubernetes.io/master --no-headers -o custom-columns=NAME:.metadata.name)
  
  local master_metrics=()
  for node in "${master_nodes[@]}"; do
    local cpu_usage memory_usage etcd_disk_usage
    
    # Get node metrics
    cpu_usage=$(oc get --raw "/api/v1/nodes/$node/proxy/metrics" 2>/dev/null | grep 'node_cpu_seconds_total' | tail -1 | awk '{print $2}' | sed 's/.*{.*} //' || echo "n/a")
    memory_usage=$(oc get --raw "/api/v1/nodes/$node/proxy/metrics" 2>/dev/null | grep 'node_memory_MemTotal_bytes' | tail -1 | awk '{print $2}' | sed 's/.*{.*} //' || echo "n/a")
    
    # ETCD disk usage (if available)
    etcd_disk_usage=$(oc get --raw "/api/v1/nodes/$node/proxy/metrics" 2>/dev/null | grep 'etcd_disk_wal_fsync_duration_seconds' | tail -1 | awk '{print $2}' | sed 's/.*{.*} //' || echo "n/a")
    
    master_metrics+=("{\"node\":\"$node\",\"cpu_usage_percent\":\"$cpu_usage\",\"memory_usage_percent\":\"$memory_usage\",\"etcd_disk_usage_percent\":\"$etcd_disk_usage\"}")
  done
  
  # Write to JSON
  cat > "$OUTPUT_DIR/master-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "master_nodes": [$(IFS=','; echo "${master_metrics[*]}")]
}
EOF
  
  log_info "Master node metrics collected for ${#master_nodes[@]} nodes"
}

# ---------- Cluster health ----------
collect_cluster_health() {
  log_info "Collecting cluster health metrics"
  
  local cluster_version cluster_operators_failed node_status
  
  # Cluster version
  cluster_version=$(oc get clusterversion version -o jsonpath='{.status.desired.version}' 2>/dev/null || echo "n/a")
  
  # Failed cluster operators
  cluster_operators_failed=$(oc get clusteroperators --no-headers | grep -v "True" | wc -l)
  
  # Node status
  node_status=$(oc get nodes --no-headers | grep -v "Ready" | wc -l)
  
  # Write to JSON
  cat > "$OUTPUT_DIR/cluster-health-$TIMESTAMP.json" << EOF
{
  "timestamp": "$TIMESTAMP",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "cluster_health": {
    "version": "$cluster_version",
    "failed_cluster_operators": $cluster_operators_failed,
    "not_ready_nodes": $node_status
  }
}
EOF
  
  log_info "Cluster health collected: Version=$cluster_version, Failed COs=$cluster_operators_failed, Not ready nodes=$node_status"
}

# ---------- Generate report ----------
generate_report() {
  log_info "Generating performance report"
  
  local report_file="$OUTPUT_DIR/performance-report-$TIMESTAMP.md"
  
  cat > "$report_file" << EOF
# OpenShift 4.16 Master Node Performance Report
## Red Hat Community of Practice

**Snapshot Type:** $SNAPSHOT_TYPE  
**Timestamp:** $TIMESTAMP  
**Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)

---

## ETCD Performance Metrics

### WAL Fsync Latency
- **p99 Latency**: $(jq -r '.etcd.wal_fsync_p99_seconds' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds

### Database Size
- **DB Size**: $(jq -r '.etcd.db_size_bytes' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") bytes

### Leadership Stability
- **Leader Changes/Hour**: $(jq -r '.etcd.leader_changes_per_hour' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

---

## API Server Performance Metrics

### CLI-based Measurements
- **GET p99**: $(jq -r '.api_server.get_p99_seconds' "$OUTPUT_DIR/api-metrics-cli-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **LIST p99**: $(jq -r '.api_server.list_p99_seconds' "$OUTPUT_DIR/api-metrics-cli-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **WATCH p99**: $(jq -r '.api_server.watch_p99_seconds' "$OUTPUT_DIR/api-metrics-cli-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **Overall p99**: $(jq -r '.api_server.overall_p99_seconds' "$OUTPUT_DIR/api-metrics-cli-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds

### Prometheus-based Measurements
- **Overall p99**: $(jq -r '.api_server.overall_p99_seconds' "$OUTPUT_DIR/api-metrics-prometheus-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **GET p99**: $(jq -r '.api_server.get_p99_seconds' "$OUTPUT_DIR/api-metrics-prometheus-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **LIST p99**: $(jq -r '.api_server.list_p99_seconds' "$OUTPUT_DIR/api-metrics-prometheus-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **WATCH p99**: $(jq -r '.api_server.watch_p99_seconds' "$OUTPUT_DIR/api-metrics-prometheus-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds

---

## Resource Counts

### Total Resources
- **ConfigMaps**: $(jq -r '.resources.total_configmaps' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")
- **Secrets**: $(jq -r '.resources.total_secrets' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

### Application Namespace Resources
- **ConfigMaps (application namespaces)**: $(jq -r '.resources.configmaps_application_namespaces' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")
- **Secrets (application namespaces)**: $(jq -r '.resources.secrets_application_namespaces' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

---

## Master Node Metrics

$(jq -r '.master_nodes[] | "- **'$node'**: CPU: \(.cpu_usage_percent)%, Memory: \(.memory_usage_percent)%, ETCD Disk: \(.etcd_disk_usage_percent)%"' "$OUTPUT_DIR/master-metrics-$TIMESTAMP.json" 2>/dev/null || echo "- No master node metrics available")

---

## Cluster Health

- **Cluster version**: $(oc get clusterversion version -o jsonpath='{.status.desired.version}' 2>/dev/null || echo "n/a")
- **Cluster operators**: $(oc get clusteroperators --no-headers | grep -v "True" | wc -l) failed
- **Node status**: $(oc get nodes --no-headers | grep -v "Ready" | wc -l) not ready

---

## Notes

- This report captures performance metrics for OpenShift 4.16 master node tuning validation
- ETCD metrics are collected via Prometheus (Thanos Querier)
- API server metrics are collected via both CLI timing and Prometheus for comparison
- Resource counts help validate the effectiveness of pruning operations
- Master node metrics provide insight into resource utilization

EOF
  
  log_info "Report generated: $report_file"
}

# ---------- Main function ----------
main() {
  log_info "Starting OpenShift 4.16 performance snapshot: $SNAPSHOT_TYPE"
  
  # Check prerequisites
  if ! command -v oc >/dev/null 2>&1; then
    log_error "oc CLI not found. Please install and configure oc CLI."
    exit 1
  fi
  
  if ! command -v jq >/dev/null 2>&1; then
    log_error "jq not found. Please install jq."
    exit 1
  fi
  
  if ! command -v bc >/dev/null 2>&1; then
    log_error "bc not found. Please install bc."
    exit 1
  fi
  
  # Check cluster connectivity
  if ! oc cluster-info >/dev/null 2>&1; then
    log_error "Cannot connect to OpenShift cluster. Please check your kubeconfig."
    exit 1
  fi
  
  # Set up Prometheus connection
  THANOS_HOST=$(get_thanos_host)
  TOKEN=$(oc whoami -t)
  INGRESS_CA_FILE=$(get_ingress_ca)
  
  if [[ -n "$THANOS_HOST" && -n "$TOKEN" ]]; then
    log_info "Prometheus connection configured: $THANOS_HOST"
    if [[ -n "$INGRESS_CA_FILE" ]]; then
      log_info "Using ingress CA: $INGRESS_CA_FILE"
    else
      log_info "Using --insecure for Prometheus queries"
    fi
  else
    log_info "Prometheus connection not available, some metrics will be n/a"
  fi
  
  # Collect all metrics
  collect_etcd_metrics
  collect_api_metrics_cli
  collect_api_metrics_prometheus
  collect_resource_counts
  collect_master_metrics
  collect_cluster_health
  
  # Generate report
  generate_report
  
  log_info "Performance snapshot completed successfully"
  log_info "Output directory: $OUTPUT_DIR"
  log_info "Report file: $OUTPUT_DIR/performance-report-$TIMESTAMP.md"
}

# Run main function
main "$@"
EOF












------------------------------------------
#!/bin/bash
# OpenShift 4.16 Master Node Performance Snapshot Script
# Red Hat CoP - Collects baseline/post-tuning metrics via Prometheus (Thanos)
# Outputs JSON artifacts and a Markdown report

set -euo pipefail

OUTPUT_DIR=${OUTPUT_DIR:-"/tmp/openshift-performance-snapshots"}
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
SNAPSHOT_TYPE=${SNAPSHOT_TYPE:-"baseline"}  # baseline | post
LOG_FILE="$OUTPUT_DIR/performance-snapshot-$TIMESTAMP.log"
CURL_TIMEOUT=${CURL_TIMEOUT:-5}

mkdir -p "$OUTPUT_DIR"

log() { echo "[$(date -u +%Y-%m-%dT%H:%M:%SZ)] $1" | tee -a "$LOG_FILE"; }
log_info() { log "INFO: $1"; }
log_error() { log "ERROR: $1"; }

# ---------- Prometheus helpers (Thanos + ingress CA) ----------
get_thanos_host() {
  oc -n openshift-monitoring get route thanos-querier -o jsonpath='{.spec.host}' 2>/dev/null || true
}

# Cache/return ingress CA (router CA bundle)
get_ingress_ca() {
  local ca_file="${INGRESS_CA_FILE:-}"
  if [[ -n "${ca_file}" && -s "${ca_file}" ]]; then
    echo "${ca_file}"; return 0
  fi
  ca_file="$(mktemp)"
  if oc -n openshift-config-managed get cm default-ingress-cert >/dev/null 2>&1; then
    oc -n openshift-config-managed get cm default-ingress-cert -o jsonpath='{.data.ca-bundle\.crt}' > "${ca_file}" 2>/dev/null || true
  fi
  if [[ ! -s "${ca_file}" ]]; then
    rm -f "${ca_file}"
    echo ""  # indicate unavailable
    return 0
  fi
  echo "${ca_file}"
}

# Run a PromQL query; returns scalar value or "n/a"
prom_query() {
  local q="$1"
  local thanos_host token enc_q ca_file
  thanos_host="$(get_thanos_host)"
  [[ -z "$thanos_host" ]] && echo "n/a" && return 0

  token="$(oc whoami -t 2>/dev/null || true)"
  [[ -z "$token" ]] && echo "n/a" && return 0

  enc_q=$(printf '%s' "$q" | jq -s -R -r @uri)

  ca_file="$(get_ingress_ca)"
  if [[ -n "$ca_file" && -s "$ca_file" ]]; then
    curl -sS --fail --connect-timeout "$CURL_TIMEOUT" -m "$CURL_TIMEOUT" \
      --cacert "$ca_file" \
      -H "Authorization: Bearer $token" \
      "https://${thanos_host}/api/v1/query?query=${enc_q}" \
      | jq -r '.data.result[0].value[1] // "n/a"' 2>/dev/null || echo "n/a"
  else
    # Fallback: allow self-signed/untrusted; prefer fixing CA trust
    curl -sS --fail --connect-timeout "$CURL_TIMEOUT" -m "$CURL_TIMEOUT" \
      --insecure \
      -H "Authorization: Bearer $token" \
      "https://${thanos_host}/api/v1/query?query=${enc_q}" \
      | jq -r '.data.result[0].value[1] // "n/a"' 2>/dev/null || echo "n/a"
  fi
}

# Return max of two numeric strings; n/a handling
max_num() {
  local a="$1" b="$2"
  if [[ "$a" == "n/a" ]]; then echo "$b"; return; fi
  if [[ "$b" == "n/a" ]]; then echo "$a"; return; fi
  if awk "BEGIN{exit !($a>$b)}"; then echo "$a"; else echo "$b"; fi
}

# ---------- etcd metrics (Prometheus) ----------
collect_etcd_metrics() {
  log_info "Collecting etcd metrics (Prometheus)"

  local wal_p99 db_size leader_changes_per_h
  wal_p99="$(prom_query 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))')"
  db_size="$(prom_query 'avg(etcd_mvcc_db_total_size_in_bytes)')"
  leader_changes_per_h="$(prom_query 'sum(rate(etcd_server_leader_changes_seen_total[1h]))')"

  cat > "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "etcd": {
    "wal_fsync_p99_seconds": "$wal_p99",
    "db_size_bytes": "$db_size",
    "leader_changes_per_hour": "$leader_changes_per_h"
  }
}
EOF

  log_info "ETCD: p99=${wal_p99}s, DB=${db_size}B, leader_changes/h=${leader_changes_per_h}"
}

# ---------- API p99 (Prometheus) ----------
# ---------- API p99 (Prometheus) - Simplified ----------
collect_api_metrics() {
  log_info "Collecting API server p99 (Prometheus)"

  # Overall API server p99 (all verbs combined) - primary metric
  local overall_p99
  overall_p99="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket[5m])))')"
  
  # Individual verbs (best effort, longer windows for sparse operations)
  local p99_get p99_list p99_watch
  p99_get="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="GET"}[5m])))')"
  p99_list="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="LIST"}[10m])))')"
  p99_watch="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="WATCH"}[30m])))')"

  cat > "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "api_server": {
    "get_p99_seconds": "$p99_get",
    "list_p99_seconds": "$p99_list",
    "watch_p99_seconds": "$p99_watch",
    "overall_p99_seconds": "$overall_p99"
  }
}
EOF

  log_info "API p99: GET=$p99_get, LIST=$p99_list, WATCH=$p99_watch, overall=$overall_p99"
}

# ---------- Resource counts (application namespaces only) ----------
collect_resource_counts() {
  log_info "Collecting resource counts (app namespaces)"

  local cm_count sec_count
  cm_count=$(oc get configmaps --all-namespaces -o json \
    | jq -r '.items[] | select(.metadata.namespace|test("^(openshift-|kube-|default$)")|not) | .metadata.name' \
    | wc -l | tr -d ' ' || echo "n/a")
  sec_count=$(oc get secrets --all-namespaces -o json \
    | jq -r '.items[] | select(.metadata.namespace|test("^(openshift-|kube-|default$)")|not) | .metadata.name' \
    | wc -l | tr -d ' ' || echo "n/a")

  cat > "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "resources": {
    "configmaps_application_namespaces": $cm_count,
    "secrets_application_namespaces": $sec_count
  }
}
EOF

  log_info "Resource counts: ConfigMaps=$cm_count, Secrets=$sec_count"
}

# ---------- Master node snapshot (best-effort) ----------
collect_master_metrics() {
  log_info "Collecting master node snapshot (best-effort)"
  local master_list items=()
  master_list=$(oc get nodes -l node-role.kubernetes.io/master= -o name 2>/dev/null | cut -d/ -f2 || true)

  for n in $master_list; do
    local cpu memory
    cpu=$(oc describe node "$n" 2>/dev/null | awk '/Allocated resources:/{p=1} p&&/cpu/{gsub(/[()%]/,""); print $3; exit}' || true)
    memory=$(oc describe node "$n" 2>/dev/null | awk '/Allocated resources:/{p=1} p&&/memory/{gsub(/[()%]/,""); print $3; exit}' || true)
    [[ -z "$cpu" ]] && cpu="n/a"
    [[ -z "$memory" ]] && memory="n/a"
    items+=("{\"node\":\"$n\",\"cpu_usage_percent\":\"$cpu\",\"memory_usage_percent\":\"$memory\"}")
  done

  local joined="[]"
  if [[ ${#items[@]} -gt 0 ]]; then
    joined=$(printf '%s\n' "${items[@]}" | jq -s '.')
  fi

  cat > "$OUTPUT_DIR/master-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "master_nodes": $joined
}
EOF

  log_info "Master nodes captured: $(echo "$master_list" | wc -w | tr -d ' ')"
}

# ---------- Markdown report ----------
generate_markdown_report() {
  log_info "Generating markdown report"
  local report_file="$OUTPUT_DIR/performance-report-$SNAPSHOT_TYPE-$TIMESTAMP.md"

  cat > "$report_file" << EOF
# OpenShift 4.16 Master Node Performance Report
## $SNAPSHOT_TYPE - $(date -u +%Y-%m-%dT%H:%M:%SZ)

### ETCD Metrics
- **WAL fsync p99 latency**: $(jq -r '.etcd.wal_fsync_p99_seconds' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **DB size**: $(jq -r '.etcd.db_size_bytes' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") bytes
- **Leader changes per hour**: $(jq -r '.etcd.leader_changes_per_hour' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

### API Server Metrics
- **GET p99**: $(jq -r '.api_server.get_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **LIST p99**: $(jq -r '.api_server.list_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **WATCH p99**: $(jq -r '.api_server.watch_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **Overall p99 (max)**: $(jq -r '.api_server.overall_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds

### Resource Counts
- **ConfigMaps (application namespaces)**: $(jq -r '.resources.configmaps_application_namespaces' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")
- **Secrets (application namespaces)**: $(jq -r '.resources.secrets_application_namespaces' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

### Master Node Snapshot
$(jq -r '.master_nodes[]? | "- " + .node + ": CPU " + (.cpu_usage_percent|tostring) + "%, Memory " + (.memory_usage_percent|tostring) + "%"' "$OUTPUT_DIR/master-metrics-$TIMESTAMP.json" 2>/dev/null || echo "- n/a")

### Cluster Health (best-effort)
- **Cluster version**: $(oc get clusterversion version -o jsonpath='{.status.desired.version}' 2>/dev/null || echo "n/a")
- **Cluster operators not fully healthy**: $(oc get clusteroperators --no-headers 2>/dev/null | grep -v "True" | wc -l | tr -d ' ' || echo "n/a")
- **Nodes not Ready**: $(oc get nodes --no-headers 2>/dev/null | grep -v " Ready " | wc -l | tr -d ' ' || echo "n/a")

### Notes
- Snapshot type: $SNAPSHOT_TYPE
- Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)
- Output directory: $OUTPUT_DIR
EOF

  log_info "Markdown report: $report_file"
  echo "$report_file"
}

main() {
  log_info "Starting performance snapshot (type=$SNAPSHOT_TYPE)"
  collect_etcd_metrics
  collect_api_metrics
  collect_resource_counts
  collect_master_metrics
  local report_file
  report_file=$(generate_markdown_report)
  log_info "Completed. Report: $report_file"
  echo "$report_file"
}

main "$@"



--------------------
#!/bin/bash
# OpenShift 4.16 Master Node Performance Snapshot Script
# Red Hat Community of Practice - Performance Validation
# Collects baseline and post-tuning performance metrics with Prometheus-based etcd and API p99

set -euo pipefail

# Config
OUTPUT_DIR=${OUTPUT_DIR:-"/tmp/openshift-performance-snapshots"}
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
SNAPSHOT_TYPE=${SNAPSHOT_TYPE:-"baseline"}  # baseline or post
LOG_FILE="$OUTPUT_DIR/performance-snapshot-$TIMESTAMP.log"
CURL_TIMEOUT=${CURL_TIMEOUT:-5}

mkdir -p "$OUTPUT_DIR"

log() { echo "[$(date -u +%Y-%m-%dT%H:%M:%SZ)] $1" | tee -a "$LOG_FILE"; }
log_info() { log "INFO: $1"; }
log_error() { log "ERROR: $1"; }

# ---------- Prometheus helpers (Thanos Querier) ----------
get_thanos_host() {
  oc -n openshift-monitoring get route thanos-querier -o jsonpath='{.spec.host}' 2>/dev/null || true
}

prom_query() {
  local q="$1"
  local thanos_host token enc_q
  thanos_host="$(get_thanos_host)"
  [[ -z "$thanos_host" ]] && echo "n/a" && return 0

  token="$(oc whoami -t 2>/dev/null || true)"
  [[ -z "$token" ]] && echo "n/a" && return 0

  # URI-encode with jq (no python dependency)
  enc_q=$(printf '%s' "$q" | jq -s -R -r @uri)
  curl -sS --connect-timeout "$CURL_TIMEOUT" -m "$CURL_TIMEOUT" \
       -H "Authorization: Bearer $token" \
       "https://${thanos_host}/api/v1/query?query=${enc_q}" \
    | jq -r '.data.result[0].value[1] // "n/a"' 2>/dev/null || echo "n/a"
}

# Numeric compare, returns max of two numeric strings or n/a
max_num() {
  local a="$1" b="$2"
  if [[ "$a" == "n/a" ]]; then echo "$b"; return; fi
  if [[ "$b" == "n/a" ]]; then echo "$a"; return; fi
  if awk "BEGIN{exit !($a>$b)}"; then echo "$a"; else echo "$b"; fi
}

# ---------- etcd metrics (Prometheus) ----------
collect_etcd_metrics() {
  log_info "Collecting etcd metrics (via Prometheus)"

  # p99 over last 5m
  local wal_p99
  wal_p99="$(prom_query 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))')"

  # DB size (bytes) average across members
  local db_size
  db_size="$(prom_query 'avg(etcd_mvcc_db_total_size_in_bytes)')"

  # Leader changes per hour (sum across members)
  local leader_changes_per_h
  leader_changes_per_h="$(prom_query 'sum(rate(etcd_server_leader_changes_seen_total[1h]))')"

  cat > "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "etcd": {
    "wal_fsync_p99_seconds": "$wal_p99",
    "db_size_bytes": "$db_size",
    "leader_changes_per_hour": "$leader_changes_per_h"
  }
}
EOF

  log_info "ETCD metrics: p99=${wal_p99}s, DB=${db_size}B, leader_changes/h=${leader_changes_per_h}"
}

# ---------- API p99 (Prometheus) ----------
collect_api_metrics() {
  log_info "Collecting API server p99 (via Prometheus)"

  # p99 by verb over last 5m
  local p99_get p99_list p99_watch
  p99_get="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="GET"}[5m])))')"
  p99_list="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="LIST"}[5m])))')"
  p99_watch="$(prom_query 'histogram_quantile(0.99, sum by (le) (rate(apiserver_request_duration_seconds_bucket{verb="WATCH"}[5m])))')"

  # Max across GET/LIST/WATCH as required
  local overall_p99
  overall_p99="$(max_num "$(max_num "$p99_get" "$p99_list")" "$p99_watch")"

  cat > "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "api_server": {
    "get_p99_seconds": "$p99_get",
    "list_p99_seconds": "$p99_list",
    "watch_p99_seconds": "$p99_watch",
    "overall_p99_seconds": "$overall_p99"
  }
}
EOF

  log_info "API p99: GET=$p99_get, LIST=$p99_list, WATCH=$p99_watch, overall=$overall_p99"
}

# ---------- Resource counts (application namespaces) ----------
collect_resource_counts() {
  log_info "Collecting resource counts (application namespaces)"

  local configmap_count secret_count
  configmap_count=$(oc get configmaps --all-namespaces -o json \
    | jq -r '.items[] | select(.metadata.namespace|test("^(openshift-|kube-|default$)")|not) | .metadata.name' | wc -l | tr -d ' ' || echo "n/a")
  secret_count=$(oc get secrets --all-namespaces -o json \
    | jq -r '.items[] | select(.metadata.namespace|test("^(openshift-|kube-|default$)")|not) | .metadata.name' | wc -l | tr -d ' ' || echo "n/a")

  cat > "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "resources": {
    "configmaps_application_namespaces": $configmap_count,
    "secrets_application_namespaces": $secret_count
  }
}
EOF

  log_info "Resource counts: CM=$configmap_count, Secrets=$secret_count"
}

# ---------- Optional: basic master node snapshot (best-effort) ----------
collect_master_metrics() {
  log_info "Collecting master node snapshot (best-effort)"
  local masters master_list
  master_list=$(oc get nodes -l node-role.kubernetes.io/master= -o name 2>/dev/null | cut -d/ -f2 || true)

  local items=()
  for n in $master_list; do
    # These are best-effort approximations from node description (percentages may be n/a)
    local cpu memory
    cpu=$(oc describe node "$n" 2>/dev/null | awk '/Allocated resources:/{p=1} p&&/cpu/{gsub(/[()%]/,""); print $3; exit}' || true)
    memory=$(oc describe node "$n" 2>/dev/null | awk '/Allocated resources:/{p=1} p&&/memory/{gsub(/[()%]/,""); print $3; exit}' || true)
    [[ -z "$cpu" ]] && cpu="n/a"
    [[ -z "$memory" ]] && memory="n/a"
    items+=("{\"node\":\"$n\",\"cpu_usage_percent\":\"$cpu\",\"memory_usage_percent\":\"$memory\"}")
  done

  local joined="[]"
  if [[ ${#items[@]} -gt 0 ]]; then
    joined=$(printf '%s\n' "${items[@]}" | jq -s '.')
  fi

  cat > "$OUTPUT_DIR/master-metrics-$TIMESTAMP.json" << EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "snapshot_type": "$SNAPSHOT_TYPE",
  "master_nodes": $joined
}
EOF

  log_info "Master nodes captured: $(echo "$master_list" | wc -w | tr -d ' ')"
}

# ---------- Markdown report ----------
generate_markdown_report() {
  log_info "Generating markdown report"
  local report_file="$OUTPUT_DIR/performance-report-$SNAPSHOT_TYPE-$TIMESTAMP.md"

  cat > "$report_file" << EOF
# OpenShift 4.16 Master Node Performance Report
## $SNAPSHOT_TYPE - $(date -u +%Y-%m-%dT%H:%M:%SZ)

### ETCD Metrics
- **WAL fsync p99 latency**: $(jq -r '.etcd.wal_fsync_p99_seconds' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **DB size**: $(jq -r '.etcd.db_size_bytes' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") bytes
- **Leader changes per hour**: $(jq -r '.etcd.leader_changes_per_hour' "$OUTPUT_DIR/etcd-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

### API Server Metrics
- **GET p99**: $(jq -r '.api_server.get_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **LIST p99**: $(jq -r '.api_server.list_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **WATCH p99**: $(jq -r '.api_server.watch_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds
- **Overall p99 (max)**: $(jq -r '.api_server.overall_p99_seconds' "$OUTPUT_DIR/api-metrics-$TIMESTAMP.json" 2>/dev/null || echo "n/a") seconds

### Resource Counts
- **ConfigMaps (application namespaces)**: $(jq -r '.resources.configmaps_application_namespaces' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")
- **Secrets (application namespaces)**: $(jq -r '.resources.secrets_application_namespaces' "$OUTPUT_DIR/resource-counts-$TIMESTAMP.json" 2>/dev/null || echo "n/a")

### Master Node Snapshot
$(jq -r '.master_nodes[]? | "- " + .node + ": CPU " + (.cpu_usage_percent|tostring) + "%, Memory " + (.memory_usage_percent|tostring) + "%"' "$OUTPUT_DIR/master-metrics-$TIMESTAMP.json" 2>/dev/null || echo "- n/a")

### Cluster Health (best-effort)
- **Cluster version**: $(oc get clusterversion version -o jsonpath='{.status.desired.version}' 2>/dev/null || echo "n/a")
- **Cluster operators not fully healthy**: $(oc get clusteroperators --no-headers 2>/dev/null | grep -v "True" | wc -l | tr -d ' ' || echo "n/a")
- **Nodes not Ready**: $(oc get nodes --no-headers 2>/dev/null | grep -v " Ready " | wc -l | tr -d ' ' || echo "n/a")

### Notes
- Snapshot type: $SNAPSHOT_TYPE
- Timestamp: $(date -u +%Y-%m-%dT%H:%M:%SZ)
- Output directory: $OUTPUT_DIR
EOF

  log_info "Markdown report: $report_file"
  echo "$report_file"
}

main() {
  log_info "Starting performance snapshot (type=$SNAPSHOT_TYPE)"
  collect_etcd_metrics
  collect_api_metrics
  collect_resource_counts
  collect_master_metrics
  local report_file
  report_file=$(generate_markdown_report)
  log_info "Completed. Report: $report_file"
  echo "$report_file"
}

main "$@"

# STAGE 1: Builder (Alpine)
FROM alpine:latest AS builder

RUN apk add --no-cache ca-certificates curl

# Use v3.2.1 (confirmed working)
ARG MIMIR_VERSION=3.2.1

# Download directly from GitHub releases
RUN curl -fsSL https://github.com/grafana/mimir/releases/download/mimir-${MIMIR_VERSION}/mimir-linux-amd64 -o /tmp/mimir && \
    chmod +x /tmp/mimir

# STAGE 2: CA Bundle provider (full UBI for ca-certificates)
FROM registry.redhat.io/ubi9/ubi:9.4 AS ca-provider

# STAGE 3: Runtime (UBI minimal)
FROM registry.redhat.io/ubi9/ubi-minimal:9.4

# Copy CA certificates from full UBI image (avoids microdnf pthread issues)
COPY --from=ca-provider /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem
COPY --from=ca-provider /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt

# Copy binary from builder
COPY --from=builder /tmp/mimir /usr/local/bin/mimir

# Create necessary directories with proper permissions for OpenShift
RUN mkdir -p /etc/mimir /data && \
    chown -R 1001:0 /etc/mimir /data && \
    chmod -R g=u /etc/mimir /data

# Labels for OpenShift
LABEL name="grafana-mimir" \
      version="3.2.1" \
      summary="Grafana Mimir - Scalable time series database" \
      description="Horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus"

EXPOSE 8080

USER 1001

WORKDIR /data

ENTRYPOINT ["/usr/local/bin/mimir"]
CMD ["-config.file=/etc/mimir/config.yaml"]


+++++++++++
# STAGE 1: Builder (Alpine)
FROM alpine:latest AS builder

RUN apk add --no-cache ca-certificates curl unzip

# Use v3.2.1 (confirmed working)
ARG MIMIR_VERSION=3.2.1

RUN curl -fsSL https://github.com/grafana/mimir/releases/download/mimir-${MIMIR_VERSION}/mimir-linux-amd64 -o /tmp/mimir && \
    chmod +x /tmp/mimir

# STAGE 2: Runtime (UBI)
FROM registry.redhat.io/ubi9/ubi-minimal:9.4

# Install CA certificates in runtime image
RUN microdnf install -y ca-certificates && \
    microdnf clean all

# Copy binary from builder
COPY --from=builder /tmp/mimir /usr/local/bin/mimir

# Create necessary directories
RUN mkdir -p /etc/mimir /data && \
    chown -R 1001:0 /etc/mimir /data && \
    chmod -R g=u /etc/mimir /data

# Labels for OpenShift
LABEL name="mimir" \
      version="${MIMIR_VERSION}" \
      summary="Grafana Mimir - Scalable time series database" \
      description="Horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus"

EXPOSE 8080

USER 1001

WORKDIR /data

ENTRYPOINT ["/usr/local/bin/mimir"]
CMD ["-config.file=/etc/mimir/config.yaml"]

----
RUN curl -fsSL https://github.com/grafana/mimir/releases/download/v${MIMIR_VERSION}/mimir-linux-amd64.zip -o mimir.zip && \
    unzip mimir.zip && \
    mv mimir-linux-amd64/mimir /usr/local/bin/mimir && \
    chmod +x /usr/local/bin/mimir && \
    rm -rf mimir.zip mimir-linux-amd64
-----
# STAGE 1: Extract in Alpine (has curl + unzip)
FROM alpine:latest AS builder

RUN apk add --no-cache ca-certificates curl unzip

ARG MIMIR_VERSION=3.0.0
RUN curl -fsSL https://github.com/grafana/mimir/releases/download/v${MIMIR_VERSION}/mimir-linux-amd64.zip -o mimir.zip && \
    unzip mimir.zip

# STAGE 2: Final UBI image
FROM registry.redhat.io/ubi9/ubi-minimal:9.4

COPY --from=builder /mimir-linux-amd64/mimir /usr/local/bin/mimir

# Add CA bundle for ODF TLS
COPY --from=registry.redhat.io/ubi9/ubi:9.4 /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem

EXPOSE 8080
USER 1001
ENTRYPOINT ["/usr/local/bin/mimir"]
CMD ["-config.file=/etc/mimir/config.yaml"]

-------
# STAGE 1: Use Alpine to extract (no subscription, no repos)
FROM alpine:latest AS builder

# Alpine has wget and tar by default
ARG MIMIR_VERSION=3.0.0
    tar -xzf mimir.tar.gz

# STAGE 2: Final image ‚Äî pure UBI minimal (no tools)
FROM registry.redhat.io/ubi9/ubi-minimal:9.4

# Copy only the binary
COPY --from=builder /mimir-linux-amd64/mimir /usr/local/bin/mimir

# Optional: Add CA bundle if you need TLS (e.g., for ODF)
# If you skip this, Mimir may fail to connect to ODF over HTTPS
COPY --from=registry.redhat.io/ubi9/ubi:9.4 /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem

EXPOSE 8080
USER 1001
ENTRYPOINT ["/usr/local/bin/mimir"]
CMD ["-config.file=/etc/mimir/config.yaml"]
-------
# Dockerfile.mimir-ubi
FROM registry.redhat.io/ubi9/ubi-minimal:9.4

# Mimir is a static binary ‚Äî no deps!
# Just ensure CA certs are available for TLS (e.g., ODF)
# Copy CA bundle from a trusted source (UBI base has it)
COPY --from=registry.redhat.io/ubi9/ubi:9.4 /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem

# Download Mimir binary
ARG MIMIR_VERSION=3.0.0
ADD https://github.com/grafana/mimir/releases/download/v${MIMIR_VERSION}/mimir-linux-amd64.zip /tmp/mimir.zip

# Unpack and install
RUN unzip /tmp/mimir.zip && \
    mv mimir-linux-amd64/mimir /usr/local/bin/mimir && \
    chmod +x /usr/local/bin/mimir && \
    rm -rf /tmp/mimir.zip mimir-linux-amd64

EXPOSE 8080
USER 1001
ENTRYPOINT ["/usr/local/bin/mimir"]
CMD ["-config.file=/etc/mimir/config.yaml"]


++++++++++++++++++++++++++++
# ============================================================
# PART 1: DOCKERFILE FOR GRAFANA MIMIR ON UBI (Save as Dockerfile.mimir)
# Build with: podman build -t quay.io/YOUR-ORG/mimir:3.0.0-ubi -f Dockerfile.mimir .
# ============================================================
FROM registry.redhat.io/ubi9/ubi-minimal:9.4

# Install deps
RUN microdnf install -y ca-certificates curl gzip tar && microdnf clean all

# Download Mimir binary (official release)
ARG MIMIR_VERSION=3.0.0
RUN curl -fsSL https://github.com/grafana/mimir/releases/download/v${MIMIR_VERSION}/mimir-linux-amd64.zip -o mimir.zip && \
    unzip mimir.zip && \
    mv mimir-linux-amd64/mimir /usr/local/bin/mimir && \
    chmod +x /usr/local/bin/mimir && \
    rm -rf mimir.zip mimir-linux-amd64

EXPOSE 8080
USER 1001
ENTRYPOINT ["/usr/local/bin/mimir"]
CMD ["-config.file=/etc/mimir/config.yaml"]

===
# ============================================================
# PART 2: DOCKERFILE FOR ALERT-EXPORTER ON UBI (Save as Dockerfile.exporter)
# Build with: podman build -t quay.io/YOUR-ORG/alert-exporter:ubi -f Dockerfile.exporter .
# ============================================================
FROM registry.redhat.io/rhel9/python-39:1-16

WORKDIR /app
RUN pip3 install --no-cache-dir prometheus_client flask

# Simple exporter (you can replace with your own file)
RUN echo 'from flask import Flask, request, jsonify; from prometheus_client import Counter, generate_latest, CONTENT_TYPE_LATEST; import os; app = Flask(__name__); CLUSTER = os.getenv("CLUSTER_NAME", "unknown"); ALERT_COUNT = Counter("alert_events_total", "Alert count", ["alertname", "severity", "cluster"]); @app.route("/alerts", methods=["POST"]); def alerts():;   for a in request.json.get("alerts", []):;     labels = a.get("labels", {});     ALERT_COUNT.labels(labels.get("alertname",""), labels.get("severity",""), CLUSTER).inc();   return jsonify({}); @app.route("/metrics"); def metrics():;   return generate_latest(), 200, {"Content-Type": CONTENT_TYPE_LATEST}; if __name__ == "__main__":;   app.run(host="0.0.0.0", port=9099)' > alert_exporter.py

EXPOSE 9099
USER 1001
CMD ["python3", "alert_exporter.py"]
====
# ============================================================
# PART 3: OPENSHIFT DEPLOYMENT YAML FOR MIMIR + ODF
# Replace ONLY these 3 values:
#   1. <YOUR_MIMIR_IMAGE>     ‚Üí e.g., quay.io/your-org/mimir:3.0.0-ubi
#   2. <ODF_S3_ROUTE>         ‚Üí e.g., s3-openshift-storage.apps.cluster.example.com
#   3. <BASE64_ACCESS_KEY>    ‚Üí echo -n "KEY" | base64
#   4. <BASE64_SECRET_KEY>    ‚Üí echo -n "SECRET" | base64
# ============================================================

---
apiVersion: v1
kind: Namespace
meta
  name: mimir
---
apiVersion: v1
kind: Secret
meta
  name: mimir-s3
  namespace: mimir
type: Opaque

  access_key_id: <BASE64_ACCESS_KEY>
  access_secret_key: <BASE64_SECRET_KEY>
---
apiVersion: v1
kind: ConfigMap
meta
  name: mimir-config
  namespace: mimir

  config.yaml: |
    auth_enabled: false
    server:
      http_listen_port: 8080
    distributor:
      ring:
        kvstore:
          store: memberlist
    ingester:
      lifecycler:
        ring:
          kvstore:
            store: memberlist
        final_sleep: 0s
      chunk_idle_period: 30m
      max_chunk_age: 1h
    memberlist:
      join_members: ["mimir-gateway.mimir.svc:8080"]
    schema:
      configs:
        - from: "2020-10-24"
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: mimir_
            period: 24h
    storage:
      backend: s3
      s3:
        endpoint: <ODF_S3_ROUTE>
        bucketnames: mimir-chunks
        access_key_id: ${STORAGE_S3_ACCESS_KEY_ID}
        secret_access_key: ${STORAGE_S3_SECRET_ACCESS_KEY}
        insecure: false
        path_style: true
        signature_version: v4
      tsdb:
        dir: /tmp/tsdb
      bucket_store:
        sync_dir: /tmp/bucket-sync
    limits:
      retention: 8760h
    compactor:
      data_dir: /tmp/compactor
      compaction_interval: 1h
      retention_enabled: true
    frontend:
      zipkin:
        enabled: false
---
apiVersion: apps/v1
kind: Deployment
meta
  name: mimir
  namespace: mimir
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mimir
  template:
    meta
      labels:
        app: mimir
    spec:
      containers:
        - name: mimir
          # üîÅ REPLACE WITH YOUR UBI IMAGE
          image: <YOUR_MIMIR_IMAGE>
          args:
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/config.yaml"
          env:
            - name: STORAGE_S3_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: mimir-s3
                  key: access_key_id
            - name: STORAGE_S3_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: mimir-s3
                  key: access_secret_key
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1001
      volumes:
        - name: config
          configMap:
            name: mimir-config
---
apiVersion: v1
kind: Service
meta
  name: mimir-gateway
  namespace: mimir
spec:
  selector:
    app: mimir
  ports:
    - port: 8080
      targetPort: 8080
---
apiVersion: route.openshift.io/v1
kind: Route
meta
  name: mimir-remote-write
  namespace: mimir
spec:
  to:
    kind: Service
    name: mimir-gateway
  port:
    targetPort: 8080
  tls:
    termination: edge
    insecurePolicy: Redirect

-------------------
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cm-secret-pruner
  namespace: configmap-secret-pruner
  labels:
    app.kubernetes.io/name: cm-secret-pruner
    app.kubernetes.io/component: cleanup
spec:
  schedule: "0 2 * * 0"
  timeZone: America/New_York
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 28800
      ttlSecondsAfterFinished: 86400
      template:
        spec:
          serviceAccountName: cm-secret-pruner
          restartPolicy: Never
          tolerations:
          - key: "node-role.kubernetes.io/infra"
            operator: "Exists"
            effect: "NoSchedule"
          containers:
          - name: pruner
            image: registry.redhat.io/openshift4/ose-cli:v4.16
            resources:
              requests:
                memory: "512Mi"
                cpu: "10m"
              limits:
                memory: "4Gi"
                cpu: "200m"
            env:
            - name: DELAY_BETWEEN_NAMESPACES
              value: "0.1"
            - name: DELAY_BETWEEN_DELETIONS
              value: "0.05"
            - name: BATCH_SIZE
              value: "10"
            - name: BATCH_PAUSE
              value: "0.5"
            command:
            - /bin/bash
            - -c
            - |
                set -uo pipefail
                
                DELAY_NS="${DELAY_BETWEEN_NAMESPACES:-0.5}"
                DELAY_DEL="${DELAY_BETWEEN_DELETIONS:-0.1}"
                BATCH_SIZE="${BATCH_SIZE:-10}"
                BATCH_PAUSE="${BATCH_PAUSE:-1}"
                
                total_deleted=0
                total_skipped=0
                total_protected=0
                total_too_new=0
                total_referenced=0
                total_system_cm=0
                total_errors=0
                namespaces_processed=0
                namespaces_excluded=0
                namespaces_high_volume=0
                namespaces_paginated=0
                namespaces_skipped_by_mode=0
                skipped_hv_count=0
                
                ns_deleted=0
                ns_skipped=0
                
                declare -a high_volume_ns_list
                declare -a error_list
                
                log() {
                  printf '%s [%s] %s\n' "$(TZ='America/New_York' date '+%Y-%m-%d %H:%M:%S')" "$1" "${*:2}" >&2
                }
                
                error_exit() {
                  log ERROR "$1"
                  exit 1
                }
                
                log INFO "ConfigMap Pruner - Starting"
                log INFO "Rate Limiting: ns_delay=${DELAY_NS}s del_delay=${DELAY_DEL}s batch=${BATCH_SIZE} pause=${BATCH_PAUSE}s"
                
                NS="configmap-secret-pruner"
                CM="cm-secret-pruner-config"
                
                if ! oc whoami &>/dev/null; then
                  error_exit "Unable to authenticate"
                fi
                
                if ! oc -n "$NS" get cm "$CM" &>/dev/null; then
                  error_exit "ConfigMap $CM not found in namespace $NS"
                fi
                
                DRY_RUN=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.dryRun}' 2>/dev/null || echo "true")
                [[ -z "$DRY_RUN" ]] && DRY_RUN="true"
                
                MIN_AGE_DAYS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.minAgeDays}' 2>/dev/null || echo "7")
                [[ -z "$MIN_AGE_DAYS" ]] && MIN_AGE_DAYS="7"
                
                HIGH_VOLUME_THRESHOLD=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.highVolumeThreshold}' 2>/dev/null || echo "1000")
                [[ -z "$HIGH_VOLUME_THRESHOLD" ]] && HIGH_VOLUME_THRESHOLD="1000"
                
                PROCESS_HIGH_VOLUME=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.processHighVolumeNamespaces}' 2>/dev/null || echo "false")
                [[ -z "$PROCESS_HIGH_VOLUME" ]] && PROCESS_HIGH_VOLUME="false"
                
                PAGINATION_LIMIT=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.paginationLimit}' 2>/dev/null || echo "500")
                [[ -z "$PAGINATION_LIMIT" ]] && PAGINATION_LIMIT="500"
                
                PROCESS_ONLY_HIGH_VOLUME=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.processOnlyHighVolume}' 2>/dev/null || echo "false")
                [[ -z "$PROCESS_ONLY_HIGH_VOLUME" ]] && PROCESS_ONLY_HIGH_VOLUME="false"
                
                EXCLUDED_NS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.excludedNamespaces}' 2>/dev/null || echo "")
                if [[ -z "$EXCLUDED_NS" ]]; then
                  EXCLUDED_NS=$(printf '%s\n' '^openshift-.*' '^kube-.*' '^default$' '^openshift$' '^configmap-secret-pruner$')
                fi
                
                PROTECTED_LABELS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.protectedLabels}' 2>/dev/null || echo "")
                if [[ -z "$PROTECTED_LABELS" ]]; then
                  PROTECTED_LABELS=$(printf '%s\n' 'app.kubernetes.io/managed-by=argocd' 'app.kubernetes.io/managed-by=Helm' 'meta.helm.sh/release-name' 'prune.protected=true')
                fi
                
                EXCLUDED_CM_NAMES=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.excludedConfigMapNames}' 2>/dev/null || echo "")
                if [[ -z "$EXCLUDED_CM_NAMES" ]]; then
                  EXCLUDED_CM_NAMES=$(printf '%s\n' \
                    'openshift-service-ca.crt' \
                    'kube-root-ca.crt' \
                    'istio-ca-root-cert' \
                    'openshift-ca.crt')
                fi
                
                EXCLUDED_NS=$(echo "$EXCLUDED_NS" | sed '/^[[:space:]]*$/d')
                PROTECTED_LABELS=$(echo "$PROTECTED_LABELS" | sed '/^[[:space:]]*$/d')
                EXCLUDED_CM_NAMES=$(echo "$EXCLUDED_CM_NAMES" | sed '/^[[:space:]]*$/d')
                
                log INFO "Config: DRY_RUN=$DRY_RUN MIN_AGE=$MIN_AGE_DAYS days THRESHOLD=$HIGH_VOLUME_THRESHOLD"
                log INFO "Config: PROCESS_HIGH_VOLUME=$PROCESS_HIGH_VOLUME PROCESS_ONLY_HIGH_VOLUME=$PROCESS_ONLY_HIGH_VOLUME"
                
                if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                  log INFO "Config: PAGINATION_LIMIT=$PAGINATION_LIMIT per page"
                fi
                
                log INFO "Detecting available API resources..."
                
                HAS_DEPLOYMENTS=0
                HAS_STATEFULSETS=0
                HAS_DAEMONSETS=0
                HAS_REPLICASETS=0
                HAS_REPLICATIONCONTROLLERS=0
                HAS_DEPLOYMENTCONFIGS=0
                HAS_CRONJOBS=0
                HAS_JOBS=0
                
                while IFS= read -r line; do
                  [[ -z "$line" ]] && continue
                  resource=$(echo "$line" | awk '{print $1}')
                  case "$resource" in
                    deployments) HAS_DEPLOYMENTS=1 ;;
                    statefulsets) HAS_STATEFULSETS=1 ;;
                    daemonsets) HAS_DAEMONSETS=1 ;;
                    replicasets) HAS_REPLICASETS=1 ;;
                    replicationcontrollers) HAS_REPLICATIONCONTROLLERS=1 ;;
                    deploymentconfigs) HAS_DEPLOYMENTCONFIGS=1 ;;
                    cronjobs) HAS_CRONJOBS=1 ;;
                    jobs) HAS_JOBS=1 ;;
                  esac
                done < <(oc api-resources --no-headers 2>/dev/null)
                
                log INFO "Available workload types: deploy=$HAS_DEPLOYMENTS sts=$HAS_STATEFULSETS ds=$HAS_DAEMONSETS rs=$HAS_REPLICASETS rc=$HAS_REPLICATIONCONTROLLERS dc=$HAS_DEPLOYMENTCONFIGS cronjob=$HAS_CRONJOBS job=$HAS_JOBS"
                
                fetch_workload_safe() {
                  local full_name="$1"
                  local ns="$2"
                  local temp_file
                  temp_file=$(mktemp)
                  
                  if ! oc get "$full_name" -n "$ns" -o json >"$temp_file" 2>/dev/null; then
                    echo '{"items":[]}'
                    rm -f "$temp_file"
                    return
                  fi
                  
                  if jq -e '(. | type == "object") and (has("items") or has("apiVersion"))' "$temp_file" >/dev/null 2>&1; then
                    cat "$temp_file"
                  else
                    echo '{"items":[]}'
                  fi
                  
                  rm -f "$temp_file"
                }
                
                get_workload_references() {
                  local ns="$1"
                  
                  local f_deploy=$(mktemp)
                  local f_sts=$(mktemp)
                  local f_ds=$(mktemp)
                  local f_rs=$(mktemp)
                  local f_cronjob=$(mktemp)
                  local f_job=$(mktemp)
                  local f_pod=$(mktemp)
                  local f_rc=$(mktemp)
                  local f_dc=$(mktemp)
                  local f_merged=$(mktemp)
                  
                  echo '{"items":[]}' > "$f_deploy"
                  echo '{"items":[]}' > "$f_sts"
                  echo '{"items":[]}' > "$f_ds"
                  echo '{"items":[]}' > "$f_rs"
                  echo '{"items":[]}' > "$f_cronjob"
                  echo '{"items":[]}' > "$f_job"
                  echo '{"items":[]}' > "$f_pod"
                  echo '{"items":[]}' > "$f_rc"
                  echo '{"items":[]}' > "$f_dc"
                  
                  [[ "$HAS_DEPLOYMENTS" == "1" ]] && fetch_workload_safe "deployments.apps" "$ns" > "$f_deploy"
                  [[ "$HAS_STATEFULSETS" == "1" ]] && fetch_workload_safe "statefulsets.apps" "$ns" > "$f_sts"
                  [[ "$HAS_DAEMONSETS" == "1" ]] && fetch_workload_safe "daemonsets.apps" "$ns" > "$f_ds"
                  [[ "$HAS_REPLICASETS" == "1" ]] && fetch_workload_safe "replicasets.apps" "$ns" > "$f_rs"
                  [[ "$HAS_CRONJOBS" == "1" ]] && fetch_workload_safe "cronjobs.batch" "$ns" > "$f_cronjob"
                  [[ "$HAS_JOBS" == "1" ]] && fetch_workload_safe "jobs.batch" "$ns" > "$f_job"
                  fetch_workload_safe "pods" "$ns" > "$f_pod"
                  [[ "$HAS_REPLICATIONCONTROLLERS" == "1" ]] && fetch_workload_safe "replicationcontrollers" "$ns" > "$f_rc"
                  [[ "$HAS_DEPLOYMENTCONFIGS" == "1" ]] && fetch_workload_safe "deploymentconfigs.apps.openshift.io" "$ns" > "$f_dc"
                  
                  if ! jq -s '{items: ([.[] | .items // []] | add)}' \
                    "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_cronjob" "$f_job" "$f_pod" "$f_rc" "$f_dc" \
                    > "$f_merged" 2>/dev/null; then
                    log WARN "Failed to merge workloads in $ns"
                    rm -f "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_cronjob" "$f_job" "$f_pod" "$f_rc" "$f_dc" "$f_merged"
                    echo ""
                    echo ""
                    return
                  fi
                  
                  # IMPROVEMENT 1: Fixed projected volume extraction with select(.configMap)
                  refs=$(jq -r '
                    [
                      .items[]? | 
                      (
                        (select(.kind == "Deployment" or .kind == "StatefulSet" or .kind == "DaemonSet" or .kind == "ReplicaSet" or .kind == "ReplicationController") |
                          .spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                            (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "DeploymentConfig") |
                          .spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                            (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "CronJob") |
                          .spec.jobTemplate.spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "Job") |
                          .spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "Pod") |
                          .spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                            (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        )
                      )
                    ] | unique | .[]
                  ' "$f_merged" 2>/dev/null || echo "")
                  
                  details=$(jq -r '
                    .items[]? | 
                    . as $item |
                    (
                      (select(.kind == "Deployment" or .kind == "StatefulSet" or .kind == "DaemonSet" or .kind == "ReplicaSet" or .kind == "ReplicationController") |
                        .spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                          (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):\($item.kind | ascii_downcase):\($item.metadata.name)"
                      ),
                      (select(.kind == "DeploymentConfig") |
                        .spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                          (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):deploymentconfig:\($item.metadata.name)"
                      ),
                      (select(.kind == "CronJob") |
                        .spec.jobTemplate.spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):cronjob:\($item.metadata.name)"
                      ),
                      (select(.kind == "Job") |
                        .spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):job:\($item.metadata.name)"
                      ),
                      (select(.kind == "Pod") |
                        .spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                          (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):pod:\($item.metadata.name)"
                      )
                    )
                  ' "$f_merged" 2>/dev/null || echo "")
                  
                  rm -f "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_cronjob" "$f_job" "$f_pod" "$f_rc" "$f_dc" "$f_merged"
                  
                  echo "---REFS-START---"
                  echo "$refs"
                  echo "---REFS-END---"
                  echo "$details"
                }
                
                process_configmaps() {
                  local target_ns="$1"
                  local cm_json="$2"
                  local referenced_cms="$3"
                  local workload_details="$4"
                  local deleted_count=0
                  local skipped_count=0
                  local batch_counter=0
                  
                  while IFS= read -r cm_data; do
                    [[ -z "$cm_data" ]] && continue
                    
                    name=$(echo "$cm_data" | jq -r '.name' 2>/dev/null)
                    [[ -z "$name" || "$name" == "null" ]] && continue
                    
                    annotations=$(echo "$cm_data" | jq -r '.annotations // {}' 2>/dev/null)
                    
                    inject_cabundle=$(echo "$annotations" | jq -r '."service.beta.openshift.io/inject-cabundle" // ."service.alpha.openshift.io/inject-cabundle" // empty' 2>/dev/null)
                    if [[ "$inject_cabundle" == "true" ]]; then
                      ((skipped_count++))
                      ((total_system_cm++))
                      log INFO "SKIPPED: $name (inject-cabundle)"
                      continue
                    fi
                    
                    excluded_by_name=false
                    while IFS= read -r excluded_name; do
                      [[ -z "$excluded_name" ]] && continue
                      if [[ "$name" == "$excluded_name" ]]; then
                        excluded_by_name=true
                        ((skipped_count++))
                        ((total_system_cm++))
                        log INFO "SKIPPED: $name (system)"
                        break
                      fi
                    done <<< "$EXCLUDED_CM_NAMES"
                    [[ "$excluded_by_name" == "true" ]] && continue
                    
                    labels=$(echo "$cm_data" | jq -r '.labels // {}' 2>/dev/null)
                    
                    inject_trusted_ca=$(echo "$labels" | jq -r '."config.openshift.io/inject-trusted-cabundle" // empty' 2>/dev/null)
                    if [[ "$inject_trusted_ca" == "true" ]]; then
                      ((skipped_count++))
                      ((total_system_cm++))
                      log INFO "SKIPPED: $name (inject-trusted-ca)"
                      continue
                    fi
                    
                    created=$(echo "$cm_data" | jq -r '.created' 2>/dev/null)
                    
                    skip=false
                    matched_label=""
                    while IFS= read -r label_rule; do
                      [[ -z "$label_rule" ]] && continue
                      
                      if [[ "$label_rule" == *"="* ]]; then
                        key="${label_rule%=*}"
                        expected="${label_rule#*=}"
                        actual=$(echo "$labels" | jq -r ".\\"$key\\" // empty" 2>/dev/null)
                        if [[ "$actual" == "$expected" ]]; then
                          skip=true
                          matched_label="$label_rule"
                          ((total_protected++))
                          break
                        fi
                      else
                        if echo "$labels" | jq -e ".\\"$label_rule\\"" &>/dev/null; then
                          skip=true
                          matched_label="$label_rule"
                          ((total_protected++))
                          break
                        fi
                      fi
                    done <<< "$PROTECTED_LABELS"
                    
                    if [[ "$skip" == "true" ]]; then
                      ((skipped_count++))
                      log INFO "PROTECTED: $name (label: $matched_label)"
                      continue
                    fi
                    
                    age_days="unknown"
                    if [[ -n "$created" && "$created" != "null" ]]; then
                      created_epoch=$(date -d "$created" +%s 2>/dev/null || echo "0")
                      if [[ "$created_epoch" -gt 0 ]]; then
                        age_seconds=$((current_epoch - created_epoch))
                        age_days=$((age_seconds / 86400))
                        
                        if [[ "$age_seconds" -lt "$min_age_seconds" ]]; then
                          ((skipped_count++))
                          ((total_too_new++))
                          log INFO "TOO-NEW: $name (age: ${age_days} days)"
                          continue
                        fi
                      fi
                    fi
                    
                    # IMPROVEMENT 2: Use printf and grep -Fxq for safer string matching
                    if printf '%s\n' "$referenced_cms" | grep -Fxq "$name" 2>/dev/null; then
                      ((skipped_count++))
                      ((total_referenced++))
                      
                      workload_ref=$(echo "$workload_details" | grep "^$name:" | head -1 | cut -d: -f2- || echo "unknown")
                      log INFO "REFERENCED: $name (by: $workload_ref)"
                      continue
                    fi
                    
                    if [[ "$DRY_RUN" == "false" ]]; then
                      sleep "$DELAY_DEL"
                      
                      if oc delete cm "$name" -n "$target_ns" --wait=false &>/dev/null; then
                        ((deleted_count++))
                        ((total_deleted++))
                        ((batch_counter++))
                        log INFO "DELETED: $name (age: ${age_days} days)"
                        
                        if [[ $((batch_counter % BATCH_SIZE)) -eq 0 ]]; then
                          sleep "$BATCH_PAUSE"
                        fi
                      else
                        ((total_errors++))
                        error_list+=("Failed to delete ConfigMap $name in namespace $target_ns")
                        log ERROR "DELETE-FAILED: $name"
                      fi
                    else
                      ((deleted_count++))
                      ((total_deleted++))
                      log INFO "DRY-RUN: Would delete $name (age: ${age_days} days, unreferenced)"
                    fi
                    
                  done < <(echo "$cm_json" | jq -c '.items[] | {name: .metadata.name, labels: .metadata.labels, annotations: .metadata.annotations, created: .metadata.creationTimestamp}' 2>/dev/null)
                  
                  ns_deleted=$deleted_count
                  ns_skipped=$skipped_count
                }
                
                process_high_volume_namespace() {
                  local ns="$1"
                  local total_cm_count="$2"
                  
                  log INFO "HIGH-VOLUME: $ns ($total_cm_count ConfigMaps) - using pagination"
                  
                  ((namespaces_high_volume++))
                  
                  if ! oc whoami &>/dev/null; then
                    log ERROR "Authentication check failed before pagination"
                    error_list+=("Failed to process namespace $ns - authentication expired")
                    ((total_errors++))
                    return
                  fi
                  
                  ref_result=$(get_workload_references "$ns")
                  referenced_cms=$(echo "$ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                  workload_details=$(echo "$ref_result" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                  
                  local deleted_in_ns=0
                  local skipped_in_ns=0
                  local pagination_failed=false
                  
                  local continue_token=""
                  local page_num=0
                  local total_pages=$(( (total_cm_count + PAGINATION_LIMIT - 1) / PAGINATION_LIMIT ))
                  local max_retries=3
                  
                  log INFO "PAGINATION: Processing $total_pages pages ($PAGINATION_LIMIT per page)"
                  
                  while true; do
                    ((page_num++))
                    
                    if [[ $page_num -gt $((total_pages + 10)) ]]; then
                      log ERROR "PAGE $page_num: Exceeded maximum page limit, possible infinite loop"
                      pagination_failed=true
                      break
                    fi
                    
                    page_result=""
                    local retry_count=0
                    local fetch_success=false
                    
                    while [[ $retry_count -lt $max_retries ]]; do
                      local api_url=""
                      if [[ -z "$continue_token" ]]; then
                        api_url="/api/v1/namespaces/$ns/configmaps?limit=$PAGINATION_LIMIT"
                      else
                        api_url="/api/v1/namespaces/$ns/configmaps?limit=$PAGINATION_LIMIT&continue=$continue_token"
                      fi
                      
                      page_result=$(oc get --raw "$api_url" 2>&1)
                      local fetch_exit_code=$?
                      
                      if [[ $fetch_exit_code -eq 0 ]] && echo "$page_result" | jq -e 'type == "object" and has("items") and (.items | type == "array")' >/dev/null 2>&1; then
                        fetch_success=true
                        break
                      else
                        ((retry_count++))
                        if [[ $retry_count -lt $max_retries ]]; then
                          sleep $((retry_count * 2))
                        fi
                      fi
                    done
                    
                    if [[ "$fetch_success" != "true" ]]; then
                      log WARN "PAGE $page_num: Failed after $max_retries retries"
                      pagination_failed=true
                      break
                    fi
                    
                    if [[ -n "$continue_token" ]] && echo "$page_result" | grep -q "continue token is expired\|too old" 2>/dev/null; then
                      log WARN "PAGE $page_num: Continue token expired, restarting from current position"
                      continue_token=""
                      ((page_num--))
                      sleep 2
                      continue
                    fi
                    
                    process_configmaps "$ns" "$page_result" "$referenced_cms" "$workload_details"
                    
                    ((deleted_in_ns += ns_deleted))
                    ((skipped_in_ns += ns_skipped))
                    
                    if [[ $((page_num % 10)) -eq 0 ]]; then
                      log INFO "PAGE $page_num/$total_pages: Complete (deleted=$deleted_in_ns skipped=$skipped_in_ns so far)"
                    fi
                    
                    continue_token=$(echo "$page_result" | jq -r '.metadata.continue // empty' 2>/dev/null)
                    
                    if [[ -z "$continue_token" ]]; then
                      break
                    fi
                    
                    sleep 1
                  done
                  
                  if [[ "$pagination_failed" == "true" ]]; then
                    log WARN "PAGINATION: Failed at page $page_num/$total_pages, using fallback"
                    
                    deleted_in_ns=0
                    skipped_in_ns=0
                    
                    log INFO "FALLBACK: Re-fetching current workload references..."
                    local ref_result_fallback=$(get_workload_references "$ns")
                    local referenced_cms_fallback=$(echo "$ref_result_fallback" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                    local workload_details_fallback=$(echo "$ref_result_fallback" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                    
                    local cm_json=""
                    if cm_json=$(oc get cm -n "$ns" -o json 2>&1); then
                      if echo "$cm_json" | jq -e 'type == "object" and has("items") and (.items | type == "array")' >/dev/null 2>&1; then
                        log INFO "FALLBACK: Fetching all ConfigMaps (may take 30-60s)..."
                        
                        process_configmaps "$ns" "$cm_json" "$referenced_cms_fallback" "$workload_details_fallback"
                        
                        deleted_in_ns=$ns_deleted
                        skipped_in_ns=$ns_skipped
                      else
                        log ERROR "FALLBACK: Invalid JSON response"
                        error_list+=("Failed to process namespace $ns - both pagination and fallback failed")
                        ((total_errors++))
                      fi
                    else
                      log ERROR "FALLBACK: Single fetch failed - $(echo "$cm_json" | head -c 100)"
                      error_list+=("Failed to process namespace $ns - both pagination and fallback failed")
                      ((total_errors++))
                    fi
                  fi
                  
                  log INFO "Namespace summary: deleted=$deleted_in_ns skipped=$skipped_in_ns"
                  ((total_skipped += skipped_in_ns))
                }
                
                namespace_list=$(oc get ns -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | sort)
                if [[ -z "$namespace_list" ]]; then
                  error_exit "No namespaces found"
                fi
                namespace_count=$(echo "$namespace_list" | wc -l)
                log INFO "Found $namespace_count namespaces in cluster"
                
                current_epoch=$(date +%s)
                min_age_seconds=$((MIN_AGE_DAYS * 86400))
                
                start_time=$(date +%s)
                
                while IFS= read -r ns; do
                  [[ -z "$ns" ]] && continue
                  
                  excluded=false
                  while IFS= read -r pattern; do
                    [[ -z "$pattern" ]] && continue
                    if echo "$ns" | grep -Eq "$pattern" 2>/dev/null; then
                      excluded=true
                      break
                    fi
                  done <<< "$EXCLUDED_NS"
                  
                  if [[ "$excluded" == "true" ]]; then
                    ((namespaces_excluded++))
                    continue
                  fi
                  
                  ((namespaces_processed++))
                  
                  if [[ $((namespaces_processed % 50)) -eq 0 ]]; then
                    elapsed=$(($(date +%s) - start_time))
                    log INFO "Progress: $namespaces_processed namespaces, ${elapsed}s elapsed"
                  fi
                  
                  log INFO "-----------------------------------------------"
                  log INFO "Processing Namespace [$namespaces_processed/$((namespace_count - namespaces_excluded))]: $ns"
                  log INFO "-----------------------------------------------"
                  
                  cm_count=0
                  if ! cm_count=$(oc get cm -n "$ns" --no-headers 2>/dev/null | wc -l); then
                    log ERROR "Unable to count ConfigMaps in $ns"
                    error_list+=("Unable to count ConfigMaps in namespace: $ns")
                    ((total_errors++))
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  if [[ "$cm_count" -eq 0 ]]; then
                    log INFO "No ConfigMaps found"
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  log INFO "Found $cm_count ConfigMap(s)"
                  
                  if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                    if [[ "$cm_count" -le "$HIGH_VOLUME_THRESHOLD" ]]; then
                      log INFO "SKIPPED: High-volume-only mode (threshold: $HIGH_VOLUME_THRESHOLD)"
                      ((namespaces_skipped_by_mode++))
                      sleep 0.01
                      continue
                    fi
                  fi
                  
                  if [[ "$cm_count" -gt "$HIGH_VOLUME_THRESHOLD" ]]; then
                    if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                      process_high_volume_namespace "$ns" "$cm_count"
                    else
                      log WARN "HIGH-VOLUME: Skipped (threshold: $HIGH_VOLUME_THRESHOLD)"
                      log WARN "Set processHighVolumeNamespaces=true to enable"
                      high_volume_ns_list+=("$ns:$cm_count")
                      ((namespaces_high_volume++))
                    fi
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  ref_result=$(get_workload_references "$ns")
                  referenced_cms=$(echo "$ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                  workload_details=$(echo "$ref_result" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                  
                  cm_json=""
                  if ! cm_json=$(oc get cm -n "$ns" -o json 2>/dev/null); then
                    log ERROR "Unable to list ConfigMaps in $ns"
                    error_list+=("Unable to list ConfigMaps in namespace: $ns")
                    ((total_errors++))
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  process_configmaps "$ns" "$cm_json" "$referenced_cms" "$workload_details"
                  
                  log INFO "Namespace summary: deleted=$ns_deleted skipped=$ns_skipped"
                  ((total_skipped += ns_skipped))
                  
                  sleep "$DELAY_NS"
                  
                done <<< "$namespace_list"
                
                end_time=$(date +%s)
                duration=$((end_time - start_time))
                duration_min=$((duration / 60))
                duration_sec=$((duration % 60))
                
                log INFO ""
                log INFO "================================================"
                log INFO "Job Completed Successfully"
                log INFO "================================================"
                log INFO "Duration: ${duration_min}m ${duration_sec}s"
                log INFO ""
                log INFO "Namespace Statistics:"
                log INFO "  Total in cluster: $namespace_count"
                log INFO "  Excluded by pattern: $namespaces_excluded"
                if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                  log INFO "  Skipped (high-volume-only mode): $namespaces_skipped_by_mode"
                fi
                log INFO "  High-volume (processed): $namespaces_high_volume"
                skipped_hv_count=0
                if [[ -n "${high_volume_ns_list+x}" ]] && [[ ${#high_volume_ns_list[@]} -gt 0 ]]; then
                  skipped_hv_count="${#high_volume_ns_list[@]}"
                  log INFO "  High-volume (skipped): $skipped_hv_count"
                fi
                log INFO "  Processed normally: $((namespaces_processed - namespaces_high_volume - namespaces_skipped_by_mode))"
                log INFO ""
                
                if [[ "$skipped_hv_count" -gt 0 ]]; then
                  log WARN "High-Volume Namespaces Skipped:"
                  for entry in "${high_volume_ns_list[@]}"; do
                    ns_name="${entry%:*}"
                    ns_count="${entry#*:}"
                    log WARN "  - $ns_name: $ns_count ConfigMaps"
                  done
                  log WARN "Set processHighVolumeNamespaces=true to process these"
                  log WARN ""
                fi
                
                log INFO "ConfigMap Statistics:"
                log INFO "  Deleted (or would delete): $total_deleted"
                log INFO "  Skipped total: $total_skipped"
                log INFO "    - System (injected): $total_system_cm"
                log INFO "    - Protected (labels): $total_protected"
                log INFO "    - Too new (< $MIN_AGE_DAYS days): $total_too_new"
                log INFO "    - Referenced (workloads): $total_referenced"
                log INFO ""
                
                if [[ "$total_errors" -gt 0 ]]; then
                  log ERROR "Errors Encountered: $total_errors"
                  for error_msg in "${error_list[@]}"; do
                    log ERROR "  $error_msg"
                  done
                  log ERROR ""
                else
                  log INFO "Errors: 0"
                fi
                
                log INFO "Mode: $(if [[ "$DRY_RUN" == "true" ]]; then echo "DRY-RUN"; else echo "LIVE"; fi)"
                if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                  log INFO "High-Volume Processing: ENABLED (limit=$PAGINATION_LIMIT, fallback available)"
                fi
                if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                  log INFO "High-Volume Only: ENABLED"
                fi
                log INFO "================================================"
                
                exit 0


----------
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cm-secret-pruner
  namespace: configmap-secret-pruner
  labels:
    app.kubernetes.io/name: cm-secret-pruner
    app.kubernetes.io/component: cleanup
spec:
  schedule: "0 2 * * 0"
  timeZone: America/New_York
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 28800
      ttlSecondsAfterFinished: 86400
      template:
        spec:
          serviceAccountName: cm-secret-pruner
          restartPolicy: Never
          tolerations:
          - key: "node-role.kubernetes.io/infra"
            operator: "Exists"
            effect: "NoSchedule"
          containers:
          - name: pruner
            image: registry.redhat.io/openshift4/ose-cli:v4.16
            resources:
              requests:
                memory: "512Mi"
                cpu: "10m"
              limits:
                memory: "4Gi"
                cpu: "200m"
            env:
            - name: DELAY_BETWEEN_NAMESPACES
              value: "0.1"
            - name: DELAY_BETWEEN_DELETIONS
              value: "0.05"
            - name: BATCH_SIZE
              value: "10"
            - name: BATCH_PAUSE
              value: "0.5"
            command:
            - /bin/bash
            - -c
            - |
                set -uo pipefail
                
                DELAY_NS="${DELAY_BETWEEN_NAMESPACES:-0.5}"
                DELAY_DEL="${DELAY_BETWEEN_DELETIONS:-0.1}"
                BATCH_SIZE="${BATCH_SIZE:-10}"
                BATCH_PAUSE="${BATCH_PAUSE:-1}"
                
                total_deleted=0
                total_skipped=0
                total_protected=0
                total_too_new=0
                total_referenced=0
                total_system_cm=0
                total_errors=0
                namespaces_processed=0
                namespaces_excluded=0
                namespaces_high_volume=0
                namespaces_paginated=0
                namespaces_skipped_by_mode=0
                
                ns_deleted=0
                ns_skipped=0
                
                declare -a high_volume_ns_list
                declare -a error_list
                
                log() {
                  printf '%s [%s] %s\n' "$(TZ='America/New_York' date '+%Y-%m-%d %H:%M:%S')" "$1" "${*:2}" >&2
                }
                
                error_exit() {
                  log ERROR "$1"
                  exit 1
                }
                
                log INFO "ConfigMap Pruner - Starting"
                log INFO "Rate Limiting: ns_delay=${DELAY_NS}s del_delay=${DELAY_DEL}s batch=${BATCH_SIZE} pause=${BATCH_PAUSE}s"
                
                NS="configmap-secret-pruner"
                CM="cm-secret-pruner-config"
                
                if ! oc whoami &>/dev/null; then
                  error_exit "Unable to authenticate"
                fi
                
                if ! oc -n "$NS" get cm "$CM" &>/dev/null; then
                  error_exit "ConfigMap $CM not found in namespace $NS"
                fi
                
                DRY_RUN=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.dryRun}' 2>/dev/null || echo "true")
                [[ -z "$DRY_RUN" ]] && DRY_RUN="true"
                
                MIN_AGE_DAYS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.minAgeDays}' 2>/dev/null || echo "7")
                [[ -z "$MIN_AGE_DAYS" ]] && MIN_AGE_DAYS="7"
                
                HIGH_VOLUME_THRESHOLD=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.highVolumeThreshold}' 2>/dev/null || echo "1000")
                [[ -z "$HIGH_VOLUME_THRESHOLD" ]] && HIGH_VOLUME_THRESHOLD="1000"
                
                PROCESS_HIGH_VOLUME=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.processHighVolumeNamespaces}' 2>/dev/null || echo "false")
                [[ -z "$PROCESS_HIGH_VOLUME" ]] && PROCESS_HIGH_VOLUME="false"
                
                PAGINATION_LIMIT=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.paginationLimit}' 2>/dev/null || echo "500")
                [[ -z "$PAGINATION_LIMIT" ]] && PAGINATION_LIMIT="500"
                
                PROCESS_ONLY_HIGH_VOLUME=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.processOnlyHighVolume}' 2>/dev/null || echo "false")
                [[ -z "$PROCESS_ONLY_HIGH_VOLUME" ]] && PROCESS_ONLY_HIGH_VOLUME="false"
                
                EXCLUDED_NS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.excludedNamespaces}' 2>/dev/null || echo "")
                if [[ -z "$EXCLUDED_NS" ]]; then
                  EXCLUDED_NS=$(printf '%s\n' '^openshift-.*' '^kube-.*' '^default$' '^openshift$' '^configmap-secret-pruner$')
                fi
                
                PROTECTED_LABELS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.protectedLabels}' 2>/dev/null || echo "")
                if [[ -z "$PROTECTED_LABELS" ]]; then
                  PROTECTED_LABELS=$(printf '%s\n' 'app.kubernetes.io/managed-by=argocd' 'app.kubernetes.io/managed-by=Helm' 'meta.helm.sh/release-name' 'prune.protected=true')
                fi
                
                EXCLUDED_CM_NAMES=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.excludedConfigMapNames}' 2>/dev/null || echo "")
                if [[ -z "$EXCLUDED_CM_NAMES" ]]; then
                  EXCLUDED_CM_NAMES=$(printf '%s\n' \
                    'openshift-service-ca.crt' \
                    'kube-root-ca.crt' \
                    'istio-ca-root-cert' \
                    'openshift-ca.crt')
                fi
                
                EXCLUDED_NS=$(echo "$EXCLUDED_NS" | sed '/^[[:space:]]*$/d')
                PROTECTED_LABELS=$(echo "$PROTECTED_LABELS" | sed '/^[[:space:]]*$/d')
                EXCLUDED_CM_NAMES=$(echo "$EXCLUDED_CM_NAMES" | sed '/^[[:space:]]*$/d')
                
                log INFO "Config: DRY_RUN=$DRY_RUN MIN_AGE=$MIN_AGE_DAYS days THRESHOLD=$HIGH_VOLUME_THRESHOLD"
                log INFO "Config: PROCESS_HIGH_VOLUME=$PROCESS_HIGH_VOLUME PROCESS_ONLY_HIGH_VOLUME=$PROCESS_ONLY_HIGH_VOLUME"
                
                if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                  log INFO "Config: PAGINATION_LIMIT=$PAGINATION_LIMIT per page"
                fi
                
                log INFO "Detecting available API resources..."
                
                HAS_DEPLOYMENTS=0
                HAS_STATEFULSETS=0
                HAS_DAEMONSETS=0
                HAS_REPLICASETS=0
                HAS_REPLICATIONCONTROLLERS=0
                HAS_DEPLOYMENTCONFIGS=0
                HAS_CRONJOBS=0
                HAS_JOBS=0
                
                while IFS= read -r line; do
                  [[ -z "$line" ]] && continue
                  resource=$(echo "$line" | awk '{print $1}')
                  case "$resource" in
                    deployments) HAS_DEPLOYMENTS=1 ;;
                    statefulsets) HAS_STATEFULSETS=1 ;;
                    daemonsets) HAS_DAEMONSETS=1 ;;
                    replicasets) HAS_REPLICASETS=1 ;;
                    replicationcontrollers) HAS_REPLICATIONCONTROLLERS=1 ;;
                    deploymentconfigs) HAS_DEPLOYMENTCONFIGS=1 ;;
                    cronjobs) HAS_CRONJOBS=1 ;;
                    jobs) HAS_JOBS=1 ;;
                  esac
                done < <(oc api-resources --no-headers 2>/dev/null)
                
                log INFO "Available workload types: deploy=$HAS_DEPLOYMENTS sts=$HAS_STATEFULSETS ds=$HAS_DAEMONSETS rs=$HAS_REPLICASETS rc=$HAS_REPLICATIONCONTROLLERS dc=$HAS_DEPLOYMENTCONFIGS cronjob=$HAS_CRONJOBS job=$HAS_JOBS"
                
                fetch_workload_safe() {
                  local full_name="$1"
                  local ns="$2"
                  local temp_file
                  temp_file=$(mktemp)
                  
                  if ! oc get "$full_name" -n "$ns" -o json >"$temp_file" 2>/dev/null; then
                    echo '{"items":[]}'
                    rm -f "$temp_file"
                    return
                  fi
                  
                  if jq -e '(. | type == "object") and (has("items") or has("apiVersion"))' "$temp_file" >/dev/null 2>&1; then
                    cat "$temp_file"
                  else
                    echo '{"items":[]}'
                  fi
                  
                  rm -f "$temp_file"
                }
                
                get_workload_references() {
                  local ns="$1"
                  
                  local f_deploy=$(mktemp)
                  local f_sts=$(mktemp)
                  local f_ds=$(mktemp)
                  local f_rs=$(mktemp)
                  local f_cronjob=$(mktemp)
                  local f_job=$(mktemp)
                  local f_pod=$(mktemp)
                  local f_rc=$(mktemp)
                  local f_dc=$(mktemp)
                  local f_merged=$(mktemp)
                  
                  echo '{"items":[]}' > "$f_deploy"
                  echo '{"items":[]}' > "$f_sts"
                  echo '{"items":[]}' > "$f_ds"
                  echo '{"items":[]}' > "$f_rs"
                  echo '{"items":[]}' > "$f_cronjob"
                  echo '{"items":[]}' > "$f_job"
                  echo '{"items":[]}' > "$f_pod"
                  echo '{"items":[]}' > "$f_rc"
                  echo '{"items":[]}' > "$f_dc"
                  
                  [[ "$HAS_DEPLOYMENTS" == "1" ]] && fetch_workload_safe "deployments.apps" "$ns" > "$f_deploy"
                  [[ "$HAS_STATEFULSETS" == "1" ]] && fetch_workload_safe "statefulsets.apps" "$ns" > "$f_sts"
                  [[ "$HAS_DAEMONSETS" == "1" ]] && fetch_workload_safe "daemonsets.apps" "$ns" > "$f_ds"
                  [[ "$HAS_REPLICASETS" == "1" ]] && fetch_workload_safe "replicasets.apps" "$ns" > "$f_rs"
                  [[ "$HAS_CRONJOBS" == "1" ]] && fetch_workload_safe "cronjobs.batch" "$ns" > "$f_cronjob"
                  [[ "$HAS_JOBS" == "1" ]] && fetch_workload_safe "jobs.batch" "$ns" > "$f_job"
                  fetch_workload_safe "pods" "$ns" > "$f_pod"
                  [[ "$HAS_REPLICATIONCONTROLLERS" == "1" ]] && fetch_workload_safe "replicationcontrollers" "$ns" > "$f_rc"
                  [[ "$HAS_DEPLOYMENTCONFIGS" == "1" ]] && fetch_workload_safe "deploymentconfigs.apps.openshift.io" "$ns" > "$f_dc"
                  
                  if ! jq -s '{items: ([.[] | .items // []] | add)}' \
                    "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_cronjob" "$f_job" "$f_pod" "$f_rc" "$f_dc" \
                    > "$f_merged" 2>/dev/null; then
                    log WARN "Failed to merge workloads in $ns"
                    rm -f "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_cronjob" "$f_job" "$f_pod" "$f_rc" "$f_dc" "$f_merged"
                    echo ""
                    echo ""
                    return
                  fi
                  
                  # IMPROVEMENT 1: Fixed projected volume extraction with select(.configMap)
                  refs=$(jq -r '
                    [
                      .items[]? | 
                      (
                        (select(.kind == "Deployment" or .kind == "StatefulSet" or .kind == "DaemonSet" or .kind == "ReplicaSet" or .kind == "ReplicationController") |
                          .spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                            (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "DeploymentConfig") |
                          .spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                            (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "CronJob") |
                          .spec.jobTemplate.spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "Job") |
                          .spec.template.spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        ),
                        (select(.kind == "Pod") |
                          .spec | (
                            (.volumes[]?.configMap?.name // empty),
                            (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                            (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                            (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                            (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                          )
                        )
                      )
                    ] | unique | .[]
                  ' "$f_merged" 2>/dev/null || echo "")
                  
                  details=$(jq -r '
                    .items[]? | 
                    . as $item |
                    (
                      (select(.kind == "Deployment" or .kind == "StatefulSet" or .kind == "DaemonSet" or .kind == "ReplicaSet" or .kind == "ReplicationController") |
                        .spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                          (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):\($item.kind | ascii_downcase):\($item.metadata.name)"
                      ),
                      (select(.kind == "DeploymentConfig") |
                        .spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                          (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):deploymentconfig:\($item.metadata.name)"
                      ),
                      (select(.kind == "CronJob") |
                        .spec.jobTemplate.spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):cronjob:\($item.metadata.name)"
                      ),
                      (select(.kind == "Job") |
                        .spec.template.spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):job:\($item.metadata.name)"
                      ),
                      (select(.kind == "Pod") |
                        .spec | 
                        [
                          (.volumes[]?.configMap?.name // empty),
                          (.volumes[]?.projected?.sources[]? | select(.configMap).configMap.name // empty),
                          (.containers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.containers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty),
                          (.initContainers[]?.envFrom[]?.configMapRef?.name // empty),
                          (.initContainers[]?.env[]?.valueFrom?.configMapKeyRef?.name // empty)
                        ] | .[] | select(length > 0) | "\(.):pod:\($item.metadata.name)"
                      )
                    )
                  ' "$f_merged" 2>/dev/null || echo "")
                  
                  rm -f "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_cronjob" "$f_job" "$f_pod" "$f_rc" "$f_dc" "$f_merged"
                  
                  echo "---REFS-START---"
                  echo "$refs"
                  echo "---REFS-END---"
                  echo "$details"
                }
                
                process_configmaps() {
                  local target_ns="$1"
                  local cm_json="$2"
                  local referenced_cms="$3"
                  local workload_details="$4"
                  local deleted_count=0
                  local skipped_count=0
                  local batch_counter=0
                  
                  while IFS= read -r cm_data; do
                    [[ -z "$cm_data" ]] && continue
                    
                    name=$(echo "$cm_data" | jq -r '.name' 2>/dev/null)
                    [[ -z "$name" || "$name" == "null" ]] && continue
                    
                    annotations=$(echo "$cm_data" | jq -r '.annotations // {}' 2>/dev/null)
                    
                    inject_cabundle=$(echo "$annotations" | jq -r '."service.beta.openshift.io/inject-cabundle" // ."service.alpha.openshift.io/inject-cabundle" // empty' 2>/dev/null)
                    if [[ "$inject_cabundle" == "true" ]]; then
                      ((skipped_count++))
                      ((total_system_cm++))
                      log INFO "SKIPPED: $name (inject-cabundle)"
                      continue
                    fi
                    
                    excluded_by_name=false
                    while IFS= read -r excluded_name; do
                      [[ -z "$excluded_name" ]] && continue
                      if [[ "$name" == "$excluded_name" ]]; then
                        excluded_by_name=true
                        ((skipped_count++))
                        ((total_system_cm++))
                        log INFO "SKIPPED: $name (system)"
                        break
                      fi
                    done <<< "$EXCLUDED_CM_NAMES"
                    [[ "$excluded_by_name" == "true" ]] && continue
                    
                    labels=$(echo "$cm_data" | jq -r '.labels // {}' 2>/dev/null)
                    
                    inject_trusted_ca=$(echo "$labels" | jq -r '."config.openshift.io/inject-trusted-cabundle" // empty' 2>/dev/null)
                    if [[ "$inject_trusted_ca" == "true" ]]; then
                      ((skipped_count++))
                      ((total_system_cm++))
                      log INFO "SKIPPED: $name (inject-trusted-ca)"
                      continue
                    fi
                    
                    created=$(echo "$cm_data" | jq -r '.created' 2>/dev/null)
                    
                    skip=false
                    matched_label=""
                    while IFS= read -r label_rule; do
                      [[ -z "$label_rule" ]] && continue
                      
                      if [[ "$label_rule" == *"="* ]]; then
                        key="${label_rule%=*}"
                        expected="${label_rule#*=}"
                        actual=$(echo "$labels" | jq -r ".\\"$key\\" // empty" 2>/dev/null)
                        if [[ "$actual" == "$expected" ]]; then
                          skip=true
                          matched_label="$label_rule"
                          ((total_protected++))
                          break
                        fi
                      else
                        if echo "$labels" | jq -e ".\\"$label_rule\\"" &>/dev/null; then
                          skip=true
                          matched_label="$label_rule"
                          ((total_protected++))
                          break
                        fi
                      fi
                    done <<< "$PROTECTED_LABELS"
                    
                    if [[ "$skip" == "true" ]]; then
                      ((skipped_count++))
                      log INFO "PROTECTED: $name (label: $matched_label)"
                      continue
                    fi
                    
                    age_days="unknown"
                    if [[ -n "$created" && "$created" != "null" ]]; then
                      created_epoch=$(date -d "$created" +%s 2>/dev/null || echo "0")
                      if [[ "$created_epoch" -gt 0 ]]; then
                        age_seconds=$((current_epoch - created_epoch))
                        age_days=$((age_seconds / 86400))
                        
                        if [[ "$age_seconds" -lt "$min_age_seconds" ]]; then
                          ((skipped_count++))
                          ((total_too_new++))
                          log INFO "TOO-NEW: $name (age: ${age_days} days)"
                          continue
                        fi
                      fi
                    fi
                    
                    # IMPROVEMENT 2: Use printf and grep -Fxq for safer string matching
                    if printf '%s\n' "$referenced_cms" | grep -Fxq "$name" 2>/dev/null; then
                      ((skipped_count++))
                      ((total_referenced++))
                      
                      workload_ref=$(echo "$workload_details" | grep "^$name:" | head -1 | cut -d: -f2- || echo "unknown")
                      log INFO "REFERENCED: $name (by: $workload_ref)"
                      continue
                    fi
                    
                    if [[ "$DRY_RUN" == "false" ]]; then
                      sleep "$DELAY_DEL"
                      
                      if oc delete cm "$name" -n "$target_ns" --wait=false &>/dev/null; then
                        ((deleted_count++))
                        ((total_deleted++))
                        ((batch_counter++))
                        log INFO "DELETED: $name (age: ${age_days} days)"
                        
                        if [[ $((batch_counter % BATCH_SIZE)) -eq 0 ]]; then
                          sleep "$BATCH_PAUSE"
                        fi
                      else
                        ((total_errors++))
                        error_list+=("Failed to delete ConfigMap $name in namespace $target_ns")
                        log ERROR "DELETE-FAILED: $name"
                      fi
                    else
                      ((deleted_count++))
                      ((total_deleted++))
                      log INFO "DRY-RUN: Would delete $name (age: ${age_days} days, unreferenced)"
                    fi
                    
                  done < <(echo "$cm_json" | jq -c '.items[] | {name: .metadata.name, labels: .metadata.labels, annotations: .metadata.annotations, created: .metadata.creationTimestamp}' 2>/dev/null)
                  
                  ns_deleted=$deleted_count
                  ns_skipped=$skipped_count
                }
                
                process_high_volume_namespace() {
                  local ns="$1"
                  local total_cm_count="$2"
                  
                  log INFO "HIGH-VOLUME: $ns ($total_cm_count ConfigMaps) - using pagination"
                  
                  ((namespaces_high_volume++))
                  
                  ref_result=$(get_workload_references "$ns")
                  referenced_cms=$(echo "$ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                  workload_details=$(echo "$ref_result" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                  
                  local deleted_in_ns=0
                  local skipped_in_ns=0
                  local pagination_failed=false
                  
                  local continue_token=""
                  local page_num=0
                  local total_pages=$(( (total_cm_count + PAGINATION_LIMIT - 1) / PAGINATION_LIMIT ))
                  
                  log INFO "PAGINATION: Processing $total_pages pages ($PAGINATION_LIMIT per page)"
                  
                  while true; do
                    ((page_num++))
                    
                    page_result=""
                    local retry_count=0
                    local max_retries=3
                    local fetch_success=false
                    
                    while [[ $retry_count -lt $max_retries ]]; do
                      local api_url=""
                      if [[ -z "$continue_token" ]]; then
                        api_url="/api/v1/namespaces/$ns/configmaps?limit=$PAGINATION_LIMIT"
                      else
                        api_url="/api/v1/namespaces/$ns/configmaps?limit=$PAGINATION_LIMIT&continue=$continue_token"
                      fi
                      
                      page_result=$(oc get --raw "$api_url" 2>&1)
                      local fetch_exit_code=$?
                      
                      if [[ $fetch_exit_code -eq 0 ]] && echo "$page_result" | jq -e '.items' >/dev/null 2>&1; then
                        fetch_success=true
                        break
                      else
                        ((retry_count++))
                        if [[ $retry_count -lt $max_retries ]]; then
                          sleep 2
                        fi
                      fi
                    done
                    
                    if [[ "$fetch_success" != "true" ]]; then
                      log WARN "PAGE $page_num: Failed after $max_retries retries"
                      pagination_failed=true
                      break
                    fi
                    
                    process_configmaps "$ns" "$page_result" "$referenced_cms" "$workload_details"
                    
                    ((deleted_in_ns += ns_deleted))
                    ((skipped_in_ns += ns_skipped))
                    
                    if [[ $((page_num % 10)) -eq 0 ]]; then
                      log INFO "PAGE $page_num/$total_pages: Complete (deleted=$deleted_in_ns skipped=$skipped_in_ns so far)"
                    fi
                    
                    continue_token=$(echo "$page_result" | jq -r '.metadata.continue // empty' 2>/dev/null)
                    
                    if [[ -z "$continue_token" ]]; then
                      break
                    fi
                    
                    sleep 1
                  done
                  
                  if [[ "$pagination_failed" == "true" ]]; then
                    log WARN "PAGINATION: Failed at page $page_num/$total_pages, using fallback"
                    
                    deleted_in_ns=0
                    skipped_in_ns=0
                    
                    local cm_json=""
                    if cm_json=$(oc get cm -n "$ns" -o json 2>&1); then
                      if echo "$cm_json" | jq -e '.items' >/dev/null 2>&1; then
                        log INFO "FALLBACK: Fetching all ConfigMaps (may take 30-60s)..."
                        
                        process_configmaps "$ns" "$cm_json" "$referenced_cms" "$workload_details"
                        
                        deleted_in_ns=$ns_deleted
                        skipped_in_ns=$ns_skipped
                      else
                        log ERROR "FALLBACK: Invalid JSON response"
                        error_list+=("Failed to process namespace $ns - both pagination and fallback failed")
                        ((total_errors++))
                      fi
                    else
                      log ERROR "FALLBACK: Single fetch failed - $(echo "$cm_json" | head -c 100)"
                      error_list+=("Failed to process namespace $ns - both pagination and fallback failed")
                      ((total_errors++))
                    fi
                  fi
                  
                  log INFO "Namespace summary: deleted=$deleted_in_ns skipped=$skipped_in_ns"
                  ((total_skipped += skipped_in_ns))
                }
                
                namespace_list=$(oc get ns -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | sort)
                if [[ -z "$namespace_list" ]]; then
                  error_exit "No namespaces found"
                fi
                namespace_count=$(echo "$namespace_list" | wc -l)
                log INFO "Found $namespace_count namespaces in cluster"
                
                current_epoch=$(date +%s)
                min_age_seconds=$((MIN_AGE_DAYS * 86400))
                
                start_time=$(date +%s)
                
                while IFS= read -r ns; do
                  [[ -z "$ns" ]] && continue
                  
                  excluded=false
                  while IFS= read -r pattern; do
                    [[ -z "$pattern" ]] && continue
                    if echo "$ns" | grep -Eq "$pattern" 2>/dev/null; then
                      excluded=true
                      break
                    fi
                  done <<< "$EXCLUDED_NS"
                  
                  if [[ "$excluded" == "true" ]]; then
                    ((namespaces_excluded++))
                    continue
                  fi
                  
                  ((namespaces_processed++))
                  
                  if [[ $((namespaces_processed % 50)) -eq 0 ]]; then
                    elapsed=$(($(date +%s) - start_time))
                    log INFO "Progress: $namespaces_processed namespaces, ${elapsed}s elapsed"
                  fi
                  
                  log INFO "-----------------------------------------------"
                  log INFO "Processing Namespace [$namespaces_processed/$((namespace_count - namespaces_excluded))]: $ns"
                  log INFO "-----------------------------------------------"
                  
                  cm_count=0
                  if ! cm_count=$(oc get cm -n "$ns" --no-headers 2>/dev/null | wc -l); then
                    log ERROR "Unable to count ConfigMaps in $ns"
                    error_list+=("Unable to count ConfigMaps in namespace: $ns")
                    ((total_errors++))
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  if [[ "$cm_count" -eq 0 ]]; then
                    log INFO "No ConfigMaps found"
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  log INFO "Found $cm_count ConfigMap(s)"
                  
                  if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                    if [[ "$cm_count" -le "$HIGH_VOLUME_THRESHOLD" ]]; then
                      log INFO "SKIPPED: High-volume-only mode (threshold: $HIGH_VOLUME_THRESHOLD)"
                      ((namespaces_skipped_by_mode++))
                      sleep 0.01
                      continue
                    fi
                  fi
                  
                  if [[ "$cm_count" -gt "$HIGH_VOLUME_THRESHOLD" ]]; then
                    if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                      process_high_volume_namespace "$ns" "$cm_count"
                    else
                      log WARN "HIGH-VOLUME: Skipped (threshold: $HIGH_VOLUME_THRESHOLD)"
                      log WARN "Set processHighVolumeNamespaces=true to enable"
                      high_volume_ns_list+=("$ns:$cm_count")
                      ((namespaces_high_volume++))
                    fi
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  ref_result=$(get_workload_references "$ns")
                  referenced_cms=$(echo "$ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                  workload_details=$(echo "$ref_result" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                  
                  cm_json=""
                  if ! cm_json=$(oc get cm -n "$ns" -o json 2>/dev/null); then
                    log ERROR "Unable to list ConfigMaps in $ns"
                    error_list+=("Unable to list ConfigMaps in namespace: $ns")
                    ((total_errors++))
                    sleep "$DELAY_NS"
                    continue
                  fi
                  
                  process_configmaps "$ns" "$cm_json" "$referenced_cms" "$workload_details"
                  
                  log INFO "Namespace summary: deleted=$ns_deleted skipped=$ns_skipped"
                  ((total_skipped += ns_skipped))
                  
                  sleep "$DELAY_NS"
                  
                done <<< "$namespace_list"
                
                end_time=$(date +%s)
                duration=$((end_time - start_time))
                duration_min=$((duration / 60))
                duration_sec=$((duration % 60))
                
                log INFO ""
                log INFO "================================================"
                log INFO "Job Completed Successfully"
                log INFO "================================================"
                log INFO "Duration: ${duration_min}m ${duration_sec}s"
                log INFO ""
                log INFO "Namespace Statistics:"
                log INFO "  Total in cluster: $namespace_count"
                log INFO "  Excluded by pattern: $namespaces_excluded"
                if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                  log INFO "  Skipped (high-volume-only mode): $namespaces_skipped_by_mode"
                fi
                log INFO "  High-volume (processed): $namespaces_high_volume"
                local skipped_hv_count=0
                if [[ -n "${high_volume_ns_list+x}" ]]; then
                  skipped_hv_count="${#high_volume_ns_list[@]}"
                fi
                if [[ "$skipped_hv_count" -gt 0 ]]; then
                  log INFO "  High-volume (skipped): $skipped_hv_count"
                fi
                log INFO "  Processed normally: $((namespaces_processed - namespaces_high_volume - namespaces_skipped_by_mode))"
                log INFO ""
                
                if [[ "$skipped_hv_count" -gt 0 ]]; then
                  log WARN "High-Volume Namespaces Skipped:"
                  for entry in "${high_volume_ns_list[@]}"; do
                    ns_name="${entry%:*}"
                    ns_count="${entry#*:}"
                    log WARN "  - $ns_name: $ns_count ConfigMaps"
                  done
                  log WARN "Set processHighVolumeNamespaces=true to process these"
                  log WARN ""
                fi
                
                log INFO "ConfigMap Statistics:"
                log INFO "  Deleted (or would delete): $total_deleted"
                log INFO "  Skipped total: $total_skipped"
                log INFO "    - System (injected): $total_system_cm"
                log INFO "    - Protected (labels): $total_protected"
                log INFO "    - Too new (< $MIN_AGE_DAYS days): $total_too_new"
                log INFO "    - Referenced (workloads): $total_referenced"
                log INFO ""
                
                if [[ "$total_errors" -gt 0 ]]; then
                  log ERROR "Errors Encountered: $total_errors"
                  for error_msg in "${error_list[@]}"; do
                    log ERROR "  $error_msg"
                  done
                  log ERROR ""
                else
                  log INFO "Errors: 0"
                fi
                
                log INFO "Mode: $(if [[ "$DRY_RUN" == "true" ]]; then echo "DRY-RUN"; else echo "LIVE"; fi)"
                if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                  log INFO "High-Volume Processing: ENABLED (limit=$PAGINATION_LIMIT, fallback available)"
                fi
                if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                  log INFO "High-Volume Only: ENABLED"
                fi
                log INFO "================================================"
                
                exit 0
---------
# 1. Create namespace
oc new-project mimir

# 2. Get ODF S3 credentials
SECRET=$(oc get secret -n openshift-storage -l app=rook-ceph-rgw -o name | head -1 | cut -d/ -f2)
ACCESS_KEY=$(oc get secret -n openshift-storage $SECRET -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
SECRET_KEY=$(oc get secret -n openshift-storage $SECRET -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

# 3. Install Mimir via Helm (replace <ODF_S3_ROUTE> below!)
helm repo add grafana https://grafana.github.io/helm-charts
helm upgrade --install mimir grafana/mimir \
  --namespace mimir \
  --set rbac.pspEnabled=false \
  --set image.tag="3.0.0" \
  --set storage.backend=s3 \
  --set storage.s3.bucketNames.chunks="mimir-chunks" \
  --set storage.s3.bucketNames.ruler="mimir-ruler" \
  --set storage.s3.bucketNames.admin="mimir-admin" \
  --set storage.s3.endpoint="<ODF_S3_ROUTE>" \
  --set storage.s3.region="us-east-1" \
  --set storage.s3.accessKeyId="$ACCESS_KEY" \
  --set storage.s3.secretAccessKey="$SECRET_KEY" \
  --set storage.s3.pathStyle=true \
  --set storage.s3.signatureVersion=v4 \
  --set limits.retention=8760h \
  --wait

# 4. Expose via Route
cat <<EOF | oc apply -f -
apiVersion: route.openshift.io/v1
kind: Route
meta
  name: mimir-remote-write
  namespace: mimir
spec:
  to: { kind: Service, name: mimir-gateway }
  port: { targetPort: http }
  tls: { termination: edge, insecurePolicy: Redirect }
EOF

# 5. Get Mimir URL (save this!)
MIMIR_URL=$(oc get route mimir-remote-write -n mimir -o jsonpath='{.spec.host}')
echo "‚úÖ Central Mimir ready! Remote write URL: https://$MIMIR_URL/api/v1/push"


-----

# 1. Create namespace
oc new-project observability

# 2. Deploy alert-exporter
cat <<EOF | oc apply -f -
apiVersion: apps/v1
kind: Deployment
meta
  name: alert-exporter
  namespace: observability
spec:
  replicas: 1
  selector: { matchLabels: { app: alert-exporter } }
  template:
    meta
      labels: { app: alert-exporter }
    spec:
      containers:
      - name: exporter
        image: quay.io/prometheuscommunity/alertmanager-webhook-exporter:latest
        ports: [{ containerPort: 9099 }]
        env:
        - name: CLUSTER_NAME
          valueFrom:
            fieldRef: { fieldPath: metadata.labels['cluster.openshift.io/cluster-name'] }
---
apiVersion: v1
kind: Service
meta
  name: alert-exporter
  namespace: observability
spec:
  selector: { app: alert-exporter }
  ports: [{ port: 9099, targetPort: 9099 }]
EOF

----\
# 1. Add Alertmanager receiver
cat <<EOF | oc apply -f -
apiVersion: monitoring.coreos.com/v1alpha1
kind: AlertmanagerConfig
meta
  name: alert-exporter-receiver
  namespace: openshift-user-workload-monitoring
spec:
  route:
    receiver: "alert-exporter"
    routes:
    - matchers: [{ name: severity, value: info }]; receiver: "alert-exporter"
    - matchers: [{ name: severity, value: warning }]; receiver: "alert-exporter"
  receivers:
  - name: "alert-exporter"
    webhookConfigs:
    - url: "http://alert-exporter.observability.svc:9099/alerts"
      sendResolved: true
EOF

# 2. Configure Prometheus remote_write (replace <MIMIR_URL> and <SPOKE_CLUSTER_NAME>)
oc patch configmap prometheus-user-workload -n openshift-user-workload-monitoring --type=merge -p="
data:
  prometheus.yaml: |
    remoteWrite:
    - url: https://<MIMIR_URL>/api/v1/push
      headers:
        X-Scope-OrgID: \"<SPOKE_CLUSTER_NAME>\"
      remoteTimeout: 30s
      queueConfig:
        capacity: 10000
"



-------
curl -X POST \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR-GRAFANA-API-KEY" \
  -d '{
    "name": "Splunk HEC",
    "type": "webhook",
    "isDefault": false,
    "sendReminder": false,
    "settings": {
      "url": "https://your-splunk-server:8088/services/collector/event",
      "httpMethod": "POST",
      "httpHeaderName1": "Authorization",
      "httpHeaderValue1": "Splunk YOUR-HEC-TOKEN-HERE",
      "httpHeaderName2": "Content-Type",
      "httpHeaderValue2": "application/json"
    }
  }' \
  https://your-grafana-instance/api/alert-notifications
-----
apiVersion: logging.openshift.io/v1
kind: ClusterLogForwarder
metadata:
  name: instance
  namespace: openshift-logging
spec:
  inputs:
    - name: prometheus-alerts
      type: http
      http:
        port: 8786
        path: /alerts
        format: json

  outputs:
    # Ensure this matches your existing LokiStack output
    - name: loki
      type: lokistack
      lokistack:
        name: lokistack  # ‚Üê must match your LokiStack CR name in openshift-logging

  pipelines:
    - name: alerts-to-loki
      inputRefs:
        - prometheus-alerts
      outputRefs:
        - loki
      # Optional: parse as JSON (implied by input format, but safe to include)
      parse: json
      labels:
        # Static labels applied to all logs in this pipeline
        job: "prometheus-alerts"
        cluster: "your-cluster-name"  # ‚Üê Replace per cluster (e.g., via GitOps)

---------
apiVersion: batch/v1
kind: CronJob
metadata:
  name: secret-pruner
  namespace: configmap-secret-pruner
  labels:
    app.kubernetes.io/name: secret-pruner
    app.kubernetes.io/component: cleanup
    app.kubernetes.io/version: "2.0-bulletproof"
spec:
  schedule: "0 4 * * 0"
  timeZone: America/New_York
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 2
      activeDeadlineSeconds: 14400
      ttlSecondsAfterFinished: 86400
      template:
        spec:
          serviceAccountName: cm-secret-pruner
          restartPolicy: Never
          tolerations:
          - key: "node-role.kubernetes.io/infra"
            operator: "Exists"
            effect: "NoSchedule"
          containers:
          - name: pruner
            image: registry.redhat.io/openshift4/ose-cli:v4.16
            resources:
              requests:
                memory: "256Mi"
                cpu: "20m"
              limits:
                memory: "1Gi"
                cpu: "500m"
            env:
            - name: DELAY_BETWEEN_NAMESPACES
              value: "0.5"
            - name: DELAY_BETWEEN_DELETIONS
              value: "0.1"
            - name: BATCH_SIZE
              value: "10"
            - name: BATCH_PAUSE
              value: "1"
            command:
            - /bin/bash
            - -c
            - |
              set -uo pipefail
              
              DELAY_NS="${DELAY_BETWEEN_NAMESPACES:-0.5}"
              DELAY_DEL="${DELAY_BETWEEN_DELETIONS:-0.1}"
              BATCH_SIZE="${BATCH_SIZE:-10}"
              BATCH_PAUSE="${BATCH_PAUSE:-1}"
              
              total_deleted=0
              total_skipped=0
              total_protected=0
              total_too_new=0
              total_referenced=0
              total_system_secret=0
              total_route_tls=0
              total_sa_refs=0
              total_no_timestamp=0
              total_errors=0
              total_continue_token_retries=0
              total_json_validation_failures=0
              namespaces_processed=0
              namespaces_excluded=0
              namespaces_high_volume=0
              namespaces_paginated=0
              namespaces_skipped_by_mode=0
              
              ns_deleted=0
              ns_skipped=0
              
              declare -a high_volume_ns_list
              declare -a error_list
              declare -a deleted_secrets_list
              
              log() {
                printf '%s [%s] %s\n' "$(TZ='America/New_York' date '+%Y-%m-%d %H:%M:%S')" "$1" "${*:2}" >&2
              }
              
              error_exit() {
                log ERROR "$1"
                exit 1
              }
              
              log INFO "Secret Pruner - Starting (Bulletproof Version 2.0)"
              log INFO "Rate Limiting: ns_delay=${DELAY_NS}s del_delay=${DELAY_DEL}s batch=${BATCH_SIZE} pause=${BATCH_PAUSE}s"
              
              NS="configmap-secret-pruner"
              CM="secret-pruner-config"
              
              if ! oc whoami &>/dev/null; then
                error_exit "Unable to authenticate"
              fi
              
              if ! oc -n "$NS" get cm "$CM" &>/dev/null; then
                error_exit "ConfigMap $CM not found in namespace $NS"
              fi
              
              # Load configuration
              DRY_RUN=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.dryRun}' 2>/dev/null || echo "true")
              [[ -z "$DRY_RUN" ]] && DRY_RUN="true"
              
              MIN_AGE_DAYS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.minAgeDays}' 2>/dev/null || echo "30")
              [[ -z "$MIN_AGE_DAYS" ]] && MIN_AGE_DAYS="30"
              
              PRUNE_WITHOUT_TIMESTAMP=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.pruneSecretsWithoutTimestamp}' 2>/dev/null || echo "false")
              [[ -z "$PRUNE_WITHOUT_TIMESTAMP" ]] && PRUNE_WITHOUT_TIMESTAMP="false"
              
              HIGH_VOLUME_THRESHOLD=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.highVolumeThreshold}' 2>/dev/null || echo "1000")
              [[ -z "$HIGH_VOLUME_THRESHOLD" ]] && HIGH_VOLUME_THRESHOLD="1000"
              
              PROCESS_HIGH_VOLUME=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.processHighVolumeNamespaces}' 2>/dev/null || echo "false")
              [[ -z "$PROCESS_HIGH_VOLUME" ]] && PROCESS_HIGH_VOLUME="false"
              
              PAGINATION_LIMIT=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.paginationLimit}' 2>/dev/null || echo "500")
              [[ -z "$PAGINATION_LIMIT" ]] && PAGINATION_LIMIT="500"
              
              PROCESS_ONLY_HIGH_VOLUME=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.processOnlyHighVolume}' 2>/dev/null || echo "false")
              [[ -z "$PROCESS_ONLY_HIGH_VOLUME" ]] && PROCESS_ONLY_HIGH_VOLUME="false"
              
              EXCLUDED_NS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.excludedNamespaces}' 2>/dev/null || echo "")
              if [[ -z "$EXCLUDED_NS" ]]; then
                EXCLUDED_NS=$(printf '%s\n' '^openshift-.*' '^kube-.*' '^default$' '^openshift$' '^configmap-secret-pruner$')
              fi
              
              EXCLUDED_TYPES=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.excludedSecretTypes}' 2>/dev/null || echo "")
              if [[ -z "$EXCLUDED_TYPES" ]]; then
                EXCLUDED_TYPES=$(printf '%s\n' \
                  'kubernetes.io/service-account-token' \
                  'kubernetes.io/dockercfg' \
                  'kubernetes.io/dockerconfigjson' \
                  'kubernetes.io/basic-auth' \
                  'kubernetes.io/ssh-auth' \
                  'bootstrap.kubernetes.io/token' \
                  'istio.io/ca-root' \
                  'kubernetes.io/tls')
              fi
              
              PROTECTED_LABELS=$(oc -n "$NS" get cm "$CM" -o jsonpath='{.data.protectedLabels}' 2>/dev/null || echo "")
              if [[ -z "$PROTECTED_LABELS" ]]; then
                PROTECTED_LABELS=$(printf '%s\n' \
                  'app.kubernetes.io/managed-by=argocd' \
                  'app.kubernetes.io/managed-by=Helm' \
                  'meta.helm.sh/release-name' \
                  'prune.protected=true' \
                  'operators.coreos.com/' \
                  'olm.operatorgroup.name')
              fi
              
              EXCLUDED_NS=$(echo "$EXCLUDED_NS" | sed '/^[[:space:]]*$/d')
              EXCLUDED_TYPES=$(echo "$EXCLUDED_TYPES" | sed '/^[[:space:]]*$/d')
              PROTECTED_LABELS=$(echo "$PROTECTED_LABELS" | sed '/^[[:space:]]*$/d')
              
              log INFO "Config: DRY_RUN=$DRY_RUN MIN_AGE=$MIN_AGE_DAYS days THRESHOLD=$HIGH_VOLUME_THRESHOLD"
              log INFO "Config: PROCESS_HIGH_VOLUME=$PROCESS_HIGH_VOLUME PRUNE_WITHOUT_TIMESTAMP=$PRUNE_WITHOUT_TIMESTAMP"
              
              if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                log INFO "Config: PAGINATION_LIMIT=$PAGINATION_LIMIT per page"
              fi
              
              log INFO "Detecting available API resources..."
              
              HAS_DEPLOYMENTS=0
              HAS_STATEFULSETS=0
              HAS_DAEMONSETS=0
              HAS_REPLICASETS=0
              HAS_PODS=0
              HAS_JOBS=0
              HAS_CRONJOBS=0
              HAS_ROUTES=0
              HAS_SERVICEACCOUNTS=0
              HAS_INGRESS=0
              
              while IFS= read -r line; do
                [[ -z "$line" ]] && continue
                resource=$(echo "$line" | awk '{print $1}')
                case "$resource" in
                  deployments) HAS_DEPLOYMENTS=1 ;;
                  statefulsets) HAS_STATEFULSETS=1 ;;
                  daemonsets) HAS_DAEMONSETS=1 ;;
                  replicasets) HAS_REPLICASETS=1 ;;
                  pods) HAS_PODS=1 ;;
                  jobs) HAS_JOBS=1 ;;
                  cronjobs) HAS_CRONJOBS=1 ;;
                  routes) HAS_ROUTES=1 ;;
                  serviceaccounts) HAS_SERVICEACCOUNTS=1 ;;
                  ingresses) HAS_INGRESS=1 ;;
                esac
              done < <(oc api-resources --no-headers 2>/dev/null)
              
              log INFO "Available resources: deploy=$HAS_DEPLOYMENTS sts=$HAS_STATEFULSETS ds=$HAS_DAEMONSETS rs=$HAS_REPLICASETS"
              log INFO "                     pods=$HAS_PODS job=$HAS_JOBS cronjob=$HAS_CRONJOBS route=$HAS_ROUTES sa=$HAS_SERVICEACCOUNTS ingress=$HAS_INGRESS"
              
              fetch_workload_safe() {
                local full_name="$1"
                local ns="$2"
                local temp_file
                temp_file=$(mktemp)
                
                if ! oc get "$full_name" -n "$ns" -o json >"$temp_file" 2>/dev/null; then
                  echo '{"items":[]}'
                  rm -f "$temp_file"
                  return
                fi
                
                if jq -e '(. | type == "object") and (has("items") or has("apiVersion"))' "$temp_file" >/dev/null 2>&1; then
                  cat "$temp_file"
                else
                  echo '{"items":[]}'
                fi
                
                rm -f "$temp_file"
              }
              
              # FIX #1: Corrected Route TLS reference extraction - secretName only
              get_route_references() {
                local ns="$1"
                
                if [[ "$HAS_ROUTES" != "1" ]]; then
                  echo ""
                  return
                fi
                
                # FIXED: Only extract spec.tls.secretName, not certificate content
                oc get routes.route.openshift.io -n "$ns" -o json 2>/dev/null | \
                  jq -r '
                    [
                      .items[]? |
                      (.spec.tls?.secretName // empty)
                    ] | unique | .[] | select(length > 0)
                  ' 2>/dev/null || echo ""
              }
              
              get_serviceaccount_references() {
                local ns="$1"
                
                if [[ "$HAS_SERVICEACCOUNTS" != "1" ]]; then
                  echo ""
                  return
                fi
                
                oc get serviceaccounts -n "$ns" -o json 2>/dev/null | \
                  jq -r '
                    [
                      .items[]? |
                      (
                        ((.imagePullSecrets // [])[]?.name // empty),
                        ((.secrets // [])[]?.name // empty)
                      )
                    ] | unique | .[] | select(length > 0)
                  ' 2>/dev/null || echo ""
              }
              
              get_ingress_references() {
                local ns="$1"
                
                if [[ "$HAS_INGRESS" != "1" ]]; then
                  echo ""
                  return
                fi
                
                oc get ingresses.networking.k8s.io -n "$ns" -o json 2>/dev/null | \
                  jq -r '
                    [
                      .items[]? |
                      ((.spec.tls // [])[]?.secretName // empty)
                    ] | unique | .[] | select(length > 0)
                  ' 2>/dev/null || echo ""
              }
              
              get_workload_references() {
                local ns="$1"
                
                local f_deploy=$(mktemp)
                local f_sts=$(mktemp)
                local f_ds=$(mktemp)
                local f_rs=$(mktemp)
                local f_job=$(mktemp)
                local f_cronjob=$(mktemp)
                local f_pod=$(mktemp)
                local f_merged=$(mktemp)
                
                # FIX #2: Trap ensures cleanup even on early return/error
                cleanup_temp_files() {
                  rm -f "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_job" "$f_cronjob" "$f_pod" "$f_merged"
                }
                trap cleanup_temp_files RETURN
                
                echo '{"items":[]}' > "$f_deploy"
                echo '{"items":[]}' > "$f_sts"
                echo '{"items":[]}' > "$f_ds"
                echo '{"items":[]}' > "$f_rs"
                echo '{"items":[]}' > "$f_job"
                echo '{"items":[]}' > "$f_cronjob"
                echo '{"items":[]}' > "$f_pod"
                
                [[ "$HAS_DEPLOYMENTS" == "1" ]] && fetch_workload_safe "deployments.apps" "$ns" > "$f_deploy"
                [[ "$HAS_STATEFULSETS" == "1" ]] && fetch_workload_safe "statefulsets.apps" "$ns" > "$f_sts"
                [[ "$HAS_DAEMONSETS" == "1" ]] && fetch_workload_safe "daemonsets.apps" "$ns" > "$f_ds"
                [[ "$HAS_REPLICASETS" == "1" ]] && fetch_workload_safe "replicasets.apps" "$ns" > "$f_rs"
                [[ "$HAS_JOBS" == "1" ]] && fetch_workload_safe "jobs.batch" "$ns" > "$f_job"
                [[ "$HAS_CRONJOBS" == "1" ]] && fetch_workload_safe "cronjobs.batch" "$ns" > "$f_cronjob"
                [[ "$HAS_PODS" == "1" ]] && fetch_workload_safe "pods" "$ns" > "$f_pod"
                
                if ! jq -s '{items: ([.[] | .items // []] | add)}' \
                  "$f_deploy" "$f_sts" "$f_ds" "$f_rs" "$f_job" "$f_cronjob" "$f_pod" \
                  > "$f_merged" 2>/dev/null; then
                  log WARN "Failed to merge workloads in $ns"
                  echo ""
                  echo ""
                  return
                fi
                
                refs=$(jq -r '
                  [
                    .items[]? |
                    (
                      (select(.kind == "Deployment" or .kind == "StatefulSet" or .kind == "DaemonSet" or .kind == "ReplicaSet") |
                        .spec.template.spec | (
                          ((.volumes // [])[]?.secret?.secretName // empty),
                          ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                          ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty),
                          ((.initContainers // [])[]?.envFrom[]?.secretRef?.name // empty),
                          ((.initContainers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                        )
                      ),
                      (select(.kind == "CronJob") |
                        .spec.jobTemplate.spec.template.spec | (
                          ((.volumes // [])[]?.secret?.secretName // empty),
                          ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                          ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                        )
                      ),
                      (select(.kind == "Job") |
                        .spec.template.spec | (
                          ((.volumes // [])[]?.secret?.secretName // empty),
                          ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                          ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                        )
                      ),
                      (select(.kind == "Pod") |
                        .spec | (
                          ((.volumes // [])[]?.secret?.secretName // empty),
                          ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                          ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty),
                          ((.initContainers // [])[]?.envFrom[]?.secretRef?.name // empty),
                          ((.initContainers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                        )
                      )
                    )
                  ] | unique | .[] | select(length > 0)
                ' "$f_merged" 2>/dev/null || echo "")
                
                details=$(jq -r '
                  .items[]? |
                  . as $item |
                  (
                    (select(.kind == "Deployment" or .kind == "StatefulSet" or .kind == "DaemonSet" or .kind == "ReplicaSet") |
                      .spec.template.spec |
                      [
                        ((.volumes // [])[]?.secret?.secretName // empty),
                        ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                        ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty),
                        ((.initContainers // [])[]?.envFrom[]?.secretRef?.name // empty),
                        ((.initContainers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                      ] | .[] | select(length > 0) | "\(.):deployment:\($item.metadata.name)"
                    ),
                    (select(.kind == "CronJob") |
                      .spec.jobTemplate.spec.template.spec |
                      [
                        ((.volumes // [])[]?.secret?.secretName // empty),
                        ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                        ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                      ] | .[] | select(length > 0) | "\(.):cronjob:\($item.metadata.name)"
                    ),
                    (select(.kind == "Job") |
                      .spec.template.spec |
                      [
                        ((.volumes // [])[]?.secret?.secretName // empty),
                        ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                        ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                      ] | .[] | select(length > 0) | "\(.):job:\($item.metadata.name)"
                    ),
                    (select(.kind == "Pod") |
                      .spec |
                      [
                        ((.volumes // [])[]?.secret?.secretName // empty),
                        ((.containers // [])[]?.envFrom[]?.secretRef?.name // empty),
                        ((.containers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty),
                        ((.initContainers // [])[]?.envFrom[]?.secretRef?.name // empty),
                        ((.initContainers // [])[]?.env[]?.valueFrom?.secretKeyRef?.name // empty)
                      ] | .[] | select(length > 0) | "\(.):pod:\($item.metadata.name)"
                    )
                  )
                ' "$f_merged" 2>/dev/null || echo "")
                
                route_refs=$(get_route_references "$ns")
                sa_refs=$(get_serviceaccount_references "$ns")
                ingress_refs=$(get_ingress_references "$ns")
                
                all_refs=$(printf '%s\n%s\n%s\n%s\n' "$refs" "$route_refs" "$sa_refs" "$ingress_refs" | sort -u | grep -v '^$')
                
                additional_details=""
                while IFS= read -r secret_name; do
                  [[ -z "$secret_name" ]] && continue
                  if echo "$route_refs" | grep -Fxq "$secret_name" 2>/dev/null; then
                    additional_details+="$secret_name:route:tls"$'\n'
                  fi
                  if echo "$sa_refs" | grep -Fxq "$secret_name" 2>/dev/null; then
                    additional_details+="$secret_name:serviceaccount:pull-secret"$'\n'
                  fi
                  if echo "$ingress_refs" | grep -Fxq "$secret_name" 2>/dev/null; then
                    additional_details+="$secret_name:ingress:tls"$'\n'
                  fi
                done <<< "$route_refs$sa_refs$ingress_refs"
                
                combined_details=$(printf '%s\n%s' "$details" "$additional_details" | grep -v '^$')
                
                echo "---REFS-START---"
                echo "$all_refs"
                echo "---REFS-END---"
                echo "$combined_details"
              }
              
              process_secrets() {
                local target_ns="$1"
                local secret_json="$2"
                local referenced_secrets="$3"
                local workload_details="$4"
                local deleted_count=0
                local skipped_count=0
                local batch_counter=0
                
                while IFS= read -r secret_data; do
                  [[ -z "$secret_data" ]] && continue
                  
                  name=$(echo "$secret_data" | jq -r '.name' 2>/dev/null)
                  [[ -z "$name" || "$name" == "null" ]] && continue
                  
                  secret_type=$(echo "$secret_data" | jq -r '.type // "Opaque"' 2>/dev/null)
                  
                  excluded_by_type=false
                  while IFS= read -r excluded_type; do
                    [[ -z "$excluded_type" ]] && continue
                    if [[ "$secret_type" == "$excluded_type" ]]; then
                      excluded_by_type=true
                      ((skipped_count++))
                      ((total_system_secret++))
                      log INFO "SKIPPED: $name (type: $secret_type)"
                      break
                    fi
                  done <<< "$EXCLUDED_TYPES"
                  [[ "$excluded_by_type" == "true" ]] && continue
                  
                  labels=$(echo "$secret_data" | jq -r '.labels // {}' 2>/dev/null)
                  
                  skip=false
                  matched_label=""
                  while IFS= read -r label_rule; do
                    [[ -z "$label_rule" ]] && continue
                    
                    if [[ "$label_rule" == *"/"* ]] && [[ "$label_rule" != *"="* ]]; then
                      if echo "$labels" | jq -e "to_entries[] | select(.key | startswith(\"${label_rule}\"))" &>/dev/null; then
                        skip=true
                        matched_label="$label_rule*"
                        ((total_protected++))
                        break
                      fi
                    elif [[ "$label_rule" == *"="* ]]; then
                      key="${label_rule%=*}"
                      expected="${label_rule#*=}"
                      actual=$(echo "$labels" | jq -r ".\"$key\" // empty" 2>/dev/null)
                      if [[ "$actual" == "$expected" ]]; then
                        skip=true
                        matched_label="$label_rule"
                        ((total_protected++))
                        break
                      fi
                    else
                      if echo "$labels" | jq -e ".\"$label_rule\"" &>/dev/null; then
                        skip=true
                        matched_label="$label_rule"
                        ((total_protected++))
                        break
                      fi
                    fi
                  done <<< "$PROTECTED_LABELS"
                  
                  if [[ "$skip" == "true" ]]; then
                    ((skipped_count++))
                    log INFO "PROTECTED: $name (label: $matched_label)"
                    continue
                  fi
                  
                  created=$(echo "$secret_data" | jq -r '.created' 2>/dev/null)
                  age_check_passed=true
                  
                  if [[ -n "$created" && "$created" != "null" ]]; then
                    created_epoch=$(date -d "$created" +%s 2>/dev/null || echo "0")
                    if [[ "$created_epoch" -gt 0 ]]; then
                      age_seconds=$((current_epoch - created_epoch))
                      age_days=$((age_seconds / 86400))
                      
                      if [[ "$age_seconds" -lt "$min_age_seconds" ]]; then
                        age_check_passed=false
                        ((skipped_count++))
                        ((total_too_new++))
                        log INFO "TOO-NEW: $name (age: ${age_days} days)"
                      fi
                    else
                      if [[ "$PRUNE_WITHOUT_TIMESTAMP" == "false" ]]; then
                        age_check_passed=false
                        ((skipped_count++))
                        ((total_no_timestamp++))
                        log INFO "INVALID-TIMESTAMP: $name (skipped due to config)"
                      fi
                    fi
                  else
                    if [[ "$PRUNE_WITHOUT_TIMESTAMP" == "false" ]]; then
                      age_check_passed=false
                      ((skipped_count++))
                      ((total_no_timestamp++))
                      log INFO "NO-TIMESTAMP: $name (skipped due to config)"
                    fi
                  fi
                  
                  if [[ "$age_check_passed" == "false" ]]; then
                    continue
                  fi
                  
                  if echo "$referenced_secrets" | grep -Fxq "$name" 2>/dev/null; then
                    ((skipped_count++))
                    
                    workload_ref=$(echo "$workload_details" | grep "^$name:" | head -1 | cut -d: -f2- || echo "unknown")
                    
                    if [[ "$workload_ref" == "route:"* ]]; then
                      ((total_route_tls++))
                      log INFO "REFERENCED: $name (by Route TLS)"
                    elif [[ "$workload_ref" == "serviceaccount:"* ]]; then
                      ((total_sa_refs++))
                      log INFO "REFERENCED: $name (by ServiceAccount)"
                    else
                      ((total_referenced++))
                      log INFO "REFERENCED: $name (by: $workload_ref)"
                    fi
                    continue
                  fi
                  
                  if [[ "$DRY_RUN" == "false" ]]; then
                    current_ref_result=$(get_workload_references "$target_ns")
                    current_referenced=$(echo "$current_ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                    
                    if echo "$current_referenced" | grep -Fxq "$name" 2>/dev/null; then
                      ((skipped_count++))
                      ((total_referenced++))
                      log WARN "RACE-PREVENTED: $name became referenced, skipping"
                      continue
                    fi
                    
                    sleep "$DELAY_DEL"
                    
                    deletion_output=$(oc delete secret "$name" -n "$target_ns" --wait=false 2>&1)
                    if [[ $? -eq 0 ]]; then
                      ((deleted_count++))
                      ((total_deleted++))
                      ((batch_counter++))
                      deleted_secrets_list+=("$target_ns/$name")
                      log INFO "DELETED: $name"
                      
                      if [[ $((batch_counter % BATCH_SIZE)) -eq 0 ]]; then
                        sleep "$BATCH_PAUSE"
                      fi
                    else
                      ((total_errors++))
                      error_msg="Failed to delete Secret $name in namespace $target_ns: $deletion_output"
                      error_list+=("$error_msg")
                      log ERROR "DELETE-FAILED: $name - $deletion_output"
                    fi
                  else
                    ((deleted_count++))
                    ((total_deleted++))
                    log INFO "DRY-RUN: Would delete $name (unreferenced, age OK)"
                  fi
                  
                done < <(echo "$secret_json" | jq -c '.items[] | {name: .metadata.name, type: .type, labels: .metadata.labels, created: .metadata.creationTimestamp}' 2>/dev/null)
                
                ns_deleted=$deleted_count
                ns_skipped=$skipped_count
              }
              
              # FIX #3: Enhanced pagination with JSON validation and continue token retry
              process_high_volume_namespace() {
                local ns="$1"
                local total_secret_count="$2"
                
                log INFO "HIGH-VOLUME: $ns ($total_secret_count Secrets) - using pagination"
                
                ((namespaces_paginated++))
                
                ref_result=$(get_workload_references "$ns")
                referenced_secrets=$(echo "$ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                workload_details=$(echo "$ref_result" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                
                local deleted_in_ns=0
                local skipped_in_ns=0
                local continue_token=""
                local page_num=0
                local total_pages=$(( (total_secret_count + PAGINATION_LIMIT - 1) / PAGINATION_LIMIT ))
                local retry_count=0
                local max_retries=3
                
                log INFO "PAGINATION: Processing ~$total_pages pages ($PAGINATION_LIMIT per page)"
                
                while true; do
                  ((page_num++))
                  log INFO "PAGE $page_num/$total_pages: Fetching..."
                  
                  page_result=""
                  if [[ -z "$continue_token" ]]; then
                    page_result=$(oc get secret -n "$ns" --limit="$PAGINATION_LIMIT" -o json 2>/dev/null)
                  else
                    page_result=$(oc get secret -n "$ns" --limit="$PAGINATION_LIMIT" --continue="$continue_token" -o json 2>/dev/null)
                  fi
                  
                  # FIX #3a: Check for empty response
                  if [[ -z "$page_result" ]]; then
                    log ERROR "PAGE $page_num: Empty response"
                    error_list+=("Empty response for page $page_num in namespace $ns")
                    ((total_errors++))
                    break
                  fi
                  
                  # FIX #3b: Validate JSON before processing
                  if ! echo "$page_result" | jq empty >/dev/null 2>&1; then
                    ((total_json_validation_failures++))
                    
                    # Check if it's a continue token expiration error
                    if echo "$page_result" | grep -qE "continue token.*expired|too old resource version"; then
                      log WARN "PAGE $page_num: Continue token expired"
                      
                      if [[ $retry_count -lt $max_retries ]]; then
                        ((retry_count++))
                        ((total_continue_token_retries++))
                        log WARN "Restarting pagination from beginning (retry $retry_count/$max_retries)"
                        continue_token=""
                        page_num=0
                        deleted_in_ns=0
                        skipped_in_ns=0
                        sleep 2
                        continue
                      else
                        log ERROR "PAGE $page_num: Max retries reached for continue token expiration"
                        error_list+=("Continue token expired and max retries reached in namespace $ns")
                        ((total_errors++))
                        break
                      fi
                    else
                      log ERROR "PAGE $page_num: Invalid JSON response"
                      error_list+=("Invalid JSON for page $page_num in namespace $ns")
                      ((total_errors++))
                      break
                    fi
                  fi
                  
                  # Reset retry count on successful fetch
                  retry_count=0
                  
                  log INFO "PAGE $page_num: Processing..."
                  process_secrets "$ns" "$page_result" "$referenced_secrets" "$workload_details"
                  
                  ((deleted_in_ns += ns_deleted))
                  ((skipped_in_ns += ns_skipped))
                  
                  log INFO "PAGE $page_num: Complete (deleted=$ns_deleted skipped=$ns_skipped)"
                  
                  continue_token=$(echo "$page_result" | jq -r '.metadata.continue // empty' 2>/dev/null)
                  
                  if [[ -z "$continue_token" ]]; then
                    log INFO "PAGE $page_num: Last page reached"
                    break
                  fi
                  
                  sleep 0.5
                done
                
                log INFO "Namespace summary: deleted=$deleted_in_ns skipped=$skipped_in_ns pages=$page_num"
                ((total_skipped += skipped_in_ns))
              }
              
              namespace_list=$(oc get ns -o jsonpath='{.items[*].metadata.name}' 2>/dev/null | tr ' ' '\n' | sort)
              if [[ -z "$namespace_list" ]]; then
                error_exit "No namespaces found"
              fi
              namespace_count=$(echo "$namespace_list" | wc -l)
              log INFO "Found $namespace_count namespaces in cluster"
              
              current_epoch=$(date +%s)
              min_age_seconds=$((MIN_AGE_DAYS * 86400))
              
              start_time=$(date +%s)
              
              while IFS= read -r ns; do
                [[ -z "$ns" ]] && continue
                
                excluded=false
                while IFS= read -r pattern; do
                  [[ -z "$pattern" ]] && continue
                  if echo "$ns" | grep -Eq "$pattern" 2>/dev/null; then
                    excluded=true
                    break
                  fi
                done <<< "$EXCLUDED_NS"
                
                if [[ "$excluded" == "true" ]]; then
                  ((namespaces_excluded++))
                  continue
                fi
                
                ((namespaces_processed++))
                
                if [[ $((namespaces_processed % 50)) -eq 0 ]]; then
                  elapsed=$(($(date +%s) - start_time))
                  log INFO "Progress: $namespaces_processed namespaces, ${elapsed}s elapsed"
                fi
                
                log INFO "-----------------------------------------------"
                log INFO "Processing Namespace [$namespaces_processed/$((namespace_count - namespaces_excluded))]: $ns"
                log INFO "-----------------------------------------------"
                
                secret_count=0
                if ! secret_count=$(oc get secret -n "$ns" --no-headers 2>/dev/null | wc -l); then
                  log ERROR "Unable to count Secrets in $ns"
                  error_list+=("Unable to count Secrets in namespace: $ns")
                  ((total_errors++))
                  sleep "$DELAY_NS"
                  continue
                fi
                
                if [[ "$secret_count" -eq 0 ]]; then
                  log INFO "No Secrets found"
                  sleep "$DELAY_NS"
                  continue
                fi
                
                log INFO "Found $secret_count Secret(s)"
                
                if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                  if [[ "$secret_count" -le "$HIGH_VOLUME_THRESHOLD" ]]; then
                    log INFO "SKIPPED: High-volume-only mode (threshold: $HIGH_VOLUME_THRESHOLD)"
                    ((namespaces_skipped_by_mode++))
                    sleep 0.01
                    continue
                  fi
                fi
                
                if [[ "$secret_count" -gt "$HIGH_VOLUME_THRESHOLD" ]]; then
                  if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                    process_high_volume_namespace "$ns" "$secret_count"
                  else
                    log WARN "HIGH-VOLUME: Skipped (threshold: $HIGH_VOLUME_THRESHOLD)"
                    log WARN "Set processHighVolumeNamespaces=true to enable"
                    high_volume_ns_list+=("$ns:$secret_count")
                    ((namespaces_high_volume++))
                  fi
                  sleep "$DELAY_NS"
                  continue
                fi
                
                ref_result=$(get_workload_references "$ns")
                referenced_secrets=$(echo "$ref_result" | sed -n '/---REFS-START---/,/---REFS-END---/p' | grep -v "^---REFS-" || echo "")
                workload_details=$(echo "$ref_result" | sed -n '/---REFS-END---/,$p' | tail -n +2 || echo "")
                
                secret_json=""
                if ! secret_json=$(oc get secret -n "$ns" -o json 2>/dev/null); then
                  log ERROR "Unable to list Secrets in $ns"
                  error_list+=("Unable to list Secrets in namespace: $ns")
                  ((total_errors++))
                  sleep "$DELAY_NS"
                  continue
                fi
                
                process_secrets "$ns" "$secret_json" "$referenced_secrets" "$workload_details"
                
                log INFO "Namespace summary: deleted=$ns_deleted skipped=$ns_skipped"
                ((total_skipped += ns_skipped))
                
                sleep "$DELAY_NS"
                
              done <<< "$namespace_list"
              
              end_time=$(date +%s)
              duration=$((end_time - start_time))
              duration_min=$((duration / 60))
              duration_sec=$((duration % 60))
              
              log INFO ""
              log INFO "================================================"
              log INFO "Job Completed Successfully"
              log INFO "================================================"
              log INFO "Duration: ${duration_min}m ${duration_sec}s"
              log INFO ""
              log INFO "Namespace Statistics:"
              log INFO "  Total in cluster: $namespace_count"
              log INFO "  Excluded by pattern: $namespaces_excluded"
              if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                log INFO "  Skipped (high-volume-only mode): $namespaces_skipped_by_mode"
              fi
              log INFO "  High-volume (paginated): $namespaces_paginated"
              log INFO "  High-volume (skipped): $namespaces_high_volume"
              log INFO "  Processed normally: $((namespaces_processed - namespaces_paginated - namespaces_skipped_by_mode))"
              log INFO ""
              
              if [[ "$namespaces_high_volume" -gt 0 ]]; then
                log WARN "High-Volume Namespaces Skipped:"
                for entry in "${high_volume_ns_list[@]}"; do
                  ns_name="${entry%:*}"
                  ns_count="${entry#*:}"
                  log WARN "  - $ns_name: $ns_count Secrets"
                done
                log WARN "Set processHighVolumeNamespaces=true to process these"
                log WARN ""
              fi
              
              log INFO "Secret Statistics:"
              log INFO "  Deleted (or would delete): $total_deleted"
              log INFO "  Skipped total: $total_skipped"
              log INFO "    - System (types): $total_system_secret"
              log INFO "    - Protected (labels): $total_protected"
              log INFO "    - Too new (< $MIN_AGE_DAYS days): $total_too_new"
              log INFO "    - No/invalid timestamp: $total_no_timestamp"
              log INFO "    - Referenced (workloads): $total_referenced"
              log INFO "    - Referenced (Route TLS): $total_route_tls"
              log INFO "    - Referenced (ServiceAccounts): $total_sa_refs"
              log INFO ""
              
              if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                log INFO "Pagination Statistics:"
                log INFO "  Continue token retries: $total_continue_token_retries"
                log INFO "  JSON validation failures: $total_json_validation_failures"
                log INFO ""
              fi
              
              if [[ "$DRY_RUN" == "false" && "$total_deleted" -gt 0 ]]; then
                log INFO "=== DELETED SECRETS ($total_deleted total) ==="
                for deleted_secret in "${deleted_secrets_list[@]}"; do
                  log INFO "  $deleted_secret"
                done
                log INFO ""
              fi
              
              if [[ "$total_errors" -gt 0 ]]; then
                log ERROR "Errors Encountered: $total_errors"
                for error_msg in "${error_list[@]}"; do
                  log ERROR "  $error_msg"
                done
                log ERROR ""
              else
                log INFO "Errors: 0"
              fi
              
              log INFO "Mode: $(if [[ "$DRY_RUN" == "true" ]]; then echo "DRY-RUN"; else echo "LIVE"; fi)"
              if [[ "$PROCESS_HIGH_VOLUME" == "true" ]]; then
                log INFO "Pagination: ENABLED (limit=$PAGINATION_LIMIT)"
              fi
              if [[ "$PROCESS_ONLY_HIGH_VOLUME" == "true" ]]; then
                log INFO "High-Volume Only: ENABLED"
              fi
              log INFO "================================================"
              
              exit 0




              apiVersion: v1
kind: ConfigMap
metadata:
  name: secret-pruner-config
  namespace: configmap-secret-pruner
data:
  # Basic settings
  dryRun: "true"
  minAgeDays: "30"
  pruneSecretsWithoutTimestamp: "false"  # Conservative default
  
  # High-volume namespace handling
  highVolumeThreshold: "1000"
  processHighVolumeNamespaces: "false"  # Start with skip mode
  paginationLimit: "500"
  processOnlyHighVolume: "false"
  
  # Rate limiting
  delayBetweenNamespaces: "0.5"
  delayBetweenDeletions: "0.1"
  batchSize: "10"
  batchPauseSeconds: "1"
  
  # Namespace exclusions (regex patterns)
  excludedNamespaces: |
    ^openshift-.*
    ^kube-.*
    ^default$
    ^openshift$
    ^configmap-secret-pruner$
  
  # Critical: Exclude system secret types
  excludedSecretTypes: |
    kubernetes.io/service-account-token
    kubernetes.io/dockercfg
    kubernetes.io/dockerconfigjson
    kubernetes.io/basic-auth
    kubernetes.io/ssh-auth
    bootstrap.kubernetes.io/token
    istio.io/ca-root
    kubernetes.io/tls
  
  # Protection labels (any match protects)
  protectedLabels: |
    app.kubernetes.io/managed-by=argocd
    app.kubernetes.io/managed-by=Helm
    meta.helm.sh/release-name
    prune.protected=true
    auth.openshift.io/managed=true
    operators.coreos.com/.*
    olm.operatorgroup.name

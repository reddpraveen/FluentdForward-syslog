import boto3
from botocore.config import Config
import warnings
import urllib3

# Suppress InsecureRequestWarning from urllib3
warnings.filterwarnings("ignore", category=urllib3.exceptions.InsecureRequestWarning)
# Configurations
SOURCE_BUCKET = 'source-bucket-name'
TARGET_BUCKET = 'target-bucket-name'
SOURCE_ENDPOINT = 'https://source-endpoint'
TARGET_ENDPOINT = 'https://target-endpoint'

# Source credentials
source_session = boto3.session.Session(
    aws_access_key_id='SOURCE_ACCESS_KEY',
    aws_secret_access_key='SOURCE_SECRET_KEY'
)
source_s3 = source_session.client('s3', endpoint_url=SOURCE_ENDPOINT, config=Config(signature_version='s3v4'))

# Target credentials
target_session = boto3.session.Session(
    aws_access_key_id='TARGET_ACCESS_KEY',
    aws_secret_access_key='TARGET_SECRET_KEY'
)
target_s3 = target_session.client('s3', endpoint_url=TARGET_ENDPOINT, config=Config(signature_version='s3v4'))

# Copy a single object (for test)
def copy_one_object():
    response = source_s3.list_objects_v2(Bucket=SOURCE_BUCKET, MaxKeys=1)
    objects = response.get('Contents', [])
    if not objects:
        print("No objects found in the source bucket.")
        return

    key = objects[0]['Key']
    print(f"Copying single object for test: {key}")

    try:
        source_obj = source_s3.get_object(Bucket=SOURCE_BUCKET, Key=key)
        target_s3.upload_fileobj(source_obj['Body'], TARGET_BUCKET, key)
        print(f"Successfully copied '{key}' to target bucket.")
    except Exception as e:
        print(f"Error copying '{key}': {str(e)}")

copy_one_object()


import boto3
from botocore.config import Config
import warnings
import urllib3

# Suppress InsecureRequestWarning from urllib3
warnings.filterwarnings("ignore", category=urllib3.exceptions.InsecureRequestWarning)
# Configuration
SOURCE_BUCKET = 'source-bucket-name'
TARGET_BUCKET = 'target-bucket-name'
SOURCE_ENDPOINT = 'https://source-endpoint'
TARGET_ENDPOINT = 'https://target-endpoint'

# Source credentials
source_session = boto3.session.Session(
    aws_access_key_id='SOURCE_ACCESS_KEY',
    aws_secret_access_key='SOURCE_SECRET_KEY'
)
source_s3 = source_session.client(
    's3',
    endpoint_url=SOURCE_ENDPOINT,
    config=Config(signature_version='s3v4'),
    verify=False  # Disable SSL validation
)

# Target credentials
target_session = boto3.session.Session(
    aws_access_key_id='TARGET_ACCESS_KEY',
    aws_secret_access_key='TARGET_SECRET_KEY'
)
target_s3 = target_session.client(
    's3',
    endpoint_url=TARGET_ENDPOINT,
    config=Config(signature_version='s3v4'),
    verify=False  # Disable SSL validation
)

# Copy all objects from source to target
def copy_all_objects():
    paginator = source_s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=SOURCE_BUCKET)

    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            print(f"Copying: {key}")
            try:
                source_obj = source_s3.get_object(Bucket=SOURCE_BUCKET, Key=key)
                target_s3.upload_fileobj(source_obj['Body'], TARGET_BUCKET, key)
                print(f"Copied: {key}")
            except Exception as e:
                print(f"Failed to copy {key}: {e}")

copy_all_objects()


#!/bin/bash

set -euo pipefail

# ==== CONFIGURE THESE VARIABLES ====
SOURCE_SECRET="logging-loki-odf"
TARGET_SECRET="logging-loki-rgw"
SOURCE_BUCKET="loki-noobaa-bucket"
TARGET_BUCKET="loki-rgw-bucket"
MC_BIN="/usr/local/bin/mc"  # Path to mc binary

# ==== Step 1: Extract credentials from OpenShift ====
echo "Extracting source credentials..."
SOURCE_ACCESS_KEY=$(oc get secret $SOURCE_SECRET -n openshift-logging -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
SOURCE_SECRET_KEY=$(oc get secret $SOURCE_SECRET -n openshift-logging -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

echo "Extracting target credentials..."
TARGET_ACCESS_KEY=$(oc get secret $TARGET_SECRET -n openshift-logging -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
TARGET_SECRET_KEY=$(oc get secret $TARGET_SECRET -n openshift-logging -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

# ==== Step 2: Get S3 routes ====
echo "Getting NooBaa and RGW routes..."
NOOBAA_HOST=$(oc get route s3 -n openshift-storage -o jsonpath='{.spec.host}')
RGW_HOST=$(oc get route rgw -n openshift-storage -o jsonpath='{.spec.host}')

NOOBAA_ENDPOINT="https://${NOOBAA_HOST}"
RGW_ENDPOINT="https://${RGW_HOST}"

# ==== Step 3: Install mc if not present ====
if ! command -v $MC_BIN &> /dev/null; then
  echo "Installing mc (MinIO Client)..."
  curl -s https://dl.min.io/client/mc/release/linux-amd64/mc -o mc
  chmod +x mc
  sudo mv mc $MC_BIN
fi

# ==== Step 4: Set aliases in mc ====
echo "Setting mc aliases..."
mc alias set noobaa "$NOOBAA_ENDPOINT" "$SOURCE_ACCESS_KEY" "$SOURCE_SECRET_KEY" --insecure
mc alias set rgw "$RGW_ENDPOINT" "$TARGET_ACCESS_KEY" "$TARGET_SECRET_KEY" --insecure

# ==== Step 5: (Optional) Clear target bucket ====
echo "WARNING: Clearing all contents from target bucket $TARGET_BUCKET"
mc rm --recursive --force rgw/$TARGET_BUCKET

# ==== Step 6: Mirror data ====
echo "Mirroring data from NooBaa to RGW..."
mc mirror --overwrite --preserve --insecure noobaa/$SOURCE_BUCKET rgw/$TARGET_BUCKET

echo "Mirror complete. Now update Loki secret and restart pods."

-------------

# ----------------------------
# 1. ConfigMap for mc mirror script
# ----------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: mc-mirror-script
  namespace: openshift-logging
data:
  mirror.sh: |
    #!/bin/sh
    set -e

    SOURCE_ACCESS_KEY=$(cat /secrets/source/AWS_ACCESS_KEY_ID)
    SOURCE_SECRET_KEY=$(cat /secrets/source/AWS_SECRET_ACCESS_KEY)
    TARGET_ACCESS_KEY=$(cat /secrets/target/AWS_ACCESS_KEY_ID)
    TARGET_SECRET_KEY=$(cat /secrets/target/AWS_SECRET_ACCESS_KEY)

    echo "Setting mc aliases..."
    mc alias set noobaa https://s3.openshift-storage.svc:443 "$SOURCE_ACCESS_KEY" "$SOURCE_SECRET_KEY" --insecure
    mc alias set rgw https://rgw.openshift-storage.svc:443 "$TARGET_ACCESS_KEY" "$TARGET_SECRET_KEY" --insecure

    echo "Starting mirror..."
    mc mirror --overwrite --preserve --insecure noobaa/loki-noobaa-bucket rgw/loki-rgw-bucket
    echo "Mirror complete."

---
# ----------------------------
# 2. Job using minio/mc
# ----------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: mc-mirror
  namespace: openshift-logging
spec:
  template:
    spec:
      containers:
      - name: mc
        image: minio/mc:RELEASE.2024-03-30T16-33-04Z
        command: ["/bin/sh", "/scripts/mirror.sh"]
        volumeMounts:
        - name: script-volume
          mountPath: /scripts
        - name: source-secret
          mountPath: /secrets/source
        - name: target-secret
          mountPath: /secrets/target
      restartPolicy: Never
      volumes:
      - name: script-volume
        configMap:
          name: mc-mirror-script
      - name: source-secret
        secret:
          secretName: logging-loki-odf
      - name: target-secret
        secret:
          secretName: logging-loki-rgw
  backoffLimit: 0

---
# ----------------------------
# 3. ConfigMap for rclone config
# ----------------------------
apiVersion: v1
kind: ConfigMap
metadata:
  name: rclone-config
  namespace: openshift-logging
data:
  rclone.conf: |
    [noobaa]
    type = s3
    provider = Minio
    access_key_id = ${NOOBAA_ACCESS_KEY}
    secret_access_key = ${NOOBAA_SECRET_KEY}
    endpoint = https://s3.openshift-storage.svc:443
    insecure_skip_verify = true

    [rgw]
    type = s3
    provider = Ceph
    access_key_id = ${RGW_ACCESS_KEY}
    secret_access_key = ${RGW_SECRET_KEY}
    endpoint = https://rgw.openshift-storage.svc:443
    insecure_skip_verify = true

---
# ----------------------------
# 4. Job using rclone
# ----------------------------
apiVersion: batch/v1
kind: Job
metadata:
  name: rclone-mirror
  namespace: openshift-logging
spec:
  template:
    spec:
      containers:
      - name: rclone
        image: rclone/rclone:latest
        command: ["/bin/sh", "-c"]
        args:
        - |
          export NOOBAA_ACCESS_KEY=$(cat /secrets/source/AWS_ACCESS_KEY_ID) && \
          export NOOBAA_SECRET_KEY=$(cat /secrets/source/AWS_SECRET_ACCESS_KEY) && \
          export RGW_ACCESS_KEY=$(cat /secrets/target/AWS_ACCESS_KEY_ID) && \
          export RGW_SECRET_KEY=$(cat /secrets/target/AWS_SECRET_ACCESS_KEY) && \
          rclone copy noobaa:loki-noobaa-bucket rgw:loki-rgw-bucket --config /etc/rclone/rclone.conf --progress --s3-no-check-bucket
        volumeMounts:
        - name: rclone-config
          mountPath: /etc/rclone
        - name: source-secret
          mountPath: /secrets/source
        - name: target-secret
          mountPath: /secrets/target
      restartPolicy: Never
      volumes:
      - name: rclone-config
        configMap:
          name: rclone-config
      - name: source-secret
        secret:
          secretName: logging-loki-odf
      - name: target-secret
        secret:
          secretName: logging-loki-rgw
  backoffLimit: 0

import boto3
from botocore.config import Config
import warnings
import urllib3

# Suppress InsecureRequestWarning from urllib3
warnings.filterwarnings("ignore", category=urllib3.exceptions.InsecureRequestWarning)
# Configurations
SOURCE_BUCKET = 'source-bucket-name'
TARGET_BUCKET = 'target-bucket-name'
SOURCE_ENDPOINT = 'https://source-endpoint'
TARGET_ENDPOINT = 'https://target-endpoint'

# Source credentials
source_session = boto3.session.Session(
    aws_access_key_id='SOURCE_ACCESS_KEY',
    aws_secret_access_key='SOURCE_SECRET_KEY'
)
source_s3 = source_session.client('s3', endpoint_url=SOURCE_ENDPOINT, config=Config(signature_version='s3v4'))

# Target credentials
target_session = boto3.session.Session(
    aws_access_key_id='TARGET_ACCESS_KEY',
    aws_secret_access_key='TARGET_SECRET_KEY'
)
target_s3 = target_session.client('s3', endpoint_url=TARGET_ENDPOINT, config=Config(signature_version='s3v4'))

# Copy a single object (for test)
def copy_one_object():
    response = source_s3.list_objects_v2(Bucket=SOURCE_BUCKET, MaxKeys=1)
    objects = response.get('Contents', [])
    if not objects:
        print("No objects found in the source bucket.")
        return

    key = objects[0]['Key']
    print(f"Copying single object for test: {key}")

    try:
        source_obj = source_s3.get_object(Bucket=SOURCE_BUCKET, Key=key)
        target_s3.upload_fileobj(source_obj['Body'], TARGET_BUCKET, key)
        print(f"Successfully copied '{key}' to target bucket.")
    except Exception as e:
        print(f"Error copying '{key}': {str(e)}")

copy_one_object()


import boto3
from botocore.config import Config
import warnings
import urllib3

# Suppress InsecureRequestWarning from urllib3
warnings.filterwarnings("ignore", category=urllib3.exceptions.InsecureRequestWarning)
# Configuration
SOURCE_BUCKET = 'source-bucket-name'
TARGET_BUCKET = 'target-bucket-name'
SOURCE_ENDPOINT = 'https://source-endpoint'
TARGET_ENDPOINT = 'https://target-endpoint'

# Source credentials
source_session = boto3.session.Session(
    aws_access_key_id='SOURCE_ACCESS_KEY',
    aws_secret_access_key='SOURCE_SECRET_KEY'
)
source_s3 = source_session.client(
    's3',
    endpoint_url=SOURCE_ENDPOINT,
    config=Config(signature_version='s3v4'),
    verify=False  # Disable SSL validation
)

# Target credentials
target_session = boto3.session.Session(
    aws_access_key_id='TARGET_ACCESS_KEY',
    aws_secret_access_key='TARGET_SECRET_KEY'
)
target_s3 = target_session.client(
    's3',
    endpoint_url=TARGET_ENDPOINT,
    config=Config(signature_version='s3v4'),
    verify=False  # Disable SSL validation
)

# Copy all objects from source to target
def copy_all_objects():
    paginator = source_s3.get_paginator('list_objects_v2')
    page_iterator = paginator.paginate(Bucket=SOURCE_BUCKET)

    for page in page_iterator:
        for obj in page.get('Contents', []):
            key = obj['Key']
            print(f"Copying: {key}")
            try:
                source_obj = source_s3.get_object(Bucket=SOURCE_BUCKET, Key=key)
                target_s3.upload_fileobj(source_obj['Body'], TARGET_BUCKET, key)
                print(f"Copied: {key}")
            except Exception as e:
                print(f"Failed to copy {key}: {e}")

copy_all_objects()


#!/bin/bash

set -euo pipefail

# ==== CONFIGURE THESE VARIABLES ====
SOURCE_SECRET="logging-loki-odf"
TARGET_SECRET="logging-loki-rgw"
SOURCE_BUCKET="loki-noobaa-bucket"
TARGET_BUCKET="loki-rgw-bucket"
MC_BIN="/usr/local/bin/mc"  # Path to mc binary

# ==== Step 1: Extract credentials from OpenShift ====
echo "Extracting source credentials..."
SOURCE_ACCESS_KEY=$(oc get secret $SOURCE_SECRET -n openshift-logging -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
SOURCE_SECRET_KEY=$(oc get secret $SOURCE_SECRET -n openshift-logging -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

echo "Extracting target credentials..."
TARGET_ACCESS_KEY=$(oc get secret $TARGET_SECRET -n openshift-logging -o jsonpath='{.data.AWS_ACCESS_KEY_ID}' | base64 -d)
TARGET_SECRET_KEY=$(oc get secret $TARGET_SECRET -n openshift-logging -o jsonpath='{.data.AWS_SECRET_ACCESS_KEY}' | base64 -d)

# ==== Step 2: Get S3 routes ====
echo "Getting NooBaa and RGW routes..."
NOOBAA_HOST=$(oc get route s3 -n openshift-storage -o jsonpath='{.spec.host}')
RGW_HOST=$(oc get route rgw -n openshift-storage -o jsonpath='{.spec.host}')

NOOBAA_ENDPOINT="https://${NOOBAA_HOST}"
RGW_ENDPOINT="https://${RGW_HOST}"

# ==== Step 3: Install mc if not present ====
if ! command -v $MC_BIN &> /dev/null; then
  echo "Installing mc (MinIO Client)..."
  curl -s https://dl.min.io/client/mc/release/linux-amd64/mc -o mc
  chmod +x mc
  sudo mv mc $MC_BIN
fi

# ==== Step 4: Set aliases in mc ====
echo "Setting mc aliases..."
mc alias set noobaa "$NOOBAA_ENDPOINT" "$SOURCE_ACCESS_KEY" "$SOURCE_SECRET_KEY" --insecure
mc alias set rgw "$RGW_ENDPOINT" "$TARGET_ACCESS_KEY" "$TARGET_SECRET_KEY" --insecure

# ==== Step 5: (Optional) Clear target bucket ====
echo "WARNING: Clearing all contents from target bucket $TARGET_BUCKET"
mc rm --recursive --force rgw/$TARGET_BUCKET

# ==== Step 6: Mirror data ====
echo "Mirroring data from NooBaa to RGW..."
mc mirror --overwrite --preserve --insecure noobaa/$SOURCE_BUCKET rgw/$TARGET_BUCKET

echo "Mirror complete. Now update Loki secret and restart pods."


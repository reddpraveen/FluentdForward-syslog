(echo "From: you@example.com"; echo "To: recipient@example.com"; echo "Subject: Your Excel Report"; echo "MIME-Version: 1.0"; echo "Content-Type: multipart/mixed; boundary=\"boundary\""; echo; echo "--boundary"; echo "Content-Type: text/plain"; echo; echo "Please find the attached Excel file."; echo "--boundary"; echo "Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet; name=\"yourfile.xlsx\""; echo "Content-Disposition: attachment; filename=\"yourfile.xlsx\""; echo "Content-Transfer-Encoding: base64"; echo; base64 /path/to/yourfile.xlsx; echo "--boundary--") | 

/usr/sbin/sendmail -t


kubernetes.namespace_name:"prj-cpt-tmx-dev" AND 
kubernetes.pod_name:/cpt-fusion-tmx-viewer-.*/ AND 
message:/([0-9]+\.[0-9]+s)|([5-9][0-9]{2}\.[0-9]+ms)/
kubernetes.namespace_name:"prj-cpt-tmx-dev" AND 
kubernetes.pod_name:/cpt-fusion-tmx-viewer-.*/ AND 
message:"GET"

Aggregation and Ratio in Grafana
	1.	Set Up Queries:
	•	Query A: Use the first query for long-running requests.
	•	Query B: Use the second query for GET requests.
	2.	Add Metrics:
	•	Use the Count metric to count logs for both queries.
	3.	Transformation:
	•	Apply the Math transformation in Grafana to calculate the ratio: A / B.
Additional Notes:
	•	Ensure your Elasticsearch indices are configured to include kubernetes.namespace_name, kubernetes.pod_name, and message fields.
	•	Use Grafana’s $__interval or $__auto_interval variables in panel settings to dynamically adjust the time range.

Let me know if you need assistance setting up the queries in Grafana!


To handle errors in the `token_command` and fall back to an alternative command if the first one fails, you can use a `try-except` block in Python. This allows you to attempt the first command and, if it raises an exception (indicating an error), execute the second command. Here's how you can modify the `prometheus_query` function to include this logic:

```python
import subprocess

def prometheus_query(query_tr, nodetoverify):
    # Define the token commands
    if cluster_name == "arockxc":
        token_command_1 = "oc create token prometheus-k8s -n openshift-monitoring"
        token_command_2 = "oc sa get-token prometheus-k8s -n openshift-monitoring"
    else:
        token_command_1 = "oc sa get-token prometheus-k8s -n openshift-monitoring"
        token_command_2 = "oc create token prometheus-k8s -n openshift-monitoring"

    # Try the first token command
    try:
        token = subprocess.check_output(token_command_1.split()).decode('utf-8').strip()
    except subprocess.CalledProcessError:
        # If the first command fails, try the second command
        try:
            token = subprocess.check_output(token_command_2.split()).decode('utf-8').strip()
        except subprocess.CalledProcessError as e:
            print(f"Failed to retrieve token: {e}")
            return None

    # Command to get the first pod name
    try:
        nod_command = "oc get pods -o name -n openshift-monitoring | head -1"
        pod_name = subprocess.check_output(nod_command, shell=True).decode('utf-8').strip()
    except subprocess.CalledProcessError as e:
        print(f"Failed to retrieve pod name: {e}")
        return None

    # Continue with the rest of the function
    # ...

    return token, pod_name
```

### Explanation:

1. **Token Commands**:
   - Two token commands are defined based on the `cluster_name`.
   - `token_command_1` is the primary command, and `token_command_2` is the fallback command.

2. **Try-Except Block**:
   - The `try` block attempts to execute `token_command_1`.
   - If `token_command_1` fails (raises a `subprocess.CalledProcessError`), the `except` block catches the exception and attempts to execute `token_command_2`.

3. **Error Handling**:
   - If both commands fail, an error message is printed, and the function returns `None`.

4. **Pod Name Retrieval**:
   - The `nod_command` retrieves the name of the first pod in the `openshift-monitoring` namespace.
   - This command is also wrapped in a `try-except` block to handle potential errors.

5. **Return Values**:
   - The function returns the token and pod name if successful.

### Notes:
- Ensure that the `cluster_name` variable is defined and correctly set before calling this function.
- Adjust the commands and error handling as needed based on your specific requirements and environment.
- This approach ensures that the script is robust and can handle failures gracefully by falling back to alternative commands.

#!/bin/bash

# Set variables
NAMESPACE="kube-system"
SECRET_NAME="azure-credentials"

# Fetch secret data from Kubernetes
CLIENT_ID=$(kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.clientId}' | base64 --decode)
CLIENT_SECRET=$(kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.clientSecret}' | base64 --decode)
TENANT_ID=$(kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.tenantId}' | base64 --decode)
SUBSCRIPTION_ID=$(kubectl get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.subscriptionId}' | base64 --decode)

# Check if all required values are retrieved
if [[ -z "$CLIENT_ID" || -z "$CLIENT_SECRET" || -z "$TENANT_ID" || -z "$SUBSCRIPTION_ID" ]]; then
  echo "Error: Failed to retrieve Azure credentials from the secret."
  exit 1
fi

# Login to Azure
echo "Logging in to Azure..."
az login --service-principal \
         --username "$CLIENT_ID" \
         --password "$CLIENT_SECRET" \
         --tenant "$TENANT_ID"

# Set the default subscription
echo "Setting the default subscription to $SUBSCRIPTION_ID..."
az account set --subscription "$SUBSCRIPTION_ID"

echo "Azure login successful."


#!/bin/bash

# Variables
NAMESPACE="kube-system"
SECRET_NAME="azure-credentials"
TARGET_POD="<target-pod-name>"
TARGET_NAMESPACE="<target-namespace>"
TARGET_CONTAINER="<target-container-name>"  # Optional, specify if needed

# Retrieve Azure credentials from Kubernetes secret
CLIENT_ID=$(oc get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.clientId}' | base64 --decode)
CLIENT_SECRET=$(oc get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.clientSecret}' | base64 --decode)
TENANT_ID=$(oc get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.tenantId}' | base64 --decode)
SUBSCRIPTION_ID=$(oc get secret "$SECRET_NAME" -n "$NAMESPACE" -o jsonpath='{.data.subscriptionId}' | base64 --decode)

# Check if all required values are retrieved
if [[ -z "$CLIENT_ID" || -z "$CLIENT_SECRET" || -z "$TENANT_ID" || -z "$SUBSCRIPTION_ID" ]]; then
  echo "Error: Failed to retrieve Azure credentials from the secret."
  exit 1
fi

# Generate the command to log in to Azure
LOGIN_COMMAND="az login --service-principal --username $CLIENT_ID --password $CLIENT_SECRET --tenant $TENANT_ID && az account set --subscription $SUBSCRIPTION_ID"

# Execute the command on the pod
echo "Executing Azure login command on the target pod..."
if [[ -z "$TARGET_CONTAINER" ]]; then
  oc exec "$TARGET_POD" -n "$TARGET_NAMESPACE" -- bash -c "$LOGIN_COMMAND"
else
  oc exec "$TARGET_POD" -n "$TARGET_NAMESPACE" -c "$TARGET_CONTAINER" -- bash -c "$LOGIN_COMMAND"
fi

echo "Azure login completed on the pod."

oc get pods --all-namespaces --field-selector=status.phase=Failed -o jsonpath='{range .items[?(@.status.reason=="Evicted")]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' | while read namespace pod; do
  oc delete pod "$pod" -n "$namespace"
done

Here's a script to generate a report and manage infrastructure pods in OpenShift. The script will check current resource allocation, verify node placement, and provide scaling recommendations:

```bash
#!/bin/bash

# OpenShift Infrastructure Resource Report Script
# Checks infra pod placement, resource utilization, and provides scaling recommendations

# Configuration
INFRA_NODE_LABEL="node-role.kubernetes.io/infra"
INFRA_PROJECTS=("openshift-ingress" "openshift-monitoring" "openshift-logging" "openshift-dns")
REPORT_FILE="infra_resource_report_$(date +%Y%m%d).txt"
WARNING_THRESHOLD=80  # Percentage of resource usage to trigger warning

# Initialize report
echo "OpenShift Infrastructure Resource Report - $(date)" > $REPORT_FILE
echo "======================================================" >> $REPORT_FILE

# Function to check infra nodes
check_infra_nodes() {
    echo -e "\n[Infra Nodes Status]" >> $REPORT_FILE
    oc get nodes -l $INFRA_NODE_LABEL -o json | jq -r '.items[] | .metadata.name + " " + .status.allocatable.cpu + " " + .status.allocatable.memory' >> $REPORT_FILE
}

# Function to check infra pods placement
check_pod_placement() {
    echo -e "\n[Infra Pods Status]" >> $REPORT_FILE
    for project in "${INFRA_PROJECTS[@]}"; do
        echo -e "\nProject: $project" >> $REPORT_FILE
        oc get pods -n $project -o json | jq -r '.items[] | .metadata.name + " " + .spec.nodeName + " " + .spec.containers[0].resources.requests.cpu + " " + .spec.containers[0].resources.requests.memory' >> $REPORT_FILE
    done
}

# Function to check resource utilization
check_resource_utilization() {
    echo -e "\n[Resource Utilization]" >> $REPORT_FILE
    oc adm top pods --all-namespaces | grep -E "$(IFS=\|; echo "${INFRA_PROJECTS[*]}")" >> $REPORT_FILE
}

# Function to analyze scaling needs
analyze_scaling() {
    echo -e "\n[Scaling Recommendations]" >> $REPORT_FILE
    nodes=($(oc get nodes -l $INFRA_NODE_LABEL -o jsonpath='{.items[*].metadata.name}'))
    
    for node in "${nodes[@]}"; do
        node_cpu=$(oc get node $node -o jsonpath='{.status.allocatable.cpu}')
        node_mem=$(oc get node $node -o jsonpath='{.status.allocatable.memory}')
        
        used_cpu=$(oc describe node $node | grep -A 5 "Allocated resources" | grep cpu | awk '{print $2}')
        used_mem=$(oc describe node $node | grep -A 5 "Allocated resources" | grep memory | awk '{print $2}')
        
        # Calculate percentage usage
        cpu_pct=$(( (${used_cpu//m/} * 100) / ${node_cpu//m/} ))
        mem_pct=$(( (${used_mem//Ki/} * 100) / ${node_mem//Ki/} ))

        echo "Node: $node" >> $REPORT_FILE
        echo "CPU Usage: $cpu_pct% | Memory Usage: $mem_pct%" >> $REPORT_FILE
        
        if [ $cpu_pct -ge $WARNING_THRESHOLD ] || [ $mem_pct -ge $WARNING_THRESHOLD ]; then
            echo "ACTION REQUIRED: Consider scaling up infra nodes" >> $REPORT_FILE
        fi
    done
}

# Function to check node selector configurations
check_configurations() {
    echo -e "\n[Configuration Checks]" >> $REPORT_FILE
    for project in "${INFRA_PROJECTS[@]}"; do
        echo -e "\nProject: $project" >> $REPORT_FILE
        oc get deployments -n $project -o jsonpath='{range .items[*]}{.metadata.name}{"\tNode Selector: "}{.spec.template.spec.nodeSelector}{"\n"}{end}' >> $REPORT_FILE
    done
}

# Main execution
check_infra_nodes
check_pod_placement
check_resource_utilization
analyze_scaling
check_configurations

# Final recommendations
echo -e "\n[Next Steps]" >> $REPORT_FILE
echo "1. Review pod placement to ensure all infrastructure components are on infra nodes" >> $REPORT_FILE
echo "2. Check resource requests/limits against actual utilization" >> $REPORT_FILE
echo "3. Scale up infra nodes if utilization exceeds $WARNING_THRESHOLD%" >> $REPORT_FILE
echo "4. Verify nodeSelectors and tolerations in deployments" >> $REPORT_FILE

echo "Report generated: $REPORT_FILE"
```

### Key Features:
1. **Node Status Check**: Lists all infra nodes and their allocatable resources
2. **Pod Placement Audit**: Verifies infrastructure pods are running on designated infra nodes
3. **Resource Utilization**: Shows current CPU/memory usage of infrastructure pods
4. **Auto-Scaling Analysis**: Identifies nodes approaching capacity limits
5. **Configuration Validation**: Checks nodeSelector configurations in deployments

### Usage:
1. Save as `infra-resource-report.sh`
2. Make executable: `chmod +x infra-resource-report.sh`
3. Run with OpenShift CLI logged in: `./infra-resource-report.sh`

### Sample Output Summary:
```
Node: infra-node-1
CPU Usage: 85% | Memory Usage: 72%
ACTION REQUIRED: Consider scaling up infra nodes

Node: infra-node-2
CPU Usage: 65% | Memory Usage: 58%

[Next Steps]
1. Review pod placement to ensure all infrastructure components are on infra nodes
2. Check resource requests/limits against actual utilization
3. Scale up infra nodes if utilization exceeds 80%
4. Verify nodeSelectors and tolerations in deployments
```

### Scaling Recommendations:
1. **Horizontal Scaling**:
```bash
# Add new infra node
oc scale machineset/<infra-machine-set> --replicas=3 -n openshift-machine-api
```

2. **Vertical Scaling**:
```bash
# Edit machine set for larger instances
oc edit machineset/<infra-machine-set> -n openshift-machine-api
```

3. **Pod Resource Adjustments**:
```bash
# Update resource requests/limits
oc set resources deployment/<deployment-name> -n <project> --requests=cpu=500m,memory=1Gi
```

This script provides actionable insights while maintaining the isolation of infrastructure components. Adjust the `WARNING_THRESHOLD` and `INFRA_PROJECTS` array based on your cluster configuration.

apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  name: ansible-playbook-template
  title: Ansible Playbook Template (PR into existing repo)
  description: Generate/update an Ansible playbook into an existing repository via PR
  tags: [ansible, playbook, rhdh]
spec:
  type: playbook
  owner: user:Maskeen-Singh1
  lifecycle: experimental

  parameters:
    - title: Target Repository
      required: [owner, repo, projectName, subPath]
      properties:
        owner:
          type: string
          title: GitHub org or user that owns the repo
          description: e.g. Maskeen-singh1 or your-org
        repo:
          type: string
          title: Existing repository name
          description: The repo you want to update (must already exist)
        projectName:
          type: string
          title: Project name (folder that will be created/updated)
          description: Becomes the leaf folder name in subPath
        subPath:
          type: string
          title: Path inside repo where the project folder should live
          description: e.g. playbooks (resulting path = subPath/projectName)

  steps:
    - id: fetch
      name: Fetch template
      action: fetch:template
      input:
        url: ./templates
        # stage the generated files into a local folder named after the project
        targetPath: ./${{ parameters.projectName }}

    - id: pr
      name: Open Pull Request to GitHub
      action: publish:github:pull-request
      input:
        allowedHosts: ['github.devops.com'] # <--- change if your host differs
        repoUrl: github.devops.com?owner=${{ parameters.owner }}&repo=${{ parameters.repo }}
        branchName: scaffold/${{ parameters.projectName }}
        title: "Add/Update ${{ parameters.projectName }} (scaffold)"
        description: |
          This PR was created by the RHDH scaffolder to add/update:
          `${{ parameters.subPath }}/${{ parameters.projectName }}`.
        commitMessage: "scaffold: add/update ${{ parameters.subPath }}/${{ parameters.projectName }}"
        # Put all generated content under subPath/projectName in the repo
        targetPath: /${{ parameters.subPath }}/${{ parameters.projectName }}
        # Optional: if your skeleton already has catalog-info.yaml at its root
        # then registering will work using the new path below.

    - id: register
      name: Register in Catalog (optional)
      if: ${{ steps.pr.output.repoContentsUrl }}
      action: catalog:register
      input:
        # points at the new branch’s contents; Backstage will follow default branch after merge
        repoContentsUrl: ${{ steps.pr.output.repoContentsUrl }}
        # adjust if your catalog file is not at the generated folder root
        catalogInfoPath: /${{ parameters.subPath }}/${{ parameters.projectName }}/catalog-info.yaml

  output:
    links:
      - title: Open Pull Request
        url: ${{ steps.pr.output.pullRequestUrl }}
      - title: Repo (browse)
        url: ${{ steps.pr.output.remoteUrl }}



apiVersion: scaffolder.backstage.io/v1beta3
kind: Template
metadata:
  name: ansible-playbook-template
  title: Ansible Playbook Template
  description: Template to spin a new Ansible playbook
  tags: [ansible-template, automation, playbook, ansible, rhdh]
spec:
  type: playbook
  owner: user:Maskeen-Singh1
  lifecycle: experimental

  parameters:
    - title: Project Info
      required: [projectName, owner]
      properties:
        projectName:
          type: string
          title: Project Name
        owner:
          type: string
          title: GitHub org or user (destination owner)

  steps:
    - id: fetch
      name: Fetch Template
      action: fetch:template
      input:
        url: ./templates
        targetPath: ./${{ parameters.projectName }}

    - id: publish
      name: Publish to GitHub
      action: publish:github
      input:
        # MUST be your actual GitHub Enterprise host
        allowedHosts: ['github.devops.com']    # <-- change to your host
        repoUrl: github.devops.com?owner=${{ parameters.owner }}&repo=${{ parameters.projectName }}
        repoVisibility: private                 # or public/internal as your org allows
        defaultBranch: main

    - id: register
      name: Register in Catalog
      action: catalog:register
      input:
        # Use the OUTPUT from publish, not the input
        repoContentsUrl: ${{ steps.publish.output.repoContentsUrl }}
        catalogInfoPath: /catalog-info.yaml

  output:
    links:
      - title: Repository
        url: ${{ steps.publish.output.remoteUrl }}
      - title: Open in Catalog
        icon: catalog
        entityRef: ${{ steps.register.output.entityRef }}






# Exec into Alertmanager and create a multi-year silence
POD=$(oc -n openshift-monitoring get pod -l app.kubernetes.io/name=alertmanager,alertmanager=main -o jsonpath='{.items[0].metadata.name}')

oc -n openshift-monitoring exec "$POD" -c alertmanager -- \
  amtool silence add alertname=CollectorHighErrorRate \
  --author="$(whoami)" \
  --comment="Keep receiving, always silenced (OpenShift)" \
  --duration=43800h    # ~5 years


Verify / Audit
List silences:
POD=$(oc -n openshift-monitoring get pod -l app.kubernetes.io/name=alertmanager,alertmanager=main -o jsonpath='{.items[0].metadata.name}')
oc -n openshift-monitoring exec "$POD" -c alertmanager -- amtool silence query alertname=CollectorHighErrorRate
Show active alerts (they’ll appear silenced, not firing):
oc -n openshift-monitoring exec "$POD" -c alertmanager -- amtool alert
Remove the “always-silence” if you ever want it to fire again:
# find ID
oc -n openshift-monitoring exec "$POD" -c alertmanager -- amtool silence query alertname=CollectorHighErrorRate -o json | jq -r '.[0].id'
# expire it
oc -n openshift-monitoring exec "$POD" -c alertmanager -- amtool silence expire <ID>



# Show the redirect chain and the final effective URL
curl -ILs https://<AAP_HOST>/api/v2/me/ | sed -n '/^HTTP\|^Location/p'

# Or just follow to the end and print the final URL + status
curl -Ls -o /dev/null -w 'FINAL_URL:%{url_effective}\nCODE:%{http_code}\n' \
  -H "Authorization: Bearer <AAP_TOKEN>" \
  https://<AAP_HOST>/api/v2/me/


# OpenShift Build Ephemeral Storage Issue - Complete Fix Guide

## Problem Summary

OpenShift builds fail with “no space left on device” error during Buildah/Skopeo commit phase, even with increased ephemeral-storage limits. The issue occurs because Buildah writes to `/var/tmp` tmpfs instead of disk-backed storage.

**Error**: `error: storing blob to image destination for cache ... no space left on device`

## Root Cause

- Buildah/Skopeo hardcoded to use `/var/tmp` during image commit phase
- `/var/tmp` in build pods is a small tmpfs filesystem
- Environment variables (TMPDIR, BUILDAH_TMPDIR) don’t override this behavior
- Increasing ephemeral-storage limits doesn’t help because the issue is tmpfs, not disk storage

## Solutions (Recommended Order)

### Solution 1: Custom Builder Image with Symlinked /var/tmp (RECOMMENDED)

Create a custom S2I builder image that pre-configures the `/var/tmp` symlink:

**File: `Dockerfile.custom-httpd-builder`**

```dockerfile
FROM registry.redhat.io/ubi8/httpd-24:latest

USER root

# Create disk-backed tmp directory
RUN mkdir -p /var/lib/containers/storage/tmp && \
    chmod 1777 /var/lib/containers/storage/tmp

# Remove existing /var/tmp and create symlink to disk-backed storage
RUN rm -rf /var/tmp && \
    ln -sf /var/lib/containers/storage/tmp /var/tmp

# Set environment variables for all build tools
ENV TMPDIR=/var/lib/containers/storage/tmp \
    BUILDAH_TMPDIR=/var/lib/containers/storage/tmp \
    S2I_TMP_DIR=/var/lib/containers/storage/tmp \
    TMP=/var/lib/containers/storage/tmp \
    TEMP=/var/lib/containers/storage/tmp

USER 1001
```

**Build and Push Commands:**

```bash
# Build the custom image
podman build -f Dockerfile.custom-httpd-builder -t your-registry/custom-httpd-s2i:latest .

# Push to your registry
podman push your-registry/custom-httpd-s2i:latest

# Update your BuildConfig to use this image
oc patch bc/your-buildconfig -p '{"spec":{"strategy":{"sourceStrategy":{"from":{"name":"your-registry/custom-httpd-s2i:latest"}}}}}'
```

### Solution 2: Enhanced S2I Assemble Script

If you cannot modify the base image, create a comprehensive assemble script:

**File: `assemble`**

```bash
#!/bin/bash
set -e

echo "=== Starting enhanced S2I assemble ==="

# Create disk-backed tmp directory
mkdir -p /var/lib/containers/storage/tmp
chmod 1777 /var/lib/containers/storage/tmp

# Backup existing /var/tmp if it exists and isn't already a symlink
if [ -d /var/tmp ] && [ ! -L /var/tmp ]; then
    echo "Backing up existing /var/tmp contents..."
    cp -r /var/tmp/* /var/lib/containers/storage/tmp/ 2>/dev/null || true
    rm -rf /var/tmp
fi

# Create symlink
echo "Creating symlink: /var/tmp -> /var/lib/containers/storage/tmp"
ln -sfn /var/lib/containers/storage/tmp /var/tmp

# Export all relevant environment variables
export TMPDIR=/var/lib/containers/storage/tmp
export BUILDAH_TMPDIR=/var/lib/containers/storage/tmp
export S2I_TMP_DIR=/var/lib/containers/storage/tmp
export TMP=/var/lib/containers/storage/tmp
export TEMP=/var/lib/containers/storage/tmp

# Verify the symlink
echo "Verifying /var/tmp symlink:"
ls -la /var/tmp

# Call the original assemble script
echo "Calling original assemble script..."
if [ -f /usr/libexec/s2i/assemble ]; then
    exec /usr/libexec/s2i/assemble
elif [ -f /opt/app-root/s2i/bin/assemble ]; then
    exec /opt/app-root/s2i/bin/assemble
else
    echo "Original assemble script not found, proceeding with default behavior..."
fi
```

**Implementation Steps:**

```bash
# Create ConfigMap with the assemble script
oc create configmap custom-s2i-scripts --from-file=assemble=./assemble

# Make the script executable
chmod +x assemble

# Update BuildConfig to use custom script
oc patch bc/your-buildconfig -p '{
  "spec": {
    "strategy": {
      "sourceStrategy": {
        "scripts": "file:///opt/s2i-scripts"
      }
    },
    "spec": {
      "volumes": [{
        "name": "custom-scripts",
        "configMap": {
          "name": "custom-s2i-scripts",
          "defaultMode": 493
        }
      }],
      "volumeMounts": [{
        "name": "custom-scripts",
        "mountPath": "/opt/s2i-scripts"
      }]
    }
  }
}'
```

### Solution 3: Docker Build Strategy (Alternative)

Switch from S2I to Docker build strategy if the above solutions don’t work:

**File: `BuildConfig-docker.yaml`**

```yaml
apiVersion: build.openshift.io/v1
kind: BuildConfig
metadata:
  name: my-app-docker
  namespace: my-namespace
spec:
  source:
    type: Git
    git:
      uri: "https://github.com/your-repo/your-app.git"
      ref: "main"
  strategy:
    type: Docker
    dockerStrategy:
      dockerfilePath: Dockerfile
      env:
        - name: TMPDIR
          value: "/var/lib/containers/storage/tmp"
        - name: BUILDAH_TMPDIR
          value: "/var/lib/containers/storage/tmp"
  output:
    to:
      kind: ImageStreamTag
      name: "my-app:latest"
  resources:
    requests:
      ephemeral-storage: "50Gi"
      memory: "2Gi"
      cpu: "1"
    limits:
      ephemeral-storage: "50Gi"
      memory: "4Gi"
      cpu: "2"
  runPolicy: Serial
```

**File: `Dockerfile`**

```dockerfile
FROM registry.redhat.io/ubi8/httpd-24:latest

USER root

# Set up disk-backed tmp directory
RUN mkdir -p /var/lib/containers/storage/tmp && \
    chmod 1777 /var/lib/containers/storage/tmp && \
    rm -rf /var/tmp && \
    ln -sf /var/lib/containers/storage/tmp /var/tmp

# Set environment variables
ENV TMPDIR=/var/lib/containers/storage/tmp \
    BUILDAH_TMPDIR=/var/lib/containers/storage/tmp \
    TMP=/var/lib/containers/storage/tmp \
    TEMP=/var/lib/containers/storage/tmp

# Copy your application files
COPY . /var/www/html/

# Set proper permissions
RUN chown -R 1001:0 /var/www/html && \
    chmod -R g+rw /var/www/html

USER 1001

EXPOSE 8080
```

**Apply Docker BuildConfig:**

```bash
oc apply -f BuildConfig-docker.yaml
```

## Debugging Tools

### Build Environment Debug Script

**File: `debug-build-env.sh`**

```bash
#!/bin/bash
echo "=== Build Environment Debug Information ==="

echo "Current user: $(whoami)"
echo "Current UID/GID: $(id)"

echo -e "\n=== Environment Variables ==="
env | grep -E "(TMP|TEMP|BUILDAH)" | sort

echo -e "\n=== Filesystem Information ==="
echo "/var/tmp details:"
ls -la /var/tmp/ || echo "/var/tmp not accessible"

echo -e "\n/var/lib/containers/storage/tmp details:"
ls -la /var/lib/containers/storage/tmp/ || echo "Directory doesn't exist"

echo -e "\n=== Mount Points ==="
mount | grep -E "(tmp|tmpfs)"

echo -e "\n=== Disk Usage ==="
df -h | grep -E "(tmp|var)"

echo -e "\n=== Available Space in Key Directories ==="
for dir in /var/tmp /var/lib/containers/storage /tmp; do
    if [ -d "$dir" ]; then
        echo "$dir: $(du -sh $dir 2>/dev/null || echo 'Unable to determine')"
    fi
done

echo -e "\n=== Process List (build related) ==="
ps aux | grep -E "(buildah|skopeo|podman)" || echo "No build processes found"
```

**Usage:**

```bash
# Add to your assemble script or run during build
chmod +x debug-build-env.sh
./debug-build-env.sh
```

## Implementation Checklist

### Pre-Implementation

- [ ] Verify current BuildConfig configuration
- [ ] Check namespace LimitRange settings
- [ ] Confirm node disk space availability
- [ ] Backup existing BuildConfig

### Solution 1 (Custom Builder Image)

- [ ] Create Dockerfile for custom builder
- [ ] Build and test custom image locally
- [ ] Push image to accessible registry
- [ ] Update BuildConfig to use custom image
- [ ] Test build with new image

### Solution 2 (Custom Assemble Script)

- [ ] Create enhanced assemble script
- [ ] Test script permissions and execution
- [ ] Create ConfigMap with script
- [ ] Update BuildConfig with volume mount
- [ ] Test build with custom script

### Solution 3 (Docker Strategy)

- [ ] Create Dockerfile for application
- [ ] Create new BuildConfig with Docker strategy
- [ ] Test Docker build locally if possible
- [ ] Deploy and test in OpenShift

### Verification Steps

- [ ] Monitor build logs for symlink creation
- [ ] Verify /var/tmp points to disk-backed storage
- [ ] Check environment variables are set correctly
- [ ] Confirm successful image push
- [ ] Test application deployment

## Troubleshooting

### Common Issues and Solutions

**Issue**: Permission denied when creating symlink
**Solution**: Ensure the assemble script runs as root or has appropriate permissions

**Issue**: Original assemble script not found
**Solution**: Check the base image documentation for correct script path

**Issue**: Build still fails with tmpfs error
**Solution**: Verify symlink creation with debug script and ensure it’s created before Buildah operations

**Issue**: Custom image not accessible
**Solution**: Check image registry permissions and authentication

### Verification Commands

```bash
# Check current BuildConfig
oc describe bc/your-buildconfig

# Monitor build logs
oc logs -f bc/your-buildconfig

# Debug build pod (if still running)
oc rsh <build-pod-name>
ls -la /var/tmp
mount | grep tmp

# Check build resources
oc describe build <build-name>
```

## Expected Results

After implementing any of these solutions:

- `/var/tmp` should be a symlink to `/var/lib/containers/storage/tmp`
- Build logs should show successful image commit and push
- No “no space left on device” errors during build
- Build pods should use ephemeral-storage from disk, not tmpfs

## Additional Notes

- **Solution 1** (Custom Builder Image) is the most reliable long-term solution
- **Solution 2** (Custom Assemble Script) is good for environments where custom images aren’t allowed
- **Solution 3** (Docker Strategy) provides the most control but requires Dockerfile maintenance
- Always test in a development environment first
- Monitor build performance as disk I/O may be different from tmpfs
- Consider automating the custom image build and push process in your CI/CD pipeline

## Support

If issues persist after implementing these solutions:

1. Check OpenShift and Buildah versions for known issues
1. Review Red Hat documentation for your specific OpenShift version
1. Consider opening a support case with detailed build logs and configuration

          log_section "Applying MachineConfig"
          
          MACHINECONFIG_YAML='YXBpVmVyc2lvbjogbWFjaGluZWNvbmZpZ3VyYXRpb24ub3BlbnNoaWZ0LmlvL3YxCmtpbmQ6IE1hY2hpbmVDb25maWcKbWV0YWRhdGE6CiAgbGFiZWxzOgogICAgbWFjaGluZWNvbmZpZ3VyYXRpb24ub3BlbnNoaWZ0LmlvL3JvbGU6IG1hc3RlcgogIG5hbWU6IDk4LXZhci1saWItZXRjZApzcGVjOgogIGNvbmZpZzoKICAgIGlnbml0aW9uOgogICAgICB2ZXJzaW9uOiAzLjIuMAogICAgc3lzdGVtZDoKICAgICAgdW5pdHM6CiAgICAgIC0gY29udGVudHM6IHwKICAgICAgICAgIFtVbml0XQogICAgICAgICAgRGVzY3JpcHRpb249TWFrZSBldGNkIGZpbGVzeXN0ZW0gb24gc2Vjb25kYXJ5IGRpc2sKICAgICAgICAgIERlZmF1bHREZXBlbmRlbmNpZXM9bm8KICAgICAgICAgIEFmdGVyPWxvY2FsLWZzLXByZS50YXJnZXQKICAgICAgICAgIEJlZm9yZT1sb2NhbC1mcy50YXJnZXQKICAgICAgICAgIENvbmRpdGlvblBhdGhFeGlzdHM9IS92YXIvbGliL2V0Y2RfbWlncmF0ZWQKICAgICAgICAgIAogICAgICAgICAgW1NlcnZpY2VdCiAgICAgICAgICBUeXBlPW9uZXNob3QKICAgICAgICAgIFJlbWFpbkFmdGVyRXhpdD15ZXMKICAgICAgICAgIEV4ZWNTdGFydD0vdXNyL2xvY2FsL2Jpbi9ldGNkLWZpbmQtc2Vjb25kYXJ5LWRldmljZS5zaAogICAgICAgICAgRXhlY1N0YXJ0PS9iaW4vYmFzaCAtYyAnREVWSUNFPSQoL3Vzci9sb2NhbC9iaW4vZXRjZC1maW5kLXNlY29uZGFyeS1kZXZpY2Uuc2gpICYmIC91c3Ivc2Jpbi9ta2ZzLnhmcyAtTCBldGNkLXNlY29uZGFyeSAtZiAkREVWSUNFJwogICAgICAgICAgRXhlY1N0YXJ0PS9iaW4vdG91Y2ggL3Zhci9saWIvZXRjZF9taWdyYXRlZAogICAgICAgICAgCiAgICAgICAgICBbSW5zdGFsbF0KICAgICAgICAgIFJlcXVpcmVkQnk9bG9jYWwtZnMudGFyZ2V0CiAgICAgICAgZW5hYmxlZDogdHJ1ZQogICAgICAgIG5hbWU6IGV0Y2QtZm9ybWF0LnNlcnZpY2UKICAgICAgLSBjb250ZW50czogfAogICAgICAgICAgW1VuaXRdCiAgICAgICAgICBEZXNjcmlwdGlvbj1Nb3VudCBldGNkIGZpbGVzeXN0ZW0KICAgICAgICAgIEJlZm9yZT1sb2NhbC1mcy50YXJnZXQKICAgICAgICAgIFJlcXVpcmVzPWV0Y2QtZm9ybWF0LnNlcnZpY2UKICAgICAgICAgIEFmdGVyPWV0Y2QtZm9ybWF0LnNlcnZpY2UKICAgICAgICAgIAogICAgICAgICAgW01vdW50XQogICAgICAgICAgV2hhdD0vZGV2L2Rpc2svYnktbGFiZWwvZXRjZC1zZWNvbmRhcnkKICAgICAgICAgIFdoZXJlPS92YXIvbGliL2V0Y2QKICAgICAgICAgIFR5cGU9eGZzCiAgICAgICAgICBPcHRpb25zPWRlZmF1bHRzLHByanF1b3RhCiAgICAgICAgICAKICAgICAgICAgIFtJbnN0YWxsXQogICAgICAgICAgUmVxdWlyZWRCeT1sb2NhbC1mcy50YXJnZXQKICAgICAgICBlbmFibGVkOiB0cnVlCiAgICAgICAgbmFtZTogdmFyLWxpYi1ldGNkLm1vdW50CiAgICBzdG9yYWdlOgogICAgICBmaWxlczoKICAgICAgLSBjb250ZW50czoKICAgICAgICAgIHNvdXJjZTogZGF0YTp0ZXh0L3BsYWluO2NoYXJzZXQ9dXRmLTg7YmFzZTY0LEl5RXZZbWx1TDJKaGMyZ0tDbU1nUm1sdVpDQjBhR1VnWkdWMmFXTmxJSGRwZEdnZ2RHaGxJSE53WldOcFptbGpJSE5wZW1VZ0tESXdNRWRDSUdsdUlIUm9hWE1nWTJGelpTa0tSVmhRUlVOVVJVUmZVMGxhUlY5Q1dWUkZVejBrS0NneU1EQWdLaUF4TURJMElDb2dNVEF5TkNBcUlERXdNalFwS1FwVVQweEZVa0ZPUTBVOUpDZ29NVEFnS2lBeE1ESTBJQ29nTVRBeU5DQXFJREV3TWpRcEtRb0tabTl5SUdSbGRtbGpaU0JwYmlBdlpHVjJMM05rS2pzZ1pHOEtJQ0JwWmlCYldpQWhJQ0lrWkdWMmFXTmxJaUI5ZmlCYk1DMDVYU0FkSUNZbUlGdGJJQzFpSUNJa1pHVjJhV05sSWlCZFhUc2dkR2hsYmdvZ0lDQWdjMmw2WlQwa0tHSnNiMk5yWkdWMklDMHRaMlYwYzJsNlpUWTBJQ0lrWkdWMmFXTmxJaUF5UGk5a1pYWXZiblZzYkNCOGZDQmxZMmh2SURBcENpQWdJQ0JrYVdabVBTUW9LSE5wZW1VZ1BpQkZXRkJGUTFSRlJGOVRTVnBGWDBKWlZFVlRJRDhnYzJsNlpTQXRJRVZZVUVWRFZFVkVYMU5KV2tWZlFsbFVSVk1nT2lCRldGQkZRMVJGUkY5VFNWcEZYMEpaVkVWVElDMGdjMmw2WlNrcENpQWdJQ0FLSUNBZ0lHbG1JRnRiSUNSa2FXWm1JQzBzZENBa1ZFOU1SVkpCVGtORklGMWRPeUIwYUdWdUNpQWdJQ0FnSUdsbUlDRWdiVzkxYm5RZ2ZDQm5jbVZ3SUMxeElDSWtaR1YyYVdObElpQW1KaUFoSUhCMlpHbHpjR3hoZVNBaUpHUmxkbWxqWlNJZ0ppWStMMlJsZGk5dWRXeHNPeUIwYUdWdUNpQWdJQ0FnSUNBZ1pXTm9ieUFpSkdSbGRtbGpaU0lLSUNBZ0lDQWdJQ0JsZUdsMElEQUtJQ0FnSUNBZ1pta0tJQ0FnSUNCbWFRb2dJR1J2Ym1VS0NtVjRhWFFnTVFvPQogICAgICAgIG1vZGU6IDQyMAogICAgICAgIG92ZXJ3cml0ZTogdHJ1ZQogICAgICAgIHBhdGg6IC91c3IvbG9jYWwvYmluL2V0Y2QtZmluZC1zZWNvbmRhcnktZGV2aWNlLnNoCg=='
          
          echo "$MACHINECONFIG_YAML" | base64 -d | oc apply -f - || { log_error "Failed to apply MachineConfig"; exit 1; }
          log_info "PASS - MachineConfig applied"

###job

apiVersion: batch/v1
kind: Job
metadata:
  name: etcd-disk-migration
  namespace: openshift-etcd
  labels:
    app.kubernetes.io/name: etcd-disk-migration
    app.kubernetes.io/component: etcd
spec:
  template:
    metadata:
      labels:
        app: etcd-disk-migration
    spec:
      imagePullSecrets:
      - name: artifactory
      containers:
      - name: etcd-disk-migration
        image: artifactory.devops.com/local-docker-ocp4aas-certified-releases/ocp4-argocd/azure-oc-cli:latest
        command:
        - /bin/bash
        - -c
        args:
        - |
          #!/usr/bin/env bash
          set -euo pipefail
          
          # Color codes for output
          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          BLUE='\033[0;34m'
          NC='\033[0m'
          
          # Logging functions
          log_info() {
            echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
          }
          
          log_error() {
            echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
          }
          
          log_warn() {
            echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
          }
          
          log_section() {
            echo ""
            echo -e "${BLUE}====== $* ======${NC}"
          }
          
          # Error handling
          trap 'handle_error $? $LINENO' ERR
          
          handle_error() {
            local exit_code=$1
            local line_number=$2
            log_error "Script failed with exit code $exit_code at line $line_number"
            log_error "Check logs and consider rollback if necessary"
            exit $exit_code
          }
          
          # Configuration
          DISK_SIZE="200G"
          ETCD_DEVICE="/dev/sdc"
          MOUNT_PATH="/var/lib/etcd"
          
          # Export environment variables for govc
          export GOVC_URL="https://${vcnServer}"
          export GOVC_USERNAME="${vcnUser}"
          export GOVC_PASSWORD="${vcnPwd}"
          export GOVC_INSECURE=true
          
          log_section "Starting etcd Disk Migration Process"
          log_info "Disk size: $DISK_SIZE"
          log_info "Mount path: $MOUNT_PATH"
          
          # Get master nodes
          log_info "Retrieving master node information..."
          masterVM1=$(oc get nodes -l node-role.kubernetes.io/master= -o jsonpath='{.items[0].metadata.name}')
          masterVM2=$(oc get nodes -l node-role.kubernetes.io/master= -o jsonpath='{.items[1].metadata.name}')
          masterVM3=$(oc get nodes -l node-role.kubernetes.io/master= -o jsonpath='{.items[2].metadata.name}')
          
          if [[ -z "$masterVM1" ]] || [[ -z "$masterVM2" ]] || [[ -z "$masterVM3" ]]; then
            log_error "Failed to retrieve all three master nodes"
            exit 1
          fi
          
          log_info "Master nodes identified:"
          log_info "  Master 1: $masterVM1"
          log_info "  Master 2: $masterVM2"
          log_info "  Master 3: $masterVM3"
          
          # Verify initial etcd health
          log_section "Verifying Initial etcd Cluster Health"
          etcd_available=$(oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}')
          etcd_degraded=$(oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Degraded")].status}')
          
          if [[ "$etcd_available" != "True" ]] || [[ "$etcd_degraded" != "False" ]]; then
            log_error "etcd cluster is not healthy before migration. Aborting."
            log_error "Available: $etcd_available, Degraded: $etcd_degraded"
            exit 1
          fi
          log_info "PASS - etcd cluster is healthy"
          
          # Get datastore information
          log_info "Retrieving datastore information..."
          DATASTORE=$(govc vm.info -json "$masterVM1" | jq -r '.VirtualMachines[0].Config.DatastoreUrl[0].Name')
          
          if [[ -z "$DATASTORE" ]]; then
            log_error "Failed to retrieve datastore information"
            exit 1
          fi
          
          log_info "Using datastore: $DATASTORE"
          
          # Function to create and attach disk
          create_and_attach_disk() {
            local node=$1
            local disk_name="${node}_dedicated_etcd_disk.vmdk"
            
            log_info "Processing node: $node"
            
            # Check if disk already exists
            if govc datastore.ls -ds "$DATASTORE" "${node}/" 2>/dev/null | grep -q "dedicated_etcd_disk.vmdk"; then
              log_warn "Dedicated etcd disk already exists for $node, skipping creation"
              return 0
            fi
            
            # Create disk
            log_info "Creating ${DISK_SIZE} disk for $node..."
            if ! govc vm.disk.create -vm "$node" -name "${node}/${disk_name}" -size "$DISK_SIZE" -ds "$DATASTORE"; then
              log_error "Failed to create disk for $node"
              return 1
            fi
            log_info "PASS - Disk created successfully"
            
            # Wait for disk to be recognized
            sleep 5
            
            # Verify disk attachment
            if ! govc vm.info -json "$node" | jq -e '.VirtualMachines[0].Config.Hardware.Device[] | select(.Backing.FileName? and (.Backing.FileName | contains("dedicated_etcd_disk")))' > /dev/null; then
              log_error "Disk verification failed for $node"
              return 1
            fi
            log_info "PASS - Disk attachment verified"
            
            return 0
          }
          
          # Create and attach disks for all master nodes
          log_section "Creating and Attaching Disks to Master Nodes"
          
          for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
            if ! create_and_attach_disk "$master"; then
              log_error "Failed to setup disk for $master"
              exit 1
            fi
            log_info "Successfully completed disk setup for $master"
            echo ""
          done
          
          log_info "PASS - All disks created and attached successfully"
          
          # Wait for disks to be recognized by the OS
          log_info "Waiting 30 seconds for OS to recognize new disks..."
          sleep 30
          
          # Create etcd-find-secondary-device.sh script
          log_section "Creating Device Discovery Script"
          cat > /tmp/etcd-find-secondary-device.sh <<'EOF_DEVICE_SCRIPT'
          #!/bin/bash
          
          # Find the device with the specific size (200GB in this case)
          EXPECTED_SIZE_BYTES=$((200 * 1024 * 1024 * 1024))
          TOLERANCE=$((10 * 1024 * 1024 * 1024))
          
          for device in /dev/sd*; do
            if [[ ! "$device" =~ [0-9]$ ]] && [[ -b "$device" ]]; then
              size=$(blockdev --getsize64 "$device" 2>/dev/null || echo 0)
              diff=$((size > EXPECTED_SIZE_BYTES ? size - EXPECTED_SIZE_BYTES : EXPECTED_SIZE_BYTES - size))
              
              if [[ $diff -lt $TOLERANCE ]]; then
                if ! mount | grep -q "$device" && ! pvdisplay "$device" &>/dev/null; then
                  echo "$device"
                  exit 0
                fi
              fi
            fi
          done
          
          exit 1
          EOF_DEVICE_SCRIPT
          
          chmod +x /tmp/etcd-find-secondary-device.sh
          log_info "PASS - Device discovery script created"
          
          # Encode the script to base64
          SCRIPT_B64=$(base64 -w0 /tmp/etcd-find-secondary-device.sh)
          
          # Create and apply MachineConfig for etcd disk
          log_section "Creating MachineConfig for etcd Disk"
          cat > /tmp/etcd-mc.yaml <<EOF_MACHINECONFIG
          apiVersion: machineconfiguration.openshift.io/v1
          kind: MachineConfig
          metadata:
            labels:
              machineconfiguration.openshift.io/role: master
            name: 98-etcd-devices
          spec:
            config:
              ignition:
                version: 3.2.0
              systemd:
                units:
                - contents: |
                    [Unit]
                    Description=Make etcd filesystem on secondary disk
                    DefaultDependencies=no
                    After=local-fs-pre.target
                    Before=local-fs.target
                    ConditionPathExists=!/var/lib/etcd_migrated
                    
                    [Service]
                    Type=oneshot
                    RemainAfterExit=yes
                    ExecStart=/usr/local/bin/etcd-find-secondary-device.sh
                    ExecStart=/usr/sbin/mkfs.xfs -L etcd-secondary -f \$(cat /tmp/etcd-device)
                    ExecStart=/bin/touch /var/lib/etcd_migrated
                    
                    [Install]
                    RequiredBy=local-fs.target
                  enabled: true
                  name: etcd-format.service
                - contents: |
                    [Unit]
                    Description=Mount etcd filesystem
                    Before=local-fs.target
                    Requires=etcd-format.service
                    After=etcd-format.service
                    
                    [Mount]
                    What=/dev/disk/by-label/etcd-secondary
                    Where=/var/lib/etcd
                    Type=xfs
                    Options=defaults,prjquota
                    
                    [Install]
                    RequiredBy=local-fs.target
                  enabled: true
                  name: var-lib-etcd.mount
              storage:
                files:
                - contents:
                    source: data:text/plain;charset=utf-8;base64,$SCRIPT_B64
                  mode: 0755
                  overwrite: true
                  path: /usr/local/bin/etcd-find-secondary-device.sh
          EOF_MACHINECONFIG
          
          log_info "Applying MachineConfig..."
          if ! oc apply -f /tmp/etcd-mc.yaml; then
            log_error "Failed to apply MachineConfig"
            exit 1
          fi
          log_info "PASS - MachineConfig applied successfully"
          
          # Monitor MachineConfig rollout
          log_section "Monitoring MachineConfig Rollout"
          log_warn "This may take 15-30 minutes per node"
          
          check_mcp_status() {
            local updated=$(oc get mcp master -o jsonpath='{.status.updatedMachineCount}')
            local total=$(oc get mcp master -o jsonpath='{.status.machineCount}')
            local degraded=$(oc get mcp master -o jsonpath='{.status.degradedMachineCount}')
            local updating=$(oc get mcp master -o jsonpath='{.status.updatingMachineCount}')
            
            log_info "Status: Updated=$updated/$total, Updating=$updating, Degraded=$degraded"
            
            if [[ "$degraded" -gt 0 ]]; then
              log_error "MachineConfigPool is degraded"
              return 2
            fi
            
            if [[ "$updated" -eq "$total" ]]; then
              return 0
            fi
            
            return 1
          }
          
          timeout=3600
          elapsed=0
          interval=30
          
          while [[ $elapsed -lt $timeout ]]; do
            status_result=0
            check_mcp_status || status_result=$?
            
            if [[ $status_result -eq 0 ]]; then
              log_info "PASS - All master nodes updated successfully"
              break
            elif [[ $status_result -eq 2 ]]; then
              log_error "MachineConfigPool degraded, aborting"
              exit 1
            fi
            
            sleep $interval
            elapsed=$((elapsed + interval))
          done
          
          if [[ $elapsed -ge $timeout ]]; then
            log_error "Timeout waiting for MachineConfig rollout"
            exit 1
          fi
          
          # Wait for cluster stabilization
          log_info "Waiting for cluster to stabilize..."
          sleep 120
          
          # Verify etcd is using new disk
          log_section "Verifying etcd Disk Migration"
          
          verify_etcd_mount() {
            local node=$1
            log_info "Verifying $node..."
            
            # Check mount point
            local mount_check=$(oc debug "node/$node" --quiet -- chroot /host df -h /var/lib/etcd 2>/dev/null | grep '/dev/sd' || echo "")
            if [[ -z "$mount_check" ]]; then
              log_error "etcd not mounted on dedicated disk for $node"
              return 1
            fi
            
            # Check filesystem type
            local fs_check=$(oc debug "node/$node" --quiet -- chroot /host mount 2>/dev/null | grep '/var/lib/etcd' | grep 'xfs' || echo "")
            if [[ -z "$fs_check" ]]; then
              log_error "etcd filesystem is not xfs on $node"
              return 1
            fi
            
            # Check disk label
            local label_check=$(oc debug "node/$node" --quiet -- chroot /host blkid 2>/dev/null | grep 'etcd-secondary' || echo "")
            if [[ -z "$label_check" ]]; then
              log_warn "Could not verify disk label on $node"
            fi
            
            log_info "PASS - $node verified successfully"
            return 0
          }
          
          all_verified=true
          for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
            if ! verify_etcd_mount "$master"; then
              all_verified=false
            fi
          done
          
          if [[ "$all_verified" != "true" ]]; then
            log_error "Verification failed for one or more nodes"
            exit 1
          fi
          
          # Final etcd health check
          log_section "Final etcd Health Verification"
          
          log_info "Checking etcd cluster operator status..."
          etcd_available=$(oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}')
          etcd_degraded=$(oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Degraded")].status}')
          etcd_progressing=$(oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Progressing")].status}')
          
          if [[ "$etcd_available" != "True" ]]; then
            log_error "etcd cluster operator is not available"
            exit 1
          fi
          
          if [[ "$etcd_degraded" != "False" ]]; then
            log_error "etcd cluster operator is degraded"
            exit 1
          fi
          
          log_info "PASS - etcd cluster operator is healthy"
          log_info "  Available: $etcd_available"
          log_info "  Degraded: $etcd_degraded"
          log_info "  Progressing: $etcd_progressing"
          
          # Check etcd pod health
          log_info "Checking etcd pod health..."
          etcd_pods=$(oc get pods -n openshift-etcd -l app=etcd --no-headers 2>/dev/null | wc -l)
          etcd_running=$(oc get pods -n openshift-etcd -l app=etcd --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l)
          
          if [[ "$etcd_pods" -ne "$etcd_running" ]]; then
            log_error "Not all etcd pods are running ($etcd_running/$etcd_pods)"
            exit 1
          fi
          
          log_info "PASS - All $etcd_pods etcd pods are running"
          
          # Check etcd endpoints
          log_info "Checking etcd member list..."
          etcd_pod=$(oc get pods -n openshift-etcd -l app=etcd -o name 2>/dev/null | head -1)
          
          if [[ -n "$etcd_pod" ]]; then
            member_list=$(oc rsh -n openshift-etcd "$etcd_pod" etcdctl member list -w table 2>/dev/null || echo "")
            if [[ -n "$member_list" ]] && echo "$member_list" | grep -q "started"; then
              log_info "PASS - etcd members verified"
              echo "$member_list"
            else
              log_error "Failed to verify etcd members"
              exit 1
            fi
          else
            log_error "Could not find etcd pod"
            exit 1
          fi
          
          # Display disk usage
          log_section "Disk Usage Summary"
          for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
            log_info "Disk usage for $master:"
            oc debug "node/$master" --quiet -- chroot /host df -h /var/lib/etcd 2>/dev/null || log_warn "Could not get disk usage for $master"
          done
          
          log_section "etcd Disk Migration Completed Successfully"
          log_info "Next steps:"
          log_info "1. Monitor etcd performance using Grafana dashboard"
          log_info "2. Run FIO performance tests to verify disk performance"
          log_info "3. Keep this Job manifest for documentation"
          log_info "4. Consider creating a backup of the MachineConfig"
          
          exit 0
        
        workingDir: /tmp
        env:
        - name: vcnServer
          valueFrom:
            secretKeyRef:
              key: server
              name: vsphere-creds
        - name: vcnDatacenter
          valueFrom:
            secretKeyRef:
              key: datacenter
              name: vsphere-creds
        - name: vcnUser
          valueFrom:
            secretKeyRef:
              key: username
              name: vsphere-creds
        - name: vcnPwd
          valueFrom:
            secretKeyRef:
              key: password
              name: vsphere-creds
      restartPolicy: Never
      serviceAccountName: etcd-migration-sa
  backoffLimit: 1
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: etcd-migration-sa
  namespace: openshift-etcd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: etcd-migration-sa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: etcd-migration-sa
  namespace: openshift-etcd
---
apiVersion: v1
kind: Secret
metadata:
  name: vsphere-creds
  namespace: openshift-etcd
type: Opaque
stringData:
  server: "vcenter.example.com"
  datacenter: "DC1"
  username: "administrator@vsphere.local"
  password: "CHANGEME"





####performance

#!/bin/bash
set -euo pipefail

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
  echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
}

log_error() {
  echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
}

log_warn() {
  echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
}

log_section() {
  echo ""
  echo -e "${BLUE}====== $* ======${NC}"
}

# Performance thresholds based on Red Hat documentation
declare -A THRESHOLDS=(
  [fio_write_iops_min]=500
  [fio_write_latency_max]=10
  [fio_fsync_latency_max]=20
  [etcd_99_percentile_max]=50
  [etcd_wal_sync_max]=10
)

log_section "FIO Disk Performance Tests"

# Function to run FIO tests on a node
run_fio_test() {
  local node=$1
  local test_name=$2
  local fio_params=$3
  
  log_info "Running FIO test '$test_name' on $node..."
  
  local result=$(oc debug node/"$node" -- chroot /host bash -c "
    fio --directory=/var/lib/etcd \
        --name=$test_name \
        $fio_params \
        --output-format=json
  " 2>/dev/null)
  
  echo "$result" > "$results_dir/${node}_${test_name}.json"
  
  if [[ -z "$result" ]]; then
    log_error "FIO test failed on $node"
    return 1
  fi
  
  # Parse results
  local write_iops=$(echo "$result" | jq -r '.jobs[0].write.iops // 0' 2>/dev/null || echo "0")
  local write_lat=$(echo "$result" | jq -r '.jobs[0].write.lat_ns.mean // 0' 2>/dev/null || echo "0")
  local write_lat_ms=$(echo "scale=2; $write_lat / 1000000" | bc 2>/dev/null || echo "0")
  
  log_info "  Write IOPS: $write_iops"
  log_info "  Average write latency: ${write_lat_ms}ms"
  
  # Check thresholds
  if (( $(echo "$write_iops < ${THRESHOLDS[fio_write_iops_min]}" | bc -l) )); then
    log_warn "  Write IOPS below threshold (${THRESHOLDS[fio_write_iops_min]})"
  else
    log_info "  ✓ Write IOPS meets requirements"
  fi
  
  if (( $(echo "$write_lat_ms > ${THRESHOLDS[fio_write_latency_max]}" | bc -l) )); then
    log_warn "  Write latency above threshold (${THRESHOLDS[fio_write_latency_max]}ms)"
  else
    log_info "  ✓ Write latency meets requirements"
  fi
  
  return 0
}

# Run FIO tests on all master nodes
for node in "${master_nodes[@]}"; do
  log_info "Testing node: $node"
  
  # Test 1: Sequential write test
  run_fio_test "$node" "seq-write" \
    "--ioengine=libaio --direct=1 --bs=32k --size=1G --rw=write --numjobs=1 --time_based --runtime=60"
  
  # Test 2: Random write with fsync (simulates etcd workload)
  run_fio_test "$node" "rand-write-fsync" \
    "--ioengine=libaio --direct=1 --bs=2300 --size=1G --rw=randwrite --numjobs=1 --time_based --runtime=60 --fsync=1"
  
  # Test 3: Fsync latency test (critical for etcd)
  log_info "Running fsync latency test on $node..."
  fsync_result=$(oc debug node/"$node" -- chroot /host bash -c "
    cd /var/lib/etcd
    for i in {1..100}; do
      dd if=/dev/zero of=test_\$i bs=2300 count=1 conv=fsync 2>&1 | grep -oP '(?<=, )[0-9.]+ (?=s)'
    done | awk '{sum+=\$1; count++} END {print sum/count}'
    rm -f test_*
  " 2>/dev/null || echo "0")
  
  fsync_lat_ms=$(echo "scale=2; $fsync_result * 1000" | bc 2>/dev/null || echo "0")
  log_info "  Average fsync latency: ${fsync_lat_ms}ms"
  
  if (( $(echo "$fsync_lat_ms > ${THRESHOLDS[fio_fsync_latency_max]}" | bc -l) )); then
    log_warn "  Fsync latency above threshold (${THRESHOLDS[fio_fsync_latency_max]}ms)"
  else
    log_info "  ✓ Fsync latency meets requirements"
  fi
  
  echo ""
done

log_section "etcd-perf Benchmark Test"

# Check if we need to deploy etcd-perf
if ! oc get pod -n openshift-etcd | grep -q etcd-perf; then
  log_info "Deploying etcd-perf benchmark pod..."
  
  cat <<'EOF_ETCD_PERF' | oc apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: etcd-perf
  namespace: openshift-etcd
spec:
  containers:
  - name: etcd-perf
    image: quay.io/openshift-scale/etcd-perf:latest
    command:
    - /bin/bash
    - -c
    - |
      # Get etcd endpoint
      ETCD_ENDPOINTS=\$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace | xargs -I {} oc get endpoints -n {} etcd -o jsonpath='{.subsets[*].addresses[*].ip}' | tr ' ' ',' | sed 's/,/:2379,/g' | sed 's/\$/\:2379/')
      
      # Run etcd performance test
      /usr/local/bin/etcd-perf --endpoints=\$ETCD_ENDPOINTS \
        --clients=30 \
        --conns=10 \
        --rounds=100 \
        --key-size=256 \
        --val-size=1024
      
      # Keep pod running for log collection
      sleep 3600
    volumeMounts:
    - name: etcd-certs
      mountPath: /etc/etcd/certs
      readOnly: true
  volumes:
  - name: etcd-certs
    secret:
      secretName: etcd-client
  restartPolicy: Never
  serviceAccountName: etcd-perf-sa
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: etcd-perf-sa
  namespace: openshift-etcd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: etcd-perf-sa-binding
  namespace: openshift-etcd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
- kind: ServiceAccount
  name: etcd-perf-sa
  namespace: openshift-etcd
EOF

  log_info "Waiting for etcd-perf pod to start..."
  sleep 10
  
  # Wait for pod to be ready
  timeout=120
  elapsed=0
  while [[ $elapsed -lt $timeout ]]; do
    if oc get pod -n openshift-etcd etcd-perf -o jsonpath='{.status.phase}' 2>/dev/null | grep -q "Running"; then
      break
    fi
    sleep 5
    elapsed=$((elapsed + 5))
  done
fi

log_info "Running etcd-perf benchmark (this may take 2-3 minutes)..."

# Alternative: Run etcd-perf from etcd pod
log_info "Running etcd performance check from etcd pod..."
etcd_pod=$(oc get pod -n openshift-etcd -l app=etcd -o name | head -1)

perf_result=$(oc exec -n openshift-etcd "$etcd_pod" -c etcd -- /bin/bash -c '
etcdctl check perf --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca/ca.crt \
  --cert=/etc/etcd/client/tls.crt \
  --key=/etc/etcd/client/tls.key
' 2>&1 || echo "perf check failed")

echo "$perf_result" | tee "$results_dir/etcd_perf_check.txt"

# Parse etcd perf results
if echo "$perf_result" | grep -q "PASS"; then
  log_info "✓ etcd performance check PASSED"
else
  log_warn "etcd performance check did not pass completely"
fi

log_section "etcd Metrics Analysis"

# Get etcd metrics
log_info "Collecting etcd metrics from Prometheus..."

# Helper function to query Prometheus
query_prometheus() {
  local query=$1
  local result=$(oc exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- \
    curl -s "http://localhost:9090/api/v1/query?query=${query}" 2>/dev/null | \
    jq -r '.data.result[0].value[1] // "N/A"' 2>/dev/null || echo "N/A")
  echo "$result"
}

log_info "Querying etcd performance metrics..."

# WAL fsync duration (99th percentile) - should be < 10ms
wal_fsync_99=$(query_prometheus 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))')
wal_fsync_99_ms=$(echo "scale=2; $wal_fsync_99 * 1000" | bc 2>/dev/null || echo "N/A")
log_info "WAL fsync duration (99th): ${wal_fsync_99_ms}ms"

# Backend commit duration (99th percentile) - should be < 25ms
backend_commit_99=$(query_prometheus 'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m]))')
backend_commit_99_ms=$(echo "scale=2; $backend_commit_99 * 1000" | bc 2>/dev/null || echo "N/A")
log_info "Backend commit duration (99th): ${backend_commit_99_ms}ms"

# Leader changes - should be 0 or very low
leader_changes=$(query_prometheus 'rate(etcd_server_leader_changes_seen_total[1h])')
log_info "Leader changes (per hour): $leader_changes"

# Apply duration (99th percentile) - should be < 50ms
apply_99=$(query_prometheus 'histogram_quantile(0.99, rate(etcd_request_duration_seconds_bucket{operation="apply"}[5m]))')
apply_99_ms=$(echo "scale=2; $apply_99 * 1000" | bc 2>/dev/null || echo "N/A")
log_info "Apply duration (99th): ${apply_99_ms}ms"

# DB size
db_size=$(query_prometheus 'etcd_mvcc_db_total_size_in_bytes')
db_size_mb=$(echo "scale=2; $db_size / 1024 / 1024" | bc 2>/dev/null || echo "N/A")
log_info "etcd database size: ${db_size_mb}MB"

# Save metrics
cat > "$results_dir/etcd_metrics.txt" <<METRICS
etcd Performance Metrics
========================
WAL fsync duration (99th): ${wal_fsync_99_ms}ms (threshold: <${THRESHOLDS[etcd_wal_sync_max]}ms)
Backend commit duration (99th): ${backend_commit_99_ms}ms (threshold: <25ms)
Apply duration (99th): ${apply_99_ms}ms (threshold: <${THRESHOLDS[etcd_99_percentile_max]}ms)
Leader changes (hourly): $leader_changes (should be 0)
Database size: ${db_size_mb}MB
METRICS

# Evaluate metrics against thresholds
log_info ""
log_info "Evaluating metrics against thresholds..."
all_passed=true

if [[ "$wal_fsync_99_ms" != "N/A" ]]; then
  if (( $(echo "$wal_fsync_99_ms > ${THRESHOLDS[etcd_wal_sync_max]}" | bc -l) )); then
    log_warn "✗ WAL fsync duration exceeds threshold"
    all_passed=false
  else
    log_info "✓ WAL fsync duration within acceptable range"
  fi
fi

if [[ "$apply_99_ms" != "N/A" ]]; then
  if (( $(echo "$apply_99_ms > ${THRESHOLDS[etcd_99_percentile_max]}" | bc -l) )); then
    log_warn "✗ Apply duration exceeds threshold"
    all_passed=false
  else
    log_info "✓ Apply duration within acceptable range"
  fi
fi

log_section "Disk I/O Statistics"

for node in "${master_nodes[@]}"; do
  log_info "Disk statistics for $node:"
  
  oc debug node/"$node" -- chroot /host bash -c '
    echo "=== Disk Usage ==="
    df -h /var/lib/etcd
    echo ""
    echo "=== I/O Statistics ==="
    iostat -x 1 3 | grep -A3 "Device"
  ' 2>/dev/null | tee "$results_dir/${node}_iostat.txt" || log_warn "Could not collect iostat for $node"
  
  echo ""
done

log_section "etcd Health Check"

# Check etcd cluster health
log_info "Checking etcd cluster health..."
health_output=$(oc exec -n openshift-etcd "$etcd_pod" -c etcd -- /bin/bash -c '
etcdctl endpoint health \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca/ca.crt \
  --cert=/etc/etcd/client/tls.crt \
  --key=/etc/etcd/client/tls.key \
  --cluster
' 2>&1)

echo "$health_output" | tee "$results_dir/etcd_health.txt"

if echo "$health_output" | grep -q "is healthy"; then
  log_info "✓ All etcd members are healthy"
else
  log_error "✗ Some etcd members are unhealthy"
  all_passed=false
fi

# Check etcd member list
log_info ""
log_info "etcd member list:"
oc exec -n openshift-etcd "$etcd_pod" -c etcd -- /bin/bash -c '
etcdctl member list -w table \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca/ca.crt \
  --cert=/etc/etcd/client/tls.crt \
  --key=/etc/etcd/client/tls.key
' 2>&1 | tee "$results_dir/etcd_members.txt"

log_section "Performance Summary Report"

# Generate summary report
cat > "$results_dir/SUMMARY_REPORT.txt" <<REPORT
etcd Performance Verification Report
=====================================
Date: $(date)
Cluster: $(oc whoami --show-server)

DISK PERFORMANCE (FIO Tests)
-----------------------------
$(for node in "${master_nodes[@]}"; do
  echo "Node: $node"
  if [[ -f "$results_dir/${node}_seq-write.json" ]]; then
    iops=$(jq -r '.jobs[0].write.iops // 0' "$results_dir/${node}_seq-write.json" 2>/dev/null)
    echo "  Sequential Write IOPS: $iops"
  fi
  if [[ -f "$results_dir/${node}_rand-write-fsync.json" ]]; then
    iops=$(jq -r '.jobs[0].write.iops // 0' "$results_dir/${node}_rand-write-fsync.json" 2>/dev/null)
    echo "  Random Write with fsync IOPS: $iops"
  fi
  echo ""
done)

ETCD METRICS
------------
WAL fsync duration (99th): ${wal_fsync_99_ms}ms (threshold: <${THRESHOLDS[etcd_wal_sync_max]}ms)
Backend commit duration (99th): ${backend_commit_99_ms}ms (threshold: <25ms)
Apply duration (99th): ${apply_99_ms}ms (threshold: <${THRESHOLDS[etcd_99_percentile_max]}ms)
Leader changes (hourly): $leader_changes
Database size: ${db_size_mb}MB

ETCD HEALTH
-----------
$(cat "$results_dir/etcd_health.txt" 2>/dev/null || echo "N/A")

OVERALL ASSESSMENT
------------------
REPORT

if [[ "$all_passed" == true ]]; then
  echo "STATUS: PASSED ✓" >> "$results_dir/SUMMARY_REPORT.txt"
  echo "All performance metrics meet requirements." >> "$results_dir/SUMMARY_REPORT.txt"
  log_info "${GREEN}✓ All performance checks PASSED${NC}"
else
  echo "STATUS: NEEDS ATTENTION ⚠" >> "$results_dir/SUMMARY_REPORT.txt"
  echo "Some metrics are outside acceptable thresholds. Review detailed logs." >> "$results_dir/SUMMARY_REPORT.txt"
  log_warn "${YELLOW}⚠ Some performance checks need attention${NC}"
fi

cat "$results_dir/SUMMARY_REPORT.txt"

log_section "Verification Complete"
log_info "Detailed results saved to: $results_dir"
log_info "Summary report: $results_dir/SUMMARY_REPORT.txt"
log_info ""
log_info "Next steps:"
log_info "  1. Review the summary report"
log_info "  2. Import Grafana dashboard for continuous monitoring"
log_info "  3. Run Prometheus query verification script"

exit 0etcd Performance Verification Script"
log_info "This script will:"
log_info "  1. Run FIO disk performance tests (Red Hat KB 4885641)"
log_info "  2. Run etcd-perf benchmark (Red Hat KB 6271341)"
log_info "  3. Check etcd metrics and health"
log_info "  4. Generate performance report"
echo ""

# Get master nodes
log_info "Retrieving master node information..."
master_nodes=($(oc get nodes -l node-role.kubernetes.io/master= -o jsonpath='{.items[*].metadata.name}'))

if [[ ${#master_nodes[@]} -ne 3 ]]; then
  log_error "Expected 3 master nodes, found ${#master_nodes[@]}"
  exit 1
fi

log_info "Master nodes: ${master_nodes[*]}"

# Results storage
results_dir="/tmp/etcd-perf-results-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$results_dir"
log_info "Results will be saved to: $results_dir"

log_section "









#####job

apiVersion: batch/v1
kind: Job
metadata:
  name: etcd-disk-migration
  namespace: openshift-logging
  labels:
    app.kubernetes.io/name: etcd-disk-migration
spec:
  template:
    spec:
      imagePullSecrets:
      - name: artifactory
      containers:
      - name: etcd-disk-migration
        image: artifactory.devops.com/local-docker-ocp4aas-certified-releases/ocp4-argocd/azure-oc-cli:latest
        command:
        - /bin/bash
        - -c
        args:
        - |
          #!/usr/bin/env bash
          set -euo pipefail
          
          # Color codes for output
          RED='\033[0;31m'
          GREEN='\033[0;32m'
          YELLOW='\033[1;33m'
          NC='\033[0m' # No Color
          
          # Logging functions
          log_info() {
            echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
          }
          
          log_error() {
            echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
          }
          
          log_warn() {
            echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
          }
          
          # Error handling
          trap 'handle_error $? $LINENO' ERR
          
          handle_error() {
            local exit_code=$1
            local line_number=$2
            log_error "Script failed with exit code $exit_code at line $line_number"
            log_error "Rolling back changes..."
            # Cleanup will be handled by separate rollback script
            exit $exit_code
          }
          
          # Configuration
          DISK_SIZE="200G"
          ETCD_DEVICE="/dev/sdc"
          MOUNT_PATH="/var/lib/etcd"
          
          # Export environment variables for govc
          export GOVC_URL="https://${vcnServer}"
          export GOVC_USERNAME="${vcnUser}"
          export GOVC_PASSWORD="${vcnPwd}"
          export GOVC_INSECURE=true
          
          log_info "Starting etcd disk migration process"
          
          # Get master nodes
          log_info "Retrieving master node information..."
          masterVM1=$(oc get nodes | grep master | awk '{print $1}' | head -n1)
          masterVM2=$(oc get nodes | grep master | awk '{print $1}' | head -n2 | tail -n1)
          masterVM3=$(oc get nodes | grep master | awk '{print $1}' | tail -n1)
          
          if [[ -z "$masterVM1" ]] || [[ -z "$masterVM2" ]] || [[ -z "$masterVM3" ]]; then
            log_error "Failed to retrieve all three master nodes"
            exit 1
          fi
          
          log_info "Master nodes identified:"
          log_info "  Master 1: $masterVM1"
          log_info "  Master 2: $masterVM2"
          log_info "  Master 3: $masterVM3"
          
          # Verify initial etcd health
          log_info "Verifying initial etcd cluster health..."
          if ! oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_error "etcd cluster is not healthy before migration. Aborting."
            exit 1
          fi
          log_info "✓ etcd cluster is healthy"
          
          # Get datastore information
          log_info "Retrieving datastore information..."
          DATASTORE=$(govc datastore.ls "[scc-w2c5-vsan] ${masterVM1}/" | grep -v "master-0_dedicated" | head -n1 | cut -d'/' -f1)
          
          if [[ -z "$DATASTORE" ]]; then
            log_error "Failed to retrieve datastore information"
            exit 1
          fi
          
          log_info "Using datastore: $DATASTORE"
          
          # Function to create and attach disk
          create_and_attach_disk() {
            local node=$1
            local disk_name="${node}_dedicated_etcd_disk.vmdk"
            
            log_info "Processing node: $node"
            
            # Check if disk already exists
            if govc datastore.ls "[$DATASTORE] ${node}/" | grep -q "dedicated_etcd_disk.vmdk"; then
              log_warn "Dedicated etcd disk already exists for $node, skipping creation"
              return 0
            fi
            
            # Create disk
            log_info "Creating ${DISK_SIZE} disk for $node..."
            if ! govc datastore.disk.create -ds "$DATASTORE" -size "$DISK_SIZE" "[$DATASTORE] ${node}/${disk_name}"; then
              log_error "Failed to create disk for $node"
              return 1
            fi
            log_info "✓ Disk created successfully"
            
            # Attach disk to VM
            log_info "Attaching disk to $node..."
            if ! govc vm.disk.attach -vm "$node" -ds "$DATASTORE" -disk "[$DATASTORE] ${node}/${disk_name}"; then
              log_error "Failed to attach disk to $node"
              return 1
            fi
            log_info "✓ Disk attached successfully"
            
            # Verify disk attachment
            sleep 5
            if ! govc vm.info -json "$node" | jq '.VirtualMachines[].Config.Hardware.Device[] | select(.Backing.FileName?)' | grep -q "dedicated_etcd_disk.vmdk"; then
              log_error "Disk verification failed for $node"
              return 1
            fi
            log_info "✓ Disk attachment verified"
            
            return 0
          }
          
          # Create and attach disks for all master nodes
          log_info "====== Creating and attaching disks to master nodes ======"
          
          for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
            if ! create_and_attach_disk "$master"; then
              log_error "Failed to setup disk for $master"
              exit 1
            fi
            log_info "Successfully completed disk setup for $master"
            echo ""
          done
          
          log_info "====== All disks created and attached successfully ======"
          
          # Wait for disks to be recognized by the OS
          log_info "Waiting 30 seconds for OS to recognize new disks..."
          sleep 30
          
          # Create etcd-find-secondary-device.sh script
          log_info "Creating etcd-find-secondary-device.sh script..."
          cat > /tmp/etcd-find-secondary-device.sh << 'SCRIPT_EOF'
          #!/bin/bash
          
          # Find the device with the specific size (200GB in this case)
          EXPECTED_SIZE_BYTES=$((200 * 1024 * 1024 * 1024))
          TOLERANCE=$((10 * 1024 * 1024 * 1024))  # 10GB tolerance
          
          for device in /dev/sd*; do
            if [[ ! "$device" =~ [0-9]$ ]] && [[ -b "$device" ]]; then
              size=$(blockdev --getsize64 "$device" 2>/dev/null || echo 0)
              diff=$((size > EXPECTED_SIZE_BYTES ? size - EXPECTED_SIZE_BYTES : EXPECTED_SIZE_BYTES - size))
              
              if [[ $diff -lt $TOLERANCE ]]; then
                # Check if device is not mounted and not part of a RAID/LVM
                if ! mount | grep -q "$device" && ! pvdisplay "$device" &>/dev/null; then
                  echo "$device"
                  exit 0
                fi
              fi
            fi
          done
          
          exit 1
          SCRIPT_EOF
          
          chmod +x /tmp/etcd-find-secondary-device.sh
          
          # Create and apply MachineConfig for etcd disk
          log_info "Creating MachineConfig for etcd disk..."
          cat > /tmp/etcd-mc.yaml << 'MC_EOF'
          apiVersion: machineconfiguration.openshift.io/v1
          kind: MachineConfig
          metadata:
            labels:
              machineconfiguration.openshift.io/role: master
            name: 98-etcd-devices
          spec:
            config:
              ignition:
                version: 3.2.0
              systemd:
                units:
                - contents: |
                    [Unit]
                    Description=Make etcd filesystem on secondary disk
                    DefaultDependencies=no
                    After=local-fs-pre.target
                    Before=local-fs.target
                    ConditionPathExists=!/var/lib/etcd_migrated
                    
                    [Service]
                    Type=oneshot
                    RemainAfterExit=yes
                    ExecStart=/usr/local/bin/etcd-find-secondary-device.sh
                    ExecStart=/usr/sbin/mkfs.xfs -L etcd -f /dev/disk/by-label/etcd-secondary
                    ExecStart=/bin/touch /var/lib/etcd_migrated
                    
                    [Install]
                    RequiredBy=local-fs.target
                  enabled: true
                  name: etcd-format.service
                - contents: |
                    [Unit]
                    Description=Mount etcd filesystem
                    Before=local-fs.target
                    Requires=etcd-format.service
                    After=etcd-format.service
                    
                    [Mount]
                    What=/dev/disk/by-label/etcd-secondary
                    Where=/var/lib/etcd
                    Type=xfs
                    Options=defaults,prjquota
                    
                    [Install]
                    RequiredBy=local-fs.target
                  enabled: true
                  name: var-lib-etcd.mount
              storage:
                files:
                - contents:
                    source: data:text/plain;charset=utf-8;base64,{{ ETCD_FIND_SCRIPT_BASE64 }}
                  mode: 0755
                  overwrite: true
                  path: /usr/local/bin/etcd-find-secondary-device.sh
          MC_EOF
          
          # Encode the script to base64
          SCRIPT_B64=$(base64 -w0 /tmp/etcd-find-secondary-device.sh)
          sed -i "s|{{ ETCD_FIND_SCRIPT_BASE64 }}|$SCRIPT_B64|g" /tmp/etcd-mc.yaml
          
          log_info "Applying MachineConfig..."
          if ! oc apply -f /tmp/etcd-mc.yaml; then
            log_error "Failed to apply MachineConfig"
            exit 1
          fi
          log_info "✓ MachineConfig applied successfully"
          
          # Monitor MachineConfig rollout
          log_info "Monitoring MachineConfig rollout (this may take 15-30 minutes)..."
          
          check_mcp_status() {
            local updated=$(oc get mcp master -o jsonpath='{.status.updatedMachineCount}')
            local total=$(oc get mcp master -o jsonpath='{.status.machineCount}')
            local degraded=$(oc get mcp master -o jsonpath='{.status.degradedMachineCount}')
            
            echo "Updated: $updated/$total, Degraded: $degraded"
            
            if [[ "$degraded" -gt 0 ]]; then
              log_error "MachineConfigPool is degraded"
              return 2
            fi
            
            if [[ "$updated" -eq "$total" ]]; then
              return 0
            fi
            
            return 1
          }
          
          timeout=3600  # 60 minutes timeout
          elapsed=0
          interval=30
          
          while [[ $elapsed -lt $timeout ]]; do
            if check_mcp_status; then
              log_info "✓ All master nodes updated successfully"
              break
            elif [[ $? -eq 2 ]]; then
              log_error "MachineConfigPool degraded, aborting"
              exit 1
            fi
            
            log_info "Waiting for nodes to update... (${elapsed}s elapsed)"
            sleep $interval
            elapsed=$((elapsed + interval))
          done
          
          if [[ $elapsed -ge $timeout ]]; then
            log_error "Timeout waiting for MachineConfig rollout"
            exit 1
          fi
          
          # Wait for cluster stabilization
          log_info "Waiting for cluster to stabilize..."
          sleep 120
          
          # Verify etcd is using new disk
          log_info "====== Verifying etcd disk migration ======"
          
          verify_etcd_mount() {
            local node=$1
            log_info "Verifying $node..."
            
            # Check mount point
            if ! oc debug node/"$node" -- chroot /host bash -c "df -h /var/lib/etcd | grep -q '/dev/sd'"; then
              log_error "etcd not mounted on dedicated disk for $node"
              return 1
            fi
            
            # Check filesystem type
            if ! oc debug node/"$node" -- chroot /host bash -c "mount | grep '/var/lib/etcd' | grep -q 'xfs'"; then
              log_error "etcd filesystem is not xfs on $node"
              return 1
            fi
            
            log_info "✓ $node verified successfully"
            return 0
          }
          
          all_verified=true
          for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
            if ! verify_etcd_mount "$master"; then
              all_verified=false
            fi
          done
          
          if [[ "$all_verified" != true ]]; then
            log_error "Verification failed for one or more nodes"
            exit 1
          fi
          
          # Final etcd health check
          log_info "====== Final etcd health verification ======"
          
          log_info "Checking etcd cluster operator status..."
          if ! oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
            log_error "etcd cluster operator is not available"
            exit 1
          fi
          
          if ! oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Degraded")].status}' | grep -q "False"; then
            log_error "etcd cluster operator is degraded"
            exit 1
          fi
          
          log_info "✓ etcd cluster operator is healthy"
          
          # Check etcd pod health
          log_info "Checking etcd pod health..."
          etcd_pods=$(oc get pods -n openshift-etcd -l app=etcd --no-headers | wc -l)
          etcd_running=$(oc get pods -n openshift-etcd -l app=etcd --field-selector=status.phase=Running --no-headers | wc -l)
          
          if [[ "$etcd_pods" -ne "$etcd_running" ]]; then
            log_error "Not all etcd pods are running ($etcd_running/$etcd_pods)"
            exit 1
          fi
          
          log_info "✓ All $etcd_pods etcd pods are running"
          
          # Check etcd endpoints
          log_info "Checking etcd member list..."
          if ! oc rsh -n openshift-etcd $(oc get pods -n openshift-etcd -l app=etcd -o name | head -1) \
            etcdctl member list -w table 2>/dev/null | grep -q "started"; then
            log_error "Failed to verify etcd members"
            exit 1
          fi
          
          log_info "✓ etcd members verified"
          
          # Display disk usage
          log_info "====== Disk usage summary ======"
          for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
            log_info "Disk usage for $master:"
            oc debug node/"$master" -- chroot /host df -h /var/lib/etcd 2>/dev/null || true
          done
          
          log_info "====== etcd disk migration completed successfully ======"
          log_info "Next steps:"
          log_info "1. Run verification script: verify-etcd-performance.sh"
          log_info "2. Monitor etcd metrics in Grafana dashboard"
          log_info "3. Keep rollback script ready in case of issues"
          
          exit 0
        
        workingDir: /opt/ibm/etcd-migration
        env:
        - name: vcnServer
          valueFrom:
            secretKeyRef:
              key: server
              name: vsphere-creds
        - name: vcnDatacenter
          valueFrom:
            secretKeyRef:
              key: datacenter
              name: vsphere-creds
        - name: vcnUser
          valueFrom:
            secretKeyRef:
              key: username
              name: vsphere-creds
        - name: vcnPwd
          valueFrom:
            secretKeyRef:
              key: password
              name: vsphere-creds
      restartPolicy: Never
      serviceAccountName: etcd-migration-sa
  backoffLimit: 1
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: etcd-migration-sa
  namespace: openshift-logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: etcd-migration-sa-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: etcd-migration-sa
  namespace: openshift-logging



####rollback


#!/bin/bash
set -euo pipefail

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() {
  echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
}

log_error() {
  echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
}

log_warn() {
  echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
}

# Confirmation prompt
echo -e "${RED}WARNING: This will rollback etcd disk migration${NC}"
echo "This will:"
echo "  1. Remove the MachineConfig 98-etcd-devices"
echo "  2. Trigger master node reboot to original configuration"
echo "  3. Detach dedicated etcd disks from master nodes"
echo "  4. Optionally delete the created disks"
echo ""
read -p "Are you sure you want to proceed? (yes/no): " -r
if [[ ! $REPLY =~ ^yes$ ]]; then
  log_info "Rollback cancelled"
  exit 0
fi

log_info "====== Starting etcd disk migration rollback ======"

# Check current etcd health
log_info "Checking current etcd health..."
if ! oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
  log_warn "etcd cluster is not healthy, but continuing with rollback"
fi

# Export govc environment variables
export GOVC_URL="https://${GOVC_SERVER}"
export GOVC_USERNAME="${GOVC_USERNAME}"
export GOVC_PASSWORD="${GOVC_PASSWORD}"
export GOVC_INSECURE=true

# Get master nodes
log_info "Retrieving master node information..."
masterVM1=$(oc get nodes | grep master | awk '{print $1}' | head -n1)
masterVM2=$(oc get nodes | grep master | awk '{print $1}' | head -n2 | tail -n1)
masterVM3=$(oc get nodes | grep master | awk '{print $1}' | tail -n1)

if [[ -z "$masterVM1" ]] || [[ -z "$masterVM2" ]] || [[ -z "$masterVM3" ]]; then
  log_error "Failed to retrieve all three master nodes"
  exit 1
fi

log_info "Master nodes: $masterVM1, $masterVM2, $masterVM3"

# Remove MachineConfig
log_info "Removing MachineConfig 98-etcd-devices..."
if oc get mc 98-etcd-devices &>/dev/null; then
  if ! oc delete mc 98-etcd-devices; then
    log_error "Failed to delete MachineConfig"
    exit 1
  fi
  log_info "✓ MachineConfig deleted"
else
  log_warn "MachineConfig 98-etcd-devices not found"
fi

# Monitor rollback
log_info "Monitoring master nodes rollback (this will take 15-30 minutes)..."
log_info "Nodes will reboot one at a time to revert to original configuration"

timeout=3600
elapsed=0
interval=30

while [[ $elapsed -lt $timeout ]]; do
  updated=$(oc get mcp master -o jsonpath='{.status.updatedMachineCount}' 2>/dev/null || echo "0")
  total=$(oc get mcp master -o jsonpath='{.status.machineCount}' 2>/dev/null || echo "3")
  degraded=$(oc get mcp master -o jsonpath='{.status.degradedMachineCount}' 2>/dev/null || echo "0")
  
  log_info "MCP Status - Updated: $updated/$total, Degraded: $degraded (${elapsed}s elapsed)"
  
  if [[ "$degraded" -gt 0 ]]; then
    log_error "MachineConfigPool is degraded during rollback"
    log_error "Manual intervention may be required"
    exit 1
  fi
  
  if [[ "$updated" -eq "$total" ]]; then
    log_info "✓ All master nodes rolled back successfully"
    break
  fi
  
  sleep $interval
  elapsed=$((elapsed + interval))
done

if [[ $elapsed -ge $timeout ]]; then
  log_error "Timeout waiting for rollback completion"
  exit 1
fi

# Wait for cluster stabilization
log_info "Waiting for cluster to stabilize..."
sleep 120

# Verify etcd health after rollback
log_info "Verifying etcd health after rollback..."
retries=10
while [[ $retries -gt 0 ]]; do
  if oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' | grep -q "True"; then
    log_info "✓ etcd cluster is healthy after rollback"
    break
  fi
  log_warn "etcd not yet healthy, waiting... ($retries retries left)"
  sleep 30
  retries=$((retries - 1))
done

if [[ $retries -eq 0 ]]; then
  log_error "etcd cluster is not healthy after rollback"
  log_error "Manual investigation required"
  exit 1
fi

# Detach disks from VMs
log_info "====== Detaching dedicated etcd disks from master nodes ======"

detach_disk() {
  local node=$1
  log_info "Detaching disk from $node..."
  
  # Find the dedicated etcd disk
  disk_path=$(govc vm.info -json "$node" | jq -r '.VirtualMachines[].Config.Hardware.Device[] | select(.Backing.FileName? and (.Backing.FileName | contains("dedicated_etcd_disk"))) | .Backing.FileName' | head -1)
  
  if [[ -z "$disk_path" ]]; then
    log_warn "No dedicated etcd disk found on $node"
    return 0
  fi
  
  log_info "Found disk: $disk_path"
  
  if ! govc vm.disk.detach -vm "$node" -disk "$disk_path"; then
    log_error "Failed to detach disk from $node"
    return 1
  fi
  
  log_info "✓ Disk detached from $node"
  return 0
}

for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
  if ! detach_disk "$master"; then
    log_error "Failed to detach disk from $master"
  fi
done

# Prompt for disk deletion
echo ""
read -p "Do you want to DELETE the created disks? This cannot be undone! (yes/no): " -r
if [[ $REPLY =~ ^yes$ ]]; then
  log_info "====== Deleting dedicated etcd disks ======"
  
  DATASTORE=$(govc datastore.ls "[scc-w2c5-vsan] ${masterVM1}/" 2>/dev/null | head -1 | cut -d'/' -f1 || echo "")
  
  if [[ -z "$DATASTORE" ]]; then
    log_error "Could not determine datastore"
  else
    for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
      disk_file="[$DATASTORE] ${master}/${master}_dedicated_etcd_disk.vmdk"
      
      log_info "Deleting disk for $master..."
      if govc datastore.rm -f "$disk_file" 2>/dev/null; then
        log_info "✓ Disk deleted for $master"
      else
        log_warn "Could not delete disk for $master (may not exist)"
      fi
    done
  fi
else
  log_info "Disks not deleted. They remain detached and can be manually removed later."
fi

# Final verification
log_info "====== Rollback verification ======"

log_info "Checking that etcd is using root disk..."
for master in "$masterVM1" "$masterVM2" "$masterVM3"; do
  log_info "Verifying $master..."
  mount_output=$(oc debug node/"$master" -- chroot /host mount 2>/dev/null | grep '/var/lib/etcd' || echo "none")
  
  if echo "$mount_output" | grep -q "dedicated"; then
    log_error "$master still shows dedicated disk mounted"
  else
    log_info "✓ $master using root filesystem for etcd"
  fi
done

# Check etcd pods
etcd_running=$(oc get pods -n openshift-etcd -l app=etcd --field-selector=status.phase=Running --no-headers | wc -l)
log_info "Running etcd pods: $etcd_running"

if [[ "$etcd_running" -eq 3 ]]; then
  log_info "✓ All etcd pods are running"
else
  log_warn "Not all etcd pods are running: $etcd_running/3"
fi

log_info "====== Rollback completed ======"
log_info "Summary:"
log_info "  - MachineConfig removed"
log_info "  - Master nodes reverted to original configuration"
log_info "  - Dedicated etcd disks detached"
log_info "  - etcd cluster health: $(oc get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}')"
log_info ""
log_info "Please monitor the cluster for the next few hours to ensure stability."

exit 0


##etcd perfomrance

#!/bin/bash
set -euo pipefail

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
  echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
}

log_error() {
  echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
}

log_warn() {
  echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
}

log_section() {
  echo ""
  echo -e "${BLUE}====== $* ======${NC}"
}

# Performance thresholds based on Red Hat documentation
declare -A THRESHOLDS=(
  [fio_write_iops_min]=500
  [fio_write_latency_max]=10
  [fio_fsync_latency_max]=20
  [etcd_99_percentile_max]=50
  [etcd_wal_sync_max]=10
)

log_section "FIO Disk Performance Tests"

# Function to run FIO tests on a node
run_fio_test() {
  local node=$1
  local test_name=$2
  local fio_params=$3
  
  log_info "Running FIO test '$test_name' on $node..."
  
  local result=$(oc debug node/"$node" -- chroot /host bash -c "
    fio --directory=/var/lib/etcd \
        --name=$test_name \
        $fio_params \
        --output-format=json
  " 2>/dev/null)
  
  echo "$result" > "$results_dir/${node}_${test_name}.json"
  
  if [[ -z "$result" ]]; then
    log_error "FIO test failed on $node"
    return 1
  fi
  
  # Parse results
  local write_iops=$(echo "$result" | jq -r '.jobs[0].write.iops // 0' 2>/dev/null || echo "0")
  local write_lat=$(echo "$result" | jq -r '.jobs[0].write.lat_ns.mean // 0' 2>/dev/null || echo "0")
  local write_lat_ms=$(echo "scale=2; $write_lat / 1000000" | bc 2>/dev/null || echo "0")
  
  log_info "  Write IOPS: $write_iops"
  log_info "  Average write latency: ${write_lat_ms}ms"
  
  # Check thresholds
  if (( $(echo "$write_iops < ${THRESHOLDS[fio_write_iops_min]}" | bc -l) )); then
    log_warn "  Write IOPS below threshold (${THRESHOLDS[fio_write_iops_min]})"
  else
    log_info "  ✓ Write IOPS meets requirements"
  fi
  
  if (( $(echo "$write_lat_ms > ${THRESHOLDS[fio_write_latency_max]}" | bc -l) )); then
    log_warn "  Write latency above threshold (${THRESHOLDS[fio_write_latency_max]}ms)"
  else
    log_info "  ✓ Write latency meets requirements"
  fi
  
  return 0
}

# Run FIO tests on all master nodes
for node in "${master_nodes[@]}"; do
  log_info "Testing node: $node"
  
  # Test 1: Sequential write test
  run_fio_test "$node" "seq-write" \
    "--ioengine=libaio --direct=1 --bs=32k --size=1G --rw=write --numjobs=1 --time_based --runtime=60"
  
  # Test 2: Random write with fsync (simulates etcd workload)
  run_fio_test "$node" "rand-write-fsync" \
    "--ioengine=libaio --direct=1 --bs=2300 --size=1G --rw=randwrite --numjobs=1 --time_based --runtime=60 --fsync=1"
  
  # Test 3: Fsync latency test (critical for etcd)
  log_info "Running fsync latency test on $node..."
  fsync_result=$(oc debug node/"$node" -- chroot /host bash -c "
    cd /var/lib/etcd
    for i in {1..100}; do
      dd if=/dev/zero of=test_\$i bs=2300 count=1 conv=fsync 2>&1 | grep -oP '(?<=, )[0-9.]+ (?=s)'
    done | awk '{sum+=\$1; count++} END {print sum/count}'
    rm -f test_*
  " 2>/dev/null || echo "0")
  
  fsync_lat_ms=$(echo "scale=2; $fsync_result * 1000" | bc 2>/dev/null || echo "0")
  log_info "  Average fsync latency: ${fsync_lat_ms}ms"
  
  if (( $(echo "$fsync_lat_ms > ${THRESHOLDS[fio_fsync_latency_max]}" | bc -l) )); then
    log_warn "  Fsync latency above threshold (${THRESHOLDS[fio_fsync_latency_max]}ms)"
  else
    log_info "  ✓ Fsync latency meets requirements"
  fi
  
  echo ""
done

log_section "etcd-perf Benchmark Test"

# Check if we need to deploy etcd-perf
if ! oc get pod -n openshift-etcd | grep -q etcd-perf; then
  log_info "Deploying etcd-perf benchmark pod..."
  
  cat <<EOF | oc apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: etcd-perf
  namespace: openshift-etcd
spec:
  containers:
  - name: etcd-perf
    image: quay.io/openshift-scale/etcd-perf:latest
    command:
    - /bin/bash
    - -c
    - |
      # Get etcd endpoint
      ETCD_ENDPOINTS=\$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace | xargs -I {} oc get endpoints -n {} etcd -o jsonpath='{.subsets[*].addresses[*].ip}' | tr ' ' ',' | sed 's/,/:2379,/g' | sed 's/\$/\:2379/')
      
      # Run etcd performance test
      /usr/local/bin/etcd-perf --endpoints=\$ETCD_ENDPOINTS \
        --clients=30 \
        --conns=10 \
        --rounds=100 \
        --key-size=256 \
        --val-size=1024
      
      # Keep pod running for log collection
      sleep 3600
    volumeMounts:
    - name: etcd-certs
      mountPath: /etc/etcd/certs
      readOnly: true
  volumes:
  - name: etcd-certs
    secret:
      secretName: etcd-client
  restartPolicy: Never
  serviceAccountName: etcd-perf-sa
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: etcd-perf-sa
  namespace: openshift-etcd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: etcd-perf-sa-binding
  namespace: openshift-etcd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
- kind: ServiceAccount
  name: etcd-perf-sa
  namespace: openshift-etcd
EOF

  log_info "Waiting for etcd-perf pod to start..."
  sleep 10
  
  # Wait for pod to be ready
  timeout=120
  elapsed=0
  while [[ $elapsed -lt $timeout ]]; do
    if oc get pod -n openshift-etcd etcd-perf -o jsonpath='{.status.phase}' 2>/dev/null | grep -q "Running"; then
      break
    fi
    sleep 5
    elapsed=$((elapsed + 5))
  done
fi

log_info "Running etcd-perf benchmark (this may take 2-3 minutes)..."

# Alternative: Run etcd-perf from etcd pod
log_info "Running etcd performance check from etcd pod..."
etcd_pod=$(oc get pod -n openshift-etcd -l app=etcd -o name | head -1)

perf_result=$(oc exec -n openshift-etcd "$etcd_pod" -c etcd -- /bin/bash -c '
etcdctl check perf --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca/ca.crt \
  --cert=/etc/etcd/client/tls.crt \
  --key=/etc/etcd/client/tls.key
' 2>&1 || echo "perf check failed")

echo "$perf_result" | tee "$results_dir/etcd_perf_check.txt"

# Parse etcd perf results
if echo "$perf_result" | grep -q "PASS"; then
  log_info "✓ etcd performance check PASSED"
else
  log_warn "etcd performance check did not pass completely"
fi

log_section "etcd Metrics Analysis"

# Get etcd metrics
log_info "Collecting etcd metrics from Prometheus..."

# Helper function to query Prometheus
query_prometheus() {
  local query=$1
  local result=$(oc exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- \
    curl -s "http://localhost:9090/api/v1/query?query=${query}" 2>/dev/null | \
    jq -r '.data.result[0].value[1] // "N/A"' 2>/dev/null || echo "N/A")
  echo "$result"
}

log_info "Querying etcd performance metrics..."

# WAL fsync duration (99th percentile) - should be < 10ms
wal_fsync_99=$(query_prometheus 'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))')
wal_fsync_99_ms=$(echo "scale=2; $wal_fsync_99 * 1000" | bc 2>/dev/null || echo "N/A")
log_info "WAL fsync duration (99th): ${wal_fsync_99_ms}ms"

# Backend commit duration (99th percentile) - should be < 25ms
backend_commit_99=$(query_prometheus 'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m]))')
backend_commit_99_ms=$(echo "scale=2; $backend_commit_99 * 1000" | bc 2>/dev/null || echo "N/A")
log_info "Backend commit duration (99th): ${backend_commit_99_ms}ms"

# Leader changes - should be 0 or very low
leader_changes=$(query_prometheus 'rate(etcd_server_leader_changes_seen_total[1h])')
log_info "Leader changes (per hour): $leader_changes"

# Apply duration (99th percentile) - should be < 50ms
apply_99=$(query_prometheus 'histogram_quantile(0.99, rate(etcd_request_duration_seconds_bucket{operation="apply"}[5m]))')
apply_99_ms=$(echo "scale=2; $apply_99 * 1000" | bc 2>/dev/null || echo "N/A")
log_info "Apply duration (99th): ${apply_99_ms}ms"

# DB size
db_size=$(query_prometheus 'etcd_mvcc_db_total_size_in_bytes')
db_size_mb=$(echo "scale=2; $db_size / 1024 / 1024" | bc 2>/dev/null || echo "N/A")
log_info "etcd database size: ${db_size_mb}MB"

# Save metrics
cat > "$results_dir/etcd_metrics.txt" <<METRICS
etcd Performance Metrics
========================
WAL fsync duration (99th): ${wal_fsync_99_ms}ms (threshold: <${THRESHOLDS[etcd_wal_sync_max]}ms)
Backend commit duration (99th): ${backend_commit_99_ms}ms (threshold: <25ms)
Apply duration (99th): ${apply_99_ms}ms (threshold: <${THRESHOLDS[etcd_99_percentile_max]}ms)
Leader changes (hourly): $leader_changes (should be 0)
Database size: ${db_size_mb}MB
METRICS

# Evaluate metrics against thresholds
log_info ""
log_info "Evaluating metrics against thresholds..."
all_passed=true

if [[ "$wal_fsync_99_ms" != "N/A" ]]; then
  if (( $(echo "$wal_fsync_99_ms > ${THRESHOLDS[etcd_wal_sync_max]}" | bc -l) )); then
    log_warn "✗ WAL fsync duration exceeds threshold"
    all_passed=false
  else
    log_info "✓ WAL fsync duration within acceptable range"
  fi
fi

if [[ "$apply_99_ms" != "N/A" ]]; then
  if (( $(echo "$apply_99_ms > ${THRESHOLDS[etcd_99_percentile_max]}" | bc -l) )); then
    log_warn "✗ Apply duration exceeds threshold"
    all_passed=false
  else
    log_info "✓ Apply duration within acceptable range"
  fi
fi

log_section "Disk I/O Statistics"

for node in "${master_nodes[@]}"; do
  log_info "Disk statistics for $node:"
  
  oc debug node/"$node" -- chroot /host bash -c '
    echo "=== Disk Usage ==="
    df -h /var/lib/etcd
    echo ""
    echo "=== I/O Statistics ==="
    iostat -x 1 3 | grep -A3 "Device"
  ' 2>/dev/null | tee "$results_dir/${node}_iostat.txt" || log_warn "Could not collect iostat for $node"
  
  echo ""
done

log_section "etcd Health Check"

# Check etcd cluster health
log_info "Checking etcd cluster health..."
health_output=$(oc exec -n openshift-etcd "$etcd_pod" -c etcd -- /bin/bash -c '
etcdctl endpoint health \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca/ca.crt \
  --cert=/etc/etcd/client/tls.crt \
  --key=/etc/etcd/client/tls.key \
  --cluster
' 2>&1)

echo "$health_output" | tee "$results_dir/etcd_health.txt"

if echo "$health_output" | grep -q "is healthy"; then
  log_info "✓ All etcd members are healthy"
else
  log_error "✗ Some etcd members are unhealthy"
  all_passed=false
fi

# Check etcd member list
log_info ""
log_info "etcd member list:"
oc exec -n openshift-etcd "$etcd_pod" -c etcd -- /bin/bash -c '
etcdctl member list -w table \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca/ca.crt \
  --cert=/etc/etcd/client/tls.crt \
  --key=/etc/etcd/client/tls.key
' 2>&1 | tee "$results_dir/etcd_members.txt"

log_section "Performance Summary Report"

# Generate summary report
cat > "$results_dir/SUMMARY_REPORT.txt" <<REPORT
etcd Performance Verification Report
=====================================
Date: $(date)
Cluster: $(oc whoami --show-server)

DISK PERFORMANCE (FIO Tests)
-----------------------------
$(for node in "${master_nodes[@]}"; do
  echo "Node: $node"
  if [[ -f "$results_dir/${node}_seq-write.json" ]]; then
    iops=$(jq -r '.jobs[0].write.iops // 0' "$results_dir/${node}_seq-write.json" 2>/dev/null)
    echo "  Sequential Write IOPS: $iops"
  fi
  if [[ -f "$results_dir/${node}_rand-write-fsync.json" ]]; then
    iops=$(jq -r '.jobs[0].write.iops // 0' "$results_dir/${node}_rand-write-fsync.json" 2>/dev/null)
    echo "  Random Write with fsync IOPS: $iops"
  fi
  echo ""
done)

ETCD METRICS
------------
WAL fsync duration (99th): ${wal_fsync_99_ms}ms (threshold: <${THRESHOLDS[etcd_wal_sync_max]}ms)
Backend commit duration (99th): ${backend_commit_99_ms}ms (threshold: <25ms)
Apply duration (99th): ${apply_99_ms}ms (threshold: <${THRESHOLDS[etcd_99_percentile_max]}ms)
Leader changes (hourly): $leader_changes
Database size: ${db_size_mb}MB

ETCD HEALTH
-----------
$(cat "$results_dir/etcd_health.txt" 2>/dev/null || echo "N/A")

OVERALL ASSESSMENT
------------------
REPORT

if [[ "$all_passed" == true ]]; then
  echo "STATUS: PASSED ✓" >> "$results_dir/SUMMARY_REPORT.txt"
  echo "All performance metrics meet requirements." >> "$results_dir/SUMMARY_REPORT.txt"
  log_info "${GREEN}✓ All performance checks PASSED${NC}"
else
  echo "STATUS: NEEDS ATTENTION ⚠" >> "$results_dir/SUMMARY_REPORT.txt"
  echo "Some metrics are outside acceptable thresholds. Review detailed logs." >> "$results_dir/SUMMARY_REPORT.txt"
  log_warn "${YELLOW}⚠ Some performance checks need attention${NC}"
fi

cat "$results_dir/SUMMARY_REPORT.txt"

log_section "Verification Complete"
log_info "Detailed results saved to: $results_dir"
log_info "Summary report: $results_dir/SUMMARY_REPORT.txt"
log_info ""
log_info "Next steps:"
log_info "  1. Review the summary report"
log_info "  2. Import Grafana dashboard for continuous monitoring"
log_info "  3. Run Prometheus query verification script"

exit 0etcd Performance Verification Script"
log_info "This script will:"
log_info "  1. Run FIO disk performance tests (Red Hat KB 4885641)"
log_info "  2. Run etcd-perf benchmark (Red Hat KB 6271341)"
log_info "  3. Check etcd metrics and health"
log_info "  4. Generate performance report"
echo ""

# Get master nodes
log_info "Retrieving master node information..."
master_nodes=($(oc get nodes -l node-role.kubernetes.io/master= -o jsonpath='{.items[*].metadata.name}'))

if [[ ${#master_nodes[@]} -ne 3 ]]; then
  log_error "Expected 3 master nodes, found ${#master_nodes[@]}"
  exit 1
fi

log_info "Master nodes: ${master_nodes[*]}"

# Results storage
results_dir="/tmp/etcd-perf-results-$(date +%Y%m%d-%H%M%S)"
mkdir -p "$results_dir"
log_info "Results will be saved to: $results_dir"

log_section "


####prometheus

#!/bin/bash
set -euo pipefail

# Prometheus Query Verification Script based on Red Hat KB 548971
# This script validates etcd performance using Prometheus queries

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
  echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
}

log_error() {
  echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
}

log_warn() {
  echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
}

log_section() {
  echo ""
  echo -e "${BLUE}====== $* ======${NC}"
}

# Get Prometheus pod
PROM_POD=$(oc get pods -n openshift-monitoring -l app.kubernetes.io/name=prometheus -o name | head -1)

if [[ -z "$PROM_POD" ]]; then
  log_error "Could not find Prometheus pod"
  exit 1
fi

log_info "Using Prometheus pod: $PROM_POD"

# Function to query Prometheus
query_prometheus() {
  local query=$1
  local description=$2
  
  log_info "Querying: $description"
  log_info "Query: $query"
  
  result=$(oc exec -n openshift-monitoring "$PROM_POD" -c prometheus -- \
    curl -s -G "http://localhost:9090/api/v1/query" \
    --data-urlencode "query=$query" 2>/dev/null)
  
  echo "$result"
}

# Function to evaluate query result
evaluate_result() {
  local result=$1
  local threshold=$2
  local comparison=$3  # "lt" for less than, "gt" for greater than
  local metric_name=$4
  
  value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
value_mb=$(echo "scale=2; $value / 1024 / 1024" | bc 2>/dev/null || echo "0")
log_info "  Result: ${value_mb}MB in use"

log_section "7. etcd Snapshot and Backup Metrics"

# Snapshot save duration
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 13: Snapshot save duration (99th percentile)"
result=$(query_prometheus \
  'histogram_quantile(0.99, rate(etcd_debugging_snap_save_marshalling_duration_seconds_bucket[5m]))' \
  "Snapshot save duration")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
value_ms=$(echo "scale=2; $value * 1000" | bc 2>/dev/null || echo "N/A")
log_info "  Result: ${value_ms}ms"
if [[ "$value_ms" != "N/A" ]]; then
  passed_checks=$((passed_checks + 1))
fi

log_section "8. etcd Proposals and Consensus"

# Pending proposals
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 14: Pending proposals (should be low)"
result=$(query_prometheus \
  'etcd_server_proposals_pending' \
  "Number of pending proposals")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value pending proposals"
if (( $(echo "$value < 100" | bc -l) )); then
  log_info "  ✓ PASS (< 100)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ WARNING (high number of pending proposals)"
fi

# Failed proposals
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 15: Failed proposals rate"
result=$(query_prometheus \
  'rate(etcd_server_proposals_failed_total[5m])' \
  "Failed proposals per second")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value failures/sec"
if (( $(echo "$value < 0.01" | bc -l) )); then
  log_info "  ✓ PASS (< 0.01/sec)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ WARNING (proposals are failing)"
fi

# Committed proposals
log_info ""
log_info "Check 16: Committed proposals rate"
result=$(query_prometheus \
  'rate(etcd_server_proposals_committed_total[5m])' \
  "Committed proposals per second")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value commits/sec"

log_section "9. etcd Member Health"

# Has leader
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 17: Cluster has leader"
result=$(query_prometheus \
  'sum(etcd_server_has_leader)' \
  "Number of members that see a leader")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value members see a leader"
if [[ "$value" == "3" ]]; then
  log_info "  ✓ PASS (all 3 members see leader)"
  passed_checks=$((passed_checks + 1))
else
  log_error "  ✗ FAIL (not all members see leader)"
fi

# Heartbeat send failures
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 18: Heartbeat send failures"
result=$(query_prometheus \
  'rate(etcd_server_heartbeat_send_failures_total[5m])' \
  "Heartbeat failures")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value failures/sec"
if (( $(echo "$value < 0.01" | bc -l) )); then
  log_info "  ✓ PASS (< 0.01/sec)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ WARNING (heartbeat failures detected)"
fi

log_section "10. Slow Operations Metrics"

# Slow apply operations
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 19: Slow apply operations"
result=$(query_prometheus \
  'rate(etcd_server_slow_apply_total[5m])' \
  "Rate of slow apply operations")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value slow applies/sec"
if (( $(echo "$value < 0.01" | bc -l) )); then
  log_info "  ✓ PASS (< 0.01/sec)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ WARNING (slow apply operations detected)"
fi

# Slow read index operations
log_info ""
log_info "Check 20: Slow read index operations"
result=$(query_prometheus \
  'rate(etcd_server_slow_read_indexes_total[5m])' \
  "Rate of slow read operations")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value slow reads/sec"
if (( $(echo "$value < 0.01" | bc -l) )); then
  log_info "  ✓ PASS (< 0.01/sec)"
else
  log_warn "  ✗ WARNING (slow read operations detected)"
fi

log_section "11. Additional Performance Indicators"

# Client requests per second
log_info ""
log_info "Check 21: Client request rate"
result=$(query_prometheus \
  'sum(rate(grpc_server_started_total{grpc_type="unary"}[5m]))' \
  "Total client requests per second")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value requests/sec"

# WAL sync operations rate
log_info ""
log_info "Check 22: WAL sync operations rate"
result=$(query_prometheus \
  'rate(etcd_disk_wal_fsync_duration_seconds_count[5m])' \
  "WAL fsync operations per second")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value syncs/sec"

# Backend commits rate
log_info ""
log_info "Check 23: Backend commit operations rate"
result=$(query_prometheus \
  'rate(etcd_disk_backend_commit_duration_seconds_count[5m])' \
  "Backend commits per second")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value commits/sec"

log_section "Summary Report"

percentage=$(echo "scale=2; ($passed_checks / $total_checks) * 100" | bc)

log_info ""
log_info "Results: $passed_checks/$total_checks checks passed (${percentage}%)"
log_info ""

if [[ "$passed_checks" -eq "$total_checks" ]]; then
  log_info "${GREEN}✓ ALL CHECKS PASSED${NC}"
  log_info "etcd cluster performance is excellent"
  exit_code=0
elif [[ "$passed_checks" -ge $((total_checks * 80 / 100)) ]]; then
  log_warn "${YELLOW}⚠ MOST CHECKS PASSED${NC}"
  log_warn "etcd cluster performance is acceptable but some metrics need attention"
  exit_code=0
else
  log_error "${RED}✗ MULTIPLE CHECKS FAILED${NC}"
  log_error "etcd cluster performance needs immediate attention"
  exit_code=1
fi

log_info ""
log_info "Detailed results saved to: $results_file"
log_info ""
log_info "Recommendations:"
log_info "  1. Review failed checks and investigate root causes"
log_info "  2. Monitor trends over time using Grafana dashboards"
log_info "  3. Consider defragmentation if database size is high"
log_info "  4. Check for application issues if request rates are abnormal"

exit $exit_code // "N/A"' 2>/dev/null)
  
  if [[ "$value" == "N/A" ]] || [[ "$value" == "null" ]]; then
    log_warn "  Result: No data available"
    return 1
  fi
  
  log_info "  Result: $value"
  
  # Convert to comparable format
  if [[ "$comparison" == "lt" ]]; then
    if (( $(echo "$value < $threshold" | bc -l) )); then
      log_info "  ✓ PASS (< $threshold)"
      return 0
    else
      log_warn "  ✗ FAIL (should be < $threshold)"
      return 1
    fi
  elif [[ "$comparison" == "gt" ]]; then
    if (( $(echo "$value > $threshold" | bc -l) )); then
      log_warn "  ✗ FAIL (should be < $threshold)"
      return 1
    else
      log_info "  ✓ PASS (< $threshold)"
      return 0
    fi
  fi
}

results_file="/tmp/prometheus-etcd-queries-$(date +%Y%m%d-%H%M%S).txt"
exec > >(tee -a "$results_file")
exec 2>&1

log_section "etcd Prometheus Query Verification (Red Hat KB 548971)"

# Counter for passed/failed checks
total_checks=0
passed_checks=0

log_section "1. etcd Disk Performance Metrics"

# 99th percentile disk WAL fsync duration
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 1: WAL fsync duration (99th percentile)"
result=$(query_prometheus \
  'histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m]))' \
  "WAL fsync duration should be < 10ms")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
value_ms=$(echo "scale=2; $value * 1000" | bc 2>/dev/null || echo "N/A")
log_info "  Result: ${value_ms}ms"
if [[ "$value_ms" != "N/A" ]] && (( $(echo "$value < 0.01" | bc -l) )); then
  log_info "  ✓ PASS (< 10ms)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ FAIL (should be < 10ms)"
fi

# 99th percentile backend commit duration
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 2: Backend commit duration (99th percentile)"
result=$(query_prometheus \
  'histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m]))' \
  "Backend commit duration should be < 25ms")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
value_ms=$(echo "scale=2; $value * 1000" | bc 2>/dev/null || echo "N/A")
log_info "  Result: ${value_ms}ms"
if [[ "$value_ms" != "N/A" ]] && (( $(echo "$value < 0.025" | bc -l) )); then
  log_info "  ✓ PASS (< 25ms)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ FAIL (should be < 25ms)"
fi

log_section "2. etcd Request Duration Metrics"

# 99th percentile apply duration
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 3: Apply request duration (99th percentile)"
result=$(query_prometheus \
  'histogram_quantile(0.99, rate(etcd_request_duration_seconds_bucket{operation="apply"}[5m]))' \
  "Apply duration should be < 50ms for healthy performance")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
value_ms=$(echo "scale=2; $value * 1000" | bc 2>/dev/null || echo "N/A")
log_info "  Result: ${value_ms}ms"
if [[ "$value_ms" != "N/A" ]] && (( $(echo "$value < 0.05" | bc -l) )); then
  log_info "  ✓ PASS (< 50ms)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ FAIL (should be < 50ms)"
fi

# Average apply duration
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 4: Average apply request duration"
result=$(query_prometheus \
  'rate(etcd_request_duration_seconds_sum{operation="apply"}[5m]) / rate(etcd_request_duration_seconds_count{operation="apply"}[5m])' \
  "Average apply duration")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
value_ms=$(echo "scale=2; $value * 1000" | bc 2>/dev/null || echo "N/A")
log_info "  Result: ${value_ms}ms"
if [[ "$value_ms" != "N/A" ]]; then
  passed_checks=$((passed_checks + 1))
fi

log_section "3. etcd Leader Stability"

# Leader changes
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 5: Leader changes (should be 0 or very low)"
result=$(query_prometheus \
  'rate(etcd_server_leader_changes_seen_total[1h])' \
  "Leader changes per hour")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
log_info "  Result: $value changes/hour"
if (( $(echo "$value < 1" | bc -l) )); then
  log_info "  ✓ PASS (< 1 change/hour)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ FAIL (frequent leader changes detected)"
fi

# Current leader
log_info ""
log_info "Check 6: Identify current leader"
result=$(query_prometheus \
  'etcd_server_is_leader' \
  "Current etcd leader")
echo "$result" | jq -r '.data.result[] | select(.value[1]=="1") | "  Leader: " + .metric.instance' || log_warn "  Could not determine leader"

log_section "4. etcd Database Metrics"

# Database size
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 7: etcd database size"
result=$(query_prometheus \
  'etcd_mvcc_db_total_size_in_bytes' \
  "Database size (should be < 8GB)")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
value_mb=$(echo "scale=2; $value / 1024 / 1024" | bc 2>/dev/null || echo "0")
value_gb=$(echo "scale=2; $value / 1024 / 1024 / 1024" | bc 2>/dev/null || echo "0")
log_info "  Result: ${value_mb}MB (${value_gb}GB)"
if (( $(echo "$value_gb < 8" | bc -l) )); then
  log_info "  ✓ PASS (< 8GB)"
  passed_checks=$((passed_checks + 1))
else
  log_warn "  ✗ WARNING (database approaching size limit)"
fi

# Database usage percentage
log_info ""
log_info "Check 8: Database size vs quota"
result=$(query_prometheus \
  '(etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) * 100' \
  "Database usage percentage")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
log_info "  Result: ${value}%"
if [[ "$value" != "N/A" ]]; then
  log_info "  Note: etcd will alarm at 95% usage"
fi

log_section "5. etcd Network Metrics"

# gRPC sent bytes rate
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 9: gRPC sent bytes rate"
result=$(query_prometheus \
  'rate(etcd_network_client_grpc_sent_bytes_total[5m])' \
  "Network throughput (bytes sent)")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
value_kb=$(echo "scale=2; $value / 1024" | bc 2>/dev/null || echo "0")
log_info "  Result: ${value_kb} KB/s"
if [[ "$value" != "N/A" ]]; then
  passed_checks=$((passed_checks + 1))
fi

# gRPC received bytes rate
log_info ""
log_info "Check 10: gRPC received bytes rate"
result=$(query_prometheus \
  'rate(etcd_network_client_grpc_received_bytes_total[5m])' \
  "Network throughput (bytes received)")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "0"')
value_kb=$(echo "scale=2; $value / 1024" | bc 2>/dev/null || echo "0")
log_info "  Result: ${value_kb} KB/s"

log_section "6. etcd Compaction and Defragmentation"

# Keys compacted
total_checks=$((total_checks + 1))
log_info ""
log_info "Check 11: Keys compacted"
result=$(query_prometheus \
  'etcd_debugging_mvcc_keys_total' \
  "Total keys in database")
value=$(echo "$result" | jq -r '.data.result[0].value[1] // "N/A"')
log_info "  Result: $value keys"
if [[ "$value" != "N/A" ]]; then
  passed_checks=$((passed_checks + 1))
fi

# DB size in use
log_info ""
log_info "Check 12: Database space in use"
result=$(query_prometheus \
  'etcd_mvcc_db_total_size_in_use_in_bytes' \
  "Database space actually in use")
value=$(echo "$result" | jq -r '.data.result[0].value[1]

#####grafana

#!/bin/bash
set -euo pipefail

# Grafana Dashboard Import Script for etcd Monitoring
# This script imports the etcd dashboard and configures Prometheus data source

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
  echo -e "${GREEN}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} INFO: $*"
}

log_error() {
  echo -e "${RED}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} ERROR: $*" >&2
}

log_warn() {
  echo -e "${YELLOW}[$(date '+%Y-%m-%d %H:%M:%S')]${NC} WARN: $*"
}

log_section() {
  echo ""
  echo -e "${BLUE}====== $* ======${NC}"
}

# Check if jq is available
if ! command -v jq &> /dev/null; then
  log_error "jq is required but not installed. Please install jq."
  exit 1
fi

log_section "Grafana Dashboard Import for etcd Monitoring"

# Get Grafana route
log_info "Finding Grafana route..."
GRAFANA_ROUTE=$(oc get route -n openshift-monitoring grafana -o jsonpath='{.spec.host}' 2>/dev/null || echo "")

if [[ -z "$GRAFANA_ROUTE" ]]; then
  # Try openshift-user-workload-monitoring namespace
  GRAFANA_ROUTE=$(oc get route -n openshift-user-workload-monitoring grafana -o jsonpath='{.spec.host}' 2>/dev/null || echo "")
fi

if [[ -z "$GRAFANA_ROUTE" ]]; then
  log_error "Could not find Grafana route. Grafana may not be deployed."
  log_info "You can manually import the dashboard JSON using the Grafana UI"
  log_info "Dashboard JSON will be saved to: /tmp/etcd-dashboard.json"
  MANUAL_IMPORT=true
else
  log_info "Grafana URL: https://$GRAFANA_ROUTE"
  MANUAL_IMPORT=false
fi

# Get Grafana admin credentials
if [[ "$MANUAL_IMPORT" == "false" ]]; then
  log_info "Retrieving Grafana admin credentials..."
  
  GRAFANA_ADMIN_USER=$(oc get secret -n openshift-monitoring grafana-admin-credentials -o jsonpath='{.data.GF_SECURITY_ADMIN_USER}' 2>/dev/null | base64 -d || echo "admin")
  GRAFANA_ADMIN_PASS=$(oc get secret -n openshift-monitoring grafana-admin-credentials -o jsonpath='{.data.GF_SECURITY_ADMIN_PASSWORD}' 2>/dev/null | base64 -d || echo "")
  
  if [[ -z "$GRAFANA_ADMIN_PASS" ]]; then
    log_warn "Could not retrieve Grafana admin password automatically"
    read -sp "Enter Grafana admin password: " GRAFANA_ADMIN_PASS
    echo ""
  fi
  
  GRAFANA_URL="https://$GRAFANA_ROUTE"
fi

# Create comprehensive etcd dashboard JSON
log_info "Creating etcd monitoring dashboard..."

cat > /tmp/etcd-dashboard.json << 'DASHBOARD_EOF'
{
  "dashboard": {
    "title": "etcd Performance Monitoring",
    "tags": ["etcd", "performance", "kubernetes"],
    "timezone": "browser",
    "schemaVersion": 16,
    "version": 1,
    "refresh": "30s",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "templating": {
      "list": [
        {
          "name": "datasource",
          "type": "datasource",
          "query": "prometheus",
          "current": {
            "text": "Prometheus",
            "value": "Prometheus"
          },
          "hide": 0,
          "label": "Data Source",
          "includeAll": false,
          "multi": false
        },
        {
          "name": "instance",
          "type": "query",
          "datasource": "$datasource",
          "query": "label_values(etcd_server_has_leader, instance)",
          "current": {
            "text": "All",
            "value": "$__all"
          },
          "hide": 0,
          "label": "etcd Instance",
          "includeAll": true,
          "multi": true,
          "refresh": 1
        }
      ]
    },
    "panels": [
      {
        "id": 1,
        "title": "etcd Cluster Health",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(etcd_server_has_leader)",
            "legendFormat": "Members with Leader"
          }
        ],
        "gridPos": {
          "h": 4,
          "w": 4,
          "x": 0,
          "y": 0
        },
        "options": {
          "graphMode": "none",
          "colorMode": "value",
          "textMode": "value_and_name"
        },
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                { "value": 0, "color": "red" },
                { "value": 2, "color": "yellow" },
                { "value": 3, "color": "green" }
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "etcd Database Size",
        "type": "stat",
        "targets": [
          {
            "expr": "etcd_mvcc_db_total_size_in_bytes / 1024 / 1024",
            "legendFormat": "DB Size (MB)"
          }
        ],
        "gridPos": {
          "h": 4,
          "w": 4,
          "x": 4,
          "y": 0
        },
        "options": {
          "graphMode": "area",
          "colorMode": "value"
        },
        "fieldConfig": {
          "defaults": {
            "unit": "decmbytes",
            "thresholds": {
              "mode": "absolute",
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 4096, "color": "yellow" },
                { "value": 7168, "color": "red" }
              ]
            }
          }
        }
      },
      {
        "id": 3,
        "title": "Leader Changes",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(etcd_server_leader_changes_seen_total[1h])",
            "legendFormat": "Changes/Hour"
          }
        ],
        "gridPos": {
          "h": 4,
          "w": 4,
          "x": 8,
          "y": 0
        },
        "options": {
          "graphMode": "area",
          "colorMode": "value"
        },
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                { "value": 0, "color": "green" },
                { "value": 1, "color": "yellow" },
                { "value": 3, "color": "red" }
              ]
            }
          }
        }
      },
      {
        "id": 4,
        "title": "WAL Fsync Duration (99th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{instance=~\"$instance\"}[5m])) * 1000",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 4
        },
        "yaxes": [
          {
            "label": "Duration (ms)",
            "format": "ms"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [10],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "name": "WAL Fsync Duration Alert",
          "message": "etcd WAL fsync duration is above 10ms"
        },
        "thresholds": [
          {
            "value": 10,
            "colorMode": "critical",
            "fill": true,
            "line": true,
            "op": "gt"
          }
        ]
      },
      {
        "id": 5,
        "title": "Backend Commit Duration (99th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{instance=~\"$instance\"}[5m])) * 1000",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 4
        },
        "yaxes": [
          {
            "label": "Duration (ms)",
            "format": "ms"
          }
        ],
        "thresholds": [
          {
            "value": 25,
            "colorMode": "critical",
            "fill": true,
            "line": true,
            "op": "gt"
          }
        ]
      },
      {
        "id": 6,
        "title": "Apply Request Duration (99th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(etcd_request_duration_seconds_bucket{operation=\"apply\",instance=~\"$instance\"}[5m])) * 1000",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 12
        },
        "yaxes": [
          {
            "label": "Duration (ms)",
            "format": "ms"
          }
        ],
        "thresholds": [
          {
            "value": 50,
            "colorMode": "critical",
            "fill": true,
            "line": true,
            "op": "gt"
          }
        ]
      },
      {
        "id": 7,
        "title": "Client Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(grpc_server_started_total{grpc_type=\"unary\",instance=~\"$instance\"}[5m])) by (instance)",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 12
        },
        "yaxes": [
          {
            "label": "Requests/sec",
            "format": "reqps"
          }
        ]
      },
      {
        "id": 8,
        "title": "Pending Proposals",
        "type": "graph",
        "targets": [
          {
            "expr": "etcd_server_proposals_pending{instance=~\"$instance\"}",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 8,
          "x": 0,
          "y": 20
        },
        "yaxes": [
          {
            "label": "Proposals"
          }
        ],
        "thresholds": [
          {
            "value": 100,
            "colorMode": "critical",
            "fill": true,
            "line": true,
            "op": "gt"
          }
        ]
      },
      {
        "id": 9,
        "title": "Failed Proposals Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(etcd_server_proposals_failed_total{instance=~\"$instance\"}[5m])",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 8,
          "x": 8,
          "y": 20
        },
        "yaxes": [
          {
            "label": "Failures/sec"
          }
        ]
      },
      {
        "id": 10,
        "title": "Network gRPC Throughput",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(etcd_network_client_grpc_sent_bytes_total{instance=~\"$instance\"}[5m])",
            "legendFormat": "Sent - {{instance}}"
          },
          {
            "expr": "rate(etcd_network_client_grpc_received_bytes_total{instance=~\"$instance\"}[5m])",
            "legendFormat": "Received - {{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 8,
          "x": 16,
          "y": 20
        },
        "yaxes": [
          {
            "label": "Bytes/sec",
            "format": "Bps"
          }
        ]
      },
      {
        "id": 11,
        "title": "Slow Operations",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(etcd_server_slow_apply_total{instance=~\"$instance\"}[5m])",
            "legendFormat": "Slow Apply - {{instance}}"
          },
          {
            "expr": "rate(etcd_server_slow_read_indexes_total{instance=~\"$instance\"}[5m])",
            "legendFormat": "Slow Read - {{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 28
        },
        "yaxes": [
          {
            "label": "Operations/sec"
          }
        ]
      },
      {
        "id": 12,
        "title": "Heartbeat Send Failures",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(etcd_server_heartbeat_send_failures_total{instance=~\"$instance\"}[5m])",
            "legendFormat": "{{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 28
        },
        "yaxes": [
          {
            "label": "Failures/sec"
          }
        ]
      },
      {
        "id": 13,
        "title": "Database Size Trend",
        "type": "graph",
        "targets": [
          {
            "expr": "etcd_mvcc_db_total_size_in_bytes{instance=~\"$instance\"} / 1024 / 1024",
            "legendFormat": "Total - {{instance}}"
          },
          {
            "expr": "etcd_mvcc_db_total_size_in_use_in_bytes{instance=~\"$instance\"} / 1024 / 1024",
            "legendFormat": "In Use - {{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 36
        },
        "yaxes": [
          {
            "label": "Size (MB)",
            "format": "decmbytes"
          }
        ]
      },
      {
        "id": 14,
        "title": "Keys and Watchers",
        "type": "graph",
        "targets": [
          {
            "expr": "etcd_debugging_mvcc_keys_total{instance=~\"$instance\"}",
            "legendFormat": "Keys - {{instance}}"
          },
          {
            "expr": "etcd_debugging_mvcc_watcher_total{instance=~\"$instance\"}",
            "legendFormat": "Watchers - {{instance}}"
          }
        ],
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 36
        },
        "yaxes": [
          {
            "label": "Count"
          }
        ]
      }
    ]
  },
  "overwrite": true,
  "inputs": [
    {
      "name": "DS_PROMETHEUS",
      "type": "datasource",
      "pluginId": "prometheus",
      "value": "Prometheus"
    }
  ]
}
DASHBOARD_EOF

log_info "✓ Dashboard JSON created: /tmp/etcd-dashboard.json"

# Import dashboard to Grafana
if [[ "$MANUAL_IMPORT" == "false" ]]; then
  log_section "Importing Dashboard to Grafana"
  
  log_info "Attempting to import dashboard..."
  
  response=$(curl -k -s -X POST \
    -H "Content-Type: application/json" \
    -u "${GRAFANA_ADMIN_USER}:${GRAFANA_ADMIN_PASS}" \
    -d @/tmp/etcd-dashboard.json \
    "${GRAFANA_URL}/api/dashboards/db" 2>&1)
  
  if echo "$response" | grep -q '"status":"success"'; then
    dashboard_url=$(echo "$response" | jq -r '.url // ""')
    log_info "✓ Dashboard imported successfully!"
    log_info "Dashboard URL: ${GRAFANA_URL}${dashboard_url}"
  else
    log_error "Failed to import dashboard automatically"
    log_error "Response: $response"
    log_info "Please import manually using the Grafana UI"
    MANUAL_IMPORT=true
  fi
fi

# Manual import instructions
if [[ "$MANUAL_IMPORT" == "true" ]]; then
  log_section "Manual Import Instructions"
  log_info ""
  log_info "To manually import the dashboard:"
  log_info "  1. Access Grafana UI (get route: oc get route -n openshift-monitoring grafana)"
  log_info "  2. Login with admin credentials"
  log_info "  3. Click '+' icon > Import"
  log_info "  4. Upload the file: /tmp/etcd-dashboard.json"
  log_info "  5. Select Prometheus as the data source"
  log_info "  6. Click 'Import'"
  log_info ""
fi

log_section "Configuration Complete"
log_info ""
log_info "Next steps:"
log_info "  1. Access the dashboard in Grafana"
log_info "  2. Select your Prometheus data source from the dropdown"
log_info "  3. Use the instance filter to view specific etcd members"
log_info "  4. Set up alert notifications if needed"
log_info ""
log_info "Dashboard features:"
log_info "  ✓ Real-time etcd performance metrics"
log_info "  ✓ WAL fsync and backend commit latencies"
log_info "  ✓ Leader stability monitoring"
log_info "  ✓ Database size tracking"
log_info "  ✓ Network throughput visualization"
log_info "  ✓ Slow operations detection"

exit 0


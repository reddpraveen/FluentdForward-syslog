# OpenShift Logging Cluster Context & Prep Call Agenda

## Environment Context
- **OpenShift Version:** 4.14
- **Logging Version:** 5.9
- **Storage Backend:** Azure Blob Storage
- **Current Ingestion Rate:** 5MB/s
- **Maximum Capacity:** 20MB/s
- **Log Outputs:** External Elasticsearch + 30 syslog containers + LokiStack (default)

## Current Architecture Challenges
- CLF (ClusterLogForwarder) with multiple outputs creating log duplication
- Debug-enabled syslog containers writing to stdout (captured by Vector)
- Vector simultaneously sending to LokiStack
- Hundreds of applications requiring debug logging capability
- Log volume unpredictability during debug scenarios

---

## Prep Call Agenda Items

### 1. Current Collector Settings & Loki Ingestion Throttle

**Questions to Address:**
- What are the current Vector collector resource limits and buffer settings?
- What is the Loki ingestion throttle set to? (5MB/s or 8MB/s)
- Are we hitting any current ingestion limits?

**Commands to Run Before Call:**
```bash
# Check Vector configuration
oc get configmap/vector -n openshift-logging -o yaml

# Check Vector DaemonSet resources
oc get daemonset/vector -n openshift-logging -o yaml | grep -A 10 resources

# Check LokiStack limits
oc get lokistack -n openshift-logging -o yaml | grep -A 20 limits

# Current Vector metrics
oc exec -n openshift-logging ds/vector -- curl -s localhost:8686/metrics | grep vector_processed_events_total
```

**Expected Default Settings:**
- Loki ingestion rate: 4MB/s per tenant
- Loki burst limit: 6MB/s
- Max line size: 256KB
- Max entries per push: 10,000

### 2. Business Hours Logging Assessment

**Questions to Address:**
- Have we measured average cluster logging ingestion during peak business hours?
- Can Red Hat confirm Loki can handle our projected throughput?
- What's our current baseline vs. peak usage patterns?

**Metrics to Gather:**
```bash
# Business hours ingestion rate (run during 9 AM - 5 PM)
oc exec -n openshift-logging ds/vector -- curl -s localhost:8686/metrics | grep rate

# Loki distributor metrics
oc exec -n openshift-logging deployment/lokistack-distributor -- curl -s localhost:3100/metrics | grep loki_distributor

# Prometheus query for average rates
rate(vector_processed_events_total[1h])
rate(vector_processed_bytes_total[1h])
```

**Red Hat Engagement:**
- Open support case with current metrics
- Request Loki capacity assessment
- Provide: node count, expected log volume, retention requirements

### 3. Debug Pods Current Assessment

**Questions to Address:**
- Do we have a current assessment of pods running with debug enabled?
- Can we confirm no new pods have enabled debug since the last scan?
- What's our process for tracking debug-enabled applications?

**Debug Detection Commands:**
```bash
# Comprehensive debug pod scan
for ns in $(oc get ns -o name | cut -d/ -f2); do
  echo "=== Namespace: $ns ==="
  oc get pods -n $ns -o json | jq -r '.items[] | select(.spec.containers[].env[]?.value? | contains("debug") or contains("DEBUG") or contains("TRACE")) | .metadata.name'
done

# Check for debug in ConfigMaps
oc get configmaps --all-namespaces -o json | jq -r '.items[] | select(.data | to_entries[] | .value | contains("debug") or contains("DEBUG")) | "\(.metadata.namespace)/\(.metadata.name)"'

# Check for debug command line args
oc get pods --all-namespaces -o json | jq -r '.items[] | select(.spec.containers[].args[]? | contains("--debug") or contains("--log-level=debug")) | "\(.metadata.namespace)/\(.metadata.name)"'
```

**Tracking Strategy:**
- Implement automated daily scans
- Use admission controllers to flag debug-enabled deployments
- Create dashboard for debug pod tracking

### 4. Log Ingestion Delay/Issue Response Plan

**Questions to Address:**
- What's our plan if log ingestion into Loki is delayed?
- What are our escalation procedures for logging issues?
- Do we have emergency log filtering capabilities?

**Emergency Response Plan:**

**Level 1 Response (< 10% over limit):**
```yaml
# Enable log sampling
apiVersion: logging.coreos.com/v1
kind: ClusterLogForwarder
metadata:
  name: emergency-sampling
spec:
  filters:
    - name: sample-debug-logs
      type: json
      json:
        javascript: |
          if (record.log && record.log.level === "DEBUG") {
            if (Math.random() < 0.1) return record; // 10% sampling
            return null;
          }
          return record;
```

**Level 2 Response (10-25% over limit):**
```yaml
# Drop non-critical logs
apiVersion: logging.coreos.com/v1
kind: ClusterLogForwarder
metadata:
  name: emergency-filter
spec:
  filters:
    - name: critical-only
      type: json
      json:
        javascript: |
          const level = record.log?.level;
          if (level === "ERROR" || level === "FATAL" || level === "WARN") {
            return record;
          }
          return null; // Drop INFO, DEBUG, TRACE
```

**Level 3 Response (> 25% over limit):**
- Activate incident response
- Temporary application debug logging moratorium
- Emergency storage scaling

**Monitoring Alerts:**
```yaml
# Prometheus alert rules
- alert: LokiIngestionBacklog
  expr: increase(loki_distributor_ingester_append_failures_total[5m]) > 100
  for: 2m
  annotations:
    summary: "Loki ingestion failing - activate response plan"

- alert: HighLogIngestionRate
  expr: rate(vector_processed_bytes_total[5m]) > 15000000  # 15MB/s
  for: 5m
  annotations:
    summary: "Log ingestion rate approaching limits"
```

### 5. Loki Storage Capacity Assessment

**Questions to Address:**
- How much storage do we have allocated for Loki in the cluster?
- Is current storage sufficient for projected log volumes?
- What's our storage growth projection?

**Storage Assessment Commands:**
```bash
# Current Loki storage usage
oc get pvc -n openshift-logging | grep loki

# LokiStack storage configuration
oc get lokistack -n openshift-logging -o jsonpath='{.spec.storage}'

# Current disk usage
oc exec -n openshift-logging deployment/lokistack-querier -- df -h /var/loki

# Storage class details
oc get storageclass
```

**Storage Calculations:**
- **Current Rate:** 5MB/s × 86,400 seconds = 432GB/day
- **Peak Rate:** 20MB/s × 86,400 seconds = 1.73TB/day
- **30-day retention:** 12.96TB - 51.8TB storage needed

**Recommended Storage Configuration:**
```yaml
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: lokistack-sample
spec:
  size: 1x.medium  # Scale based on requirements
  storage:
    size: 15Ti  # Minimum for 30-day retention with headroom
    storageClassName: managed-premium  # Azure premium SSD
  limits:
    global:
      retention:
        days: 30
      storage:
        maxGlobalStreamsPerUser: 10000
        maxChunksPerQuery: 2000000
```

### 6. Success Confidence Assessment

**Questions to Address:**
- How confident are we this implementation will be successful?
- What are our biggest risk factors?
- What would cause us to abort/rollback the implementation?

**Risk Assessment:**

**HIGH RISK FACTORS:**
- 4x potential increase in log volume (5MB/s → 20MB/s)
- Log duplication across multiple outputs
- Unpredictable debug logging from hundreds of applications
- Limited experience with high-volume Loki deployments

**MEDIUM RISK FACTORS:**
- Azure Blob storage latency considerations
- Vector buffer/resource limitations
- Storage capacity planning uncertainties

**MITIGATION STRATEGIES:**
```yaml
# Gradual rollout configuration
apiVersion: logging.coreos.com/v1
kind: ClusterLogForwarder
metadata:
  name: staged-rollout
spec:
  filters:
    - name: namespace-sampling
      type: json
      json:
        javascript: |
          // Stage 1: Only specific namespaces
          const allowedNamespaces = ["app-tier1", "app-tier2"];
          if (allowedNamespaces.includes(record.kubernetes.namespace_name)) {
            return record;
          }
          return null;
```

**Success Metrics:**
- Log ingestion rate consistently < 15MB/s
- Loki write latency < 500ms (p95)
- Zero log loss during business hours
- Storage growth < 1TB/day average
- No application performance impact

**Rollback Triggers:**
- Sustained ingestion rate > 18MB/s
- Loki write failures > 5% for 10+ minutes
- Storage growth > 2TB/day for 3+ consecutive days
- Application performance degradation attributed to logging

**Confidence Level: MEDIUM-HIGH**
- ✅ Technical solution is architecturally sound
- ✅ Comprehensive monitoring and alerting planned
- ✅ Clear rollback procedures defined
- ⚠️ High log volume variability risk
- ⚠️ Limited production testing at scale

**Recommendation:** Proceed with staged rollout approach, robust monitoring, and clearly defined success/failure criteria.

---

## Pre-Call Action Items

### Technical Team Tasks:
1. Run all diagnostic commands listed above
2. Document current resource utilization
3. Identify all current debug-enabled applications
4. Test emergency log filtering in non-production
5. Validate storage capacity calculations

### Business/Management Tasks:
1. Define acceptable risk tolerance levels
2. Approve staged rollout timeline
3. Establish incident response procedures
4. Coordinate with application teams for debug logging schedules

### Red Hat Engagement:
1. Open support case for capacity assessment
2. Request architectural review of proposed solution
3. Validate emergency procedures with support team

---

## Post-Call Deliverables

1. **Decision Matrix:** Go/No-Go criteria with specific metrics
2. **Implementation Timeline:** Staged rollout schedule
3. **Monitoring Dashboard:** Real-time ingestion and health metrics
4. **Runbook:** Emergency procedures and rollback steps
5. **Communication Plan:** Stakeholder updates and escalation procedures




=============================

# OpenShift 4.14 Dashboard Namespace Switching Troubleshooting

## Problem Description
Dashboards only populate when first visiting the monitoring namespace, but fail to load data when switching to other namespaces. This indicates issues with:
- Namespace context not being properly passed to Prometheus queries
- RBAC permissions for cross-namespace monitoring
- Console frontend caching issues
- Prometheus query template variable substitution

## Step 1: Verify Console and Monitoring Pod Health

### Check Console Pods
```bash
# Check console pod status
oc get pods -n openshift-console
oc logs -n openshift-console deployment/console -c console --tail=50

# Check for JavaScript errors in console logs
oc logs -n openshift-console deployment/console -c console | grep -i error

# Check console configuration
oc get consoles.config.openshift.io cluster -o yaml
```

### Check Monitoring Stack Health
```bash
# Check Prometheus pods
oc get pods -n openshift-monitoring | grep prometheus

# Check Prometheus operator
oc get pods -n openshift-monitoring | grep prometheus-operator
oc logs -n openshift-monitoring deployment/prometheus-operator --tail=50

# Check Thanos querier (handles multi-namespace queries)
oc get pods -n openshift-monitoring | grep thanos-querier
oc logs -n openshift-monitoring deployment/thanos-querier --tail=50

# Check cluster monitoring config
oc get configmap cluster-monitoring-config -n openshift-monitoring -o yaml
```

## Step 2: Verify RBAC and Permissions

### Check User Monitoring Permissions
```bash
# Check if user can view monitoring in target namespaces
oc auth can-i get pods --as=system:serviceaccount:openshift-console:console -n <target-namespace>
oc auth can-i get services --as=system:serviceaccount:openshift-console:console -n <target-namespace>

# Check monitoring-specific permissions
oc auth can-i get servicemonitors --as=system:serviceaccount:openshift-console:console -n <target-namespace>

# Verify console service account permissions
oc describe clusterrolebinding console
oc describe rolebinding -n <target-namespace> | grep console
```

### Fix Console Service Account Permissions
```yaml
# If permissions are missing, apply this RBAC fix
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: console-monitoring-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-monitoring-view
subjects:
- kind: ServiceAccount
  name: console
  namespace: openshift-console
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: console-namespace-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: ServiceAccount
  name: console
  namespace: openshift-console
```

## Step 3: Check Prometheus Query Issues

### Test Prometheus Queries Manually
```bash
# Port forward to Prometheus
oc port-forward -n openshift-monitoring svc/prometheus-k8s 9090:9090

# Test namespace-specific queries (in another terminal)
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=up{namespace="<your-namespace>"}'

# Test cross-namespace queries
curl -G 'http://localhost:9090/api/v1/query' \
  --data-urlencode 'query=kube_pod_info{namespace="<your-namespace>"}'

# Check if namespace label exists in metrics
curl -G 'http://localhost:9090/api/v1/label/namespace/values'
```

### Check ServiceMonitor Configuration
```bash
# List ServiceMonitors in target namespace
oc get servicemonitor -n <target-namespace>

# Check ServiceMonitor configuration
oc get servicemonitor -n <target-namespace> -o yaml

# Verify service discovery
oc get endpoints -n <target-namespace>
```

## Step 4: Console Frontend Debugging

### Clear Console Cache and Restart
```bash
# Restart console pods to clear cache
oc rollout restart deployment/console -n openshift-console

# Check console startup logs
oc logs -n openshift-console deployment/console -c console -f

# Verify console service
oc get svc console -n openshift-console
oc describe svc console -n openshift-console
```

### Check Console Configuration
```bash
# Check console operator status
oc get consoles.operator.openshift.io cluster -o yaml

# Check console routes
oc get routes -n openshift-console

# Verify console configmap
oc get configmap console-config -n openshift-console -o yaml
```

## Step 5: Browser-Side Debugging

### Browser Console Debug Steps
```javascript
// Open browser developer tools (F12)
// Navigate to Console tab and run these checks:

// 1. Check for JavaScript errors
console.log("Checking for errors...");

// 2. Check namespace context in local storage
localStorage.getItem('bridge/last-namespace-name');
localStorage.getItem('bridge/active-namespace');

// 3. Clear console cache
localStorage.clear();
sessionStorage.clear();

// 4. Check network requests
// Go to Network tab, filter by "api/prometheus" and observe failed requests
```

### Browser Network Analysis
1. Open Network tab in browser dev tools
2. Switch namespaces and observe API calls
3. Look for failed requests to `/api/prometheus/` endpoints
4. Check request headers for proper namespace parameters

## Step 6: User Workload Monitoring Configuration

### Enable User Workload Monitoring (if not enabled)
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusK8s:
      retention: 15d
      volumeClaimTemplate:
        spec:
          storageClassName: managed-premium
          resources:
            requests:
              storage: 100Gi
```

### Configure User Workload Monitoring
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: 7d
      resources:
        requests:
          cpu: 100m
          memory: 512Mi
    thanosRuler:
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
```

## Step 7: Namespace-Specific Monitoring Setup

### Verify Target Namespace Configuration
```bash
# Check if namespace has monitoring labels
oc get namespace <target-namespace> --show-labels

# Add monitoring labels if missing
oc label namespace <target-namespace> openshift.io/cluster-monitoring="true"

# Check for PodMonitor and ServiceMonitor resources
oc get podmonitor,servicemonitor -n <target-namespace>
```

### Create Test ServiceMonitor
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: test-service-monitor
  namespace: <target-namespace>
spec:
  selector:
    matchLabels:
      app: test-app
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
```

## Step 8: Thanos Querier Issues

### Check Thanos Configuration
```bash
# Check Thanos querier logs for namespace-related errors
oc logs -n openshift-monitoring deployment/thanos-querier -c thanos-query --tail=100

# Check Thanos querier configuration
oc get secret thanos-querier-web-config -n openshift-monitoring -o yaml

# Test Thanos API directly
oc port-forward -n openshift-monitoring svc/thanos-querier 9091:9091

# Test namespace query through Thanos
curl -G 'http://localhost:9091/api/v1/query' \
  --data-urlencode 'query=up{namespace="<your-namespace>"}'
```

### Fix Thanos Querier Issues
```bash
# Restart Thanos querier
oc rollout restart deployment/thanos-querier -n openshift-monitoring

# Check Thanos store gateway
oc get pods -n openshift-monitoring | grep thanos-store
oc logs -n openshift-monitoring deployment/thanos-store-shard-0 --tail=50
```

## Step 9: Prometheus Operator Issues

### Check Prometheus Operator
```bash
# Check operator logs
oc logs -n openshift-monitoring deployment/prometheus-operator --tail=100

# Check Prometheus custom resources
oc get prometheus -n openshift-monitoring
oc get prometheus -n openshift-monitoring prometheus-k8s -o yaml

# Check ServiceMonitor discovery
oc logs -n openshift-monitoring prometheus-k8s-0 -c prometheus | grep -i servicemonitor
```

### Fix ServiceMonitor Discovery
```bash
# Check if Prometheus is discovering ServiceMonitors
oc exec -n openshift-monitoring prometheus-k8s-0 -c prometheus -- \
  wget -qO- localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.namespace=="<your-namespace>")'

# Restart Prometheus if needed
oc delete pod prometheus-k8s-0 -n openshift-monitoring
```

## Step 10: Complete Fix Implementation

### Apply All Fixes
```bash
#!/bin/bash
# comprehensive-dashboard-fix.sh

echo "=== OpenShift 4.14 Dashboard Fix ==="

# 1. Fix RBAC permissions
oc apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: console-monitoring-fix
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-monitoring-view
subjects:
- kind: ServiceAccount
  name: console
  namespace: openshift-console
EOF

# 2. Restart console
echo "Restarting console..."
oc rollout restart deployment/console -n openshift-console

# 3. Restart Thanos querier
echo "Restarting Thanos querier..."
oc rollout restart deployment/thanos-querier -n openshift-monitoring

# 4. Enable user workload monitoring if not enabled
echo "Checking user workload monitoring..."
if ! oc get configmap cluster-monitoring-config -n openshift-monitoring >/dev/null 2>&1; then
  oc apply -f - <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
EOF
fi

# 5. Wait for rollouts
oc rollout status deployment/console -n openshift-console
oc rollout status deployment/thanos-querier -n openshift-monitoring

echo "=== Fix applied successfully ==="
echo "Please clear browser cache and retry dashboard access"
```

## Step 11: Verification Steps

### Test Dashboard Functionality
```bash
# 1. Verify console is accessible
curl -k -I https://$(oc get route console -n openshift-console -o jsonpath='{.spec.host}')

# 2. Test namespace switching via API
oc get --raw="/api/v1/namespaces" | jq '.items[].metadata.name'

# 3. Test Prometheus metrics access
oc get --raw="/api/v1/namespaces/openshift-monitoring/services/thanos-querier:web/proxy/api/v1/query?query=up" | jq

# 4. Verify user can access target namespace
oc auth can-i get pods -n <target-namespace>
```

### Browser Testing Checklist
1. ✅ Clear browser cache completely
2. ✅ Open new incognito/private window
3. ✅ Navigate to OpenShift console
4. ✅ Go to Monitoring → Dashboards
5. ✅ Select first namespace → verify data loads
6. ✅ Switch to second namespace → verify data loads
7. ✅ Switch back to first namespace → verify data still loads

## Common Root Causes and Solutions

| **Issue** | **Root Cause** | **Solution** |
|-----------|----------------|--------------|
| No data after namespace switch | Console service account lacks permissions | Apply RBAC fixes in Step 2 |
| Dashboard loads but no metrics | ServiceMonitors not discovered | Check ServiceMonitor configuration |
| 403 errors in browser | User lacks monitoring permissions | Grant cluster-monitoring-view role |
| Queries timeout | Thanos querier issues | Restart Thanos querier |
| Cache issues | Browser/console cache | Clear cache and restart console |
| Partial data loading | Prometheus service discovery | Restart Prometheus operator |

This comprehensive troubleshooting should resolve the namespace switching dashboard issues in OpenShift 4.14.
